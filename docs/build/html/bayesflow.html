<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>bayesflow package &mdash; BayesFlow beta documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->

        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="bayesflow.benchmarks package" href="bayesflow.benchmarks.html" />
    <link rel="prev" title="bayesflow" href="modules.html" />
</head>

<body class="wy-body-for-nav">
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> BayesFlow
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">bayesflow</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">bayesflow package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#subpackages">Subpackages</a><ul>
<li class="toctree-l4"><a class="reference internal" href="bayesflow.benchmarks.html">bayesflow.benchmarks package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.amortizers">bayesflow.amortizers module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.computational_utilities">bayesflow.computational_utilities module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.configuration">bayesflow.configuration module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.coupling_networks">bayesflow.coupling_networks module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.default_settings">bayesflow.default_settings module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.diagnostics">bayesflow.diagnostics module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.exceptions">bayesflow.exceptions module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.helper_classes">bayesflow.helper_classes module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.helper_functions">bayesflow.helper_functions module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.helper_networks">bayesflow.helper_networks module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.inference_networks">bayesflow.inference_networks module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.losses">bayesflow.losses module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.mcmc">bayesflow.mcmc module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.networks">bayesflow.networks module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.simulation">bayesflow.simulation module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.summary_networks">bayesflow.summary_networks module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.trainers">bayesflow.trainers module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.version">bayesflow.version module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow.wrappers">bayesflow.wrappers module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-bayesflow">Module contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">BayesFlow</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a></li>
          <li class="breadcrumb-item"><a href="modules.html">bayesflow</a></li>
      <li class="breadcrumb-item active">bayesflow package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/bayesflow.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">


<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="bayesflow-package">
<h1>bayesflow package<a class="headerlink" href="#bayesflow-package" title="Permalink to this heading"></a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this heading"></a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="bayesflow.benchmarks.html">bayesflow.benchmarks package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.benchmarks.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.benchmarks.html#module-bayesflow.benchmarks.bernoulli_glm">bayesflow.benchmarks.bernoulli_glm module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.benchmarks.html#module-bayesflow.benchmarks.bernoulli_glm_raw">bayesflow.benchmarks.bernoulli_glm_raw module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.benchmarks.html#module-bayesflow.benchmarks.gaussian_linear">bayesflow.benchmarks.gaussian_linear module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.benchmarks.html#module-bayesflow.benchmarks.gaussian_linear_uniform">bayesflow.benchmarks.gaussian_linear_uniform module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.benchmarks.html#module-bayesflow.benchmarks.gaussian_mixture">bayesflow.benchmarks.gaussian_mixture module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.benchmarks.html#module-bayesflow.benchmarks.lotka_volterra">bayesflow.benchmarks.lotka_volterra module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.benchmarks.html#module-bayesflow.benchmarks.sir">bayesflow.benchmarks.sir module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.benchmarks.html#module-bayesflow.benchmarks.slcp">bayesflow.benchmarks.slcp module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.benchmarks.html#module-bayesflow.benchmarks.slcp_distractors">bayesflow.benchmarks.slcp_distractors module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.benchmarks.html#module-bayesflow.benchmarks.two_moons">bayesflow.benchmarks.two_moons module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.benchmarks.html#module-bayesflow.benchmarks">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading"></a></h2>
</section>
<section id="module-bayesflow.amortizers">
<span id="bayesflow-amortizers-module"></span><h2>bayesflow.amortizers module<a class="headerlink" href="#module-bayesflow.amortizers" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedLikelihood">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.amortizers.</span></span><span class="sig-name descname"><span class="pre">AmortizedLikelihood</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedLikelihood"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedLikelihood" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code>, <a class="reference internal" href="#bayesflow.amortizers.AmortizedTarget" title="bayesflow.amortizers.AmortizedTarget"><code class="xref py py-class docutils literal notranslate"><span class="pre">AmortizedTarget</span></code></a></p>
<p>An interface for a surrogate model of a simulator, or an implicit likelihood
<code class="docutils literal notranslate"><span class="pre">p(data</span> <span class="pre">|</span> <span class="pre">parameters,</span> <span class="pre">context).</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedLikelihood.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedLikelihood.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedLikelihood.call" title="Permalink to this definition"></a></dt>
<dd><p>Performs a forward pass through the summary and inference network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input_dict</strong> (<em>dict</em>) – Input dictionary containing the following mandatory keys:
<cite>observables</cite> - the observables over which a condition density is learned (i.e., the data)
<cite>conditions</cite>  - the conditioning variables that the directly passed to the inference network</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the outputs of <code class="docutils literal notranslate"><span class="pre">surrogate_net(theta,</span> <span class="pre">summary_net(x,</span> <span class="pre">c_s),</span> <span class="pre">c_d)</span></code>, usually a latent variable and
log(det(Jacobian)), that is a tuple <code class="docutils literal notranslate"><span class="pre">(z,</span> <span class="pre">log_det_J)</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>net_out</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedLikelihood.call_loop">
<span class="sig-name descname"><span class="pre">call_loop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_list</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedLikelihood.call_loop"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedLikelihood.call_loop" title="Permalink to this definition"></a></dt>
<dd><p>Performs a forward pass through the surrogate network given a list of dicts
with the appropriate entries (i.e., as used for the standard call method).</p>
<p>This method is useful when GPU memory is limited or data sets have a different (non-Tensor) structure.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input_list</strong> (<em>list of dicts</em><em>, </em><em>where each dict contains the following mandatory keys</em><em>, </em><em>if DEFAULT keys unchanged:</em>) – <cite>observables</cite> - the observables over which a condition density is learned (i.e., the data)
<cite>conditions</cite>  - the conditioning variables that the directly passed to the inference network</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>net_out or (net_out, summary_out)</strong> – the outputs of <code class="docutils literal notranslate"><span class="pre">inference_net(theta,</span> <span class="pre">summary_net(x,</span> <span class="pre">c_s),</span> <span class="pre">c_d)</span></code>, usually a latent variable and
log(det(Jacobian)), that is a tuple <code class="docutils literal notranslate"><span class="pre">(z,</span> <span class="pre">log_det_J)</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple of tf.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedLikelihood.compute_loss">
<span class="sig-name descname"><span class="pre">compute_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedLikelihood.compute_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedLikelihood.compute_loss" title="Permalink to this definition"></a></dt>
<dd><p>Computes the loss of the amortized given input data provided in input_dict.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input_dict</strong> (<em>dict</em>) – Input dictionary containing the following mandatory keys:
<cite>data</cite>        - the observables over which a condition density is learned (i.e., the observables)
<cite>conditions</cite>  - the conditioning variables that the directly passed to the inference network</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor of shape (1,) - the total computed loss given input variables</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedLikelihood.log_likelihood">
<span class="sig-name descname"><span class="pre">log_likelihood</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedLikelihood.log_likelihood"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedLikelihood.log_likelihood" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the approximate log-likelihood of targets given conditional variables via
the change-of-variable formula for a conditional normalizing flow.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dict</strong> (<em>dict</em>) – Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged:
<cite>observables</cite> - the variables over which a condition density is learned (i.e., the observables)
<cite>conditions</cite>  - the conditioning variables that the directly passed to the inference network</p></li>
<li><p><strong>to_numpy</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – Boolean flag indicating whether to return the log-lik values as a <cite>np.array</cite> or a <cite>tf.Tensor</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>log_lik</strong> – the approximate log-likelihood of each data point in each data set</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor of shape (batch_size, n_obs)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedLikelihood.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedLikelihood.log_prob"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedLikelihood.log_prob" title="Permalink to this definition"></a></dt>
<dd><p>Identical to <cite>log_likelihood(input_dict, to_numpy, **kwargs)</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedLikelihood.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedLikelihood.sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedLikelihood.sample" title="Permalink to this definition"></a></dt>
<dd><p>Generates <cite>n_samples</cite> random draws from the surrogate likelihood given input conditions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dict</strong> (<em>dict</em>) – Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged:
<cite>conditions</cite> - the conditioning variables that the directly passed to the inference network</p></li>
<li><p><strong>n_samples</strong> (<em>int</em>) – The number of posterior samples to obtain from the approximate posterior</p></li>
<li><p><strong>to_numpy</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – Flag indicating whether to return the samples as a <cite>np.array</cite> or a <cite>tf.Tensor</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>lik_samples</strong> – Simulated batch of observables from the surrogate likelihood.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor or np.ndarray of shape (n_datasets, n_samples, None)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedLikelihood.sample_loop">
<span class="sig-name descname"><span class="pre">sample_loop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedLikelihood.sample_loop"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedLikelihood.sample_loop" title="Permalink to this definition"></a></dt>
<dd><p>Generates random draws from the surrogate network given a list of dicts with conditonal variables.
Useful when GPU memory is limited or data sets have a different (non-Tensor) structure.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_list</strong> (<em>list of dictionaries</em><em>, </em><em>each dictionary having the following mandatory keys</em><em>, </em><em>if DEFAULT KEYS unchanged:</em>) – <cite>conditions</cite> - the conditioning variables that the directly passed to the inference network</p></li>
<li><p><strong>n_samples</strong> (<em>int</em>) – The number of posterior draws (samples) to obtain from the approximate posterior</p></li>
<li><p><strong>to_numpy</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – Flag indicating whether to return the samples as a <cite>np.array</cite> or a <cite>tf.Tensor</cite></p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Additional keyword arguments passed to the networks</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>post_samples</strong> – the sampled parameters per data set</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor or np.ndarray of shape (n_data_sets, n_samples, data_dim)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedModelComparison">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.amortizers.</span></span><span class="sig-name descname"><span class="pre">AmortizedModelComparison</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedModelComparison"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedModelComparison" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code></p>
<p>An interface to connect an evidential network for Bayesian model comparison with an optional summary network,
as described in the original paper on evidential neural networks for model comparison:</p>
<p>[1] Radev, S. T., D’Alessandro, M., Mertens, U. K., Voss, A., Köthe, U., &amp; Bürkner, P. C. (2021).
Amortized bayesian model comparison with evidential deep learning.
IEEE Transactions on Neural Networks and Learning Systems.</p>
<p>Note: the original paper does not distinguish between the summary and the evidential networks, but
treats them as a whole, with the appropriate architetcure dictated by the model application. For the
sake of consistency, the BayesFlow library distinguisahes the two modules.</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedModelComparison.compute_loss">
<span class="sig-name descname"><span class="pre">compute_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedModelComparison.compute_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedModelComparison.compute_loss" title="Permalink to this definition"></a></dt>
<dd><p>Computes the loss of the amortized model comparison instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input_dict</strong> (<em>dict</em>) – <dl class="simple">
<dt>Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged::</dt><dd><p><cite>summary_conditions</cite> - the conditioning variables that are first passed through a summary network
<cite>direct_conditions</cite>  - the conditioning variables that the directly passed to the evidence network
<cite>model_indices</cite>      - the ground-truth, one-hot encoded model indices sampled from the model prior</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>total_loss</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor of shape (1,) - the total computed loss given input variables</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedModelComparison.evidence">
<span class="sig-name descname"><span class="pre">evidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedModelComparison.evidence"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedModelComparison.evidence" title="Permalink to this definition"></a></dt>
<dd><p>Computes the evidence for the competing models given the data sets
contained in <cite>input_dict</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedModelComparison.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedModelComparison.sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedModelComparison.sample" title="Permalink to this definition"></a></dt>
<dd><p>Samples posterior model probabilities from the higher order Dirichlet density.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dict</strong> (<em>dict</em>) – Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged
<cite>summary_conditions</cite> - the conditioning variables that are first passed through a summary network
<cite>direct_conditions</cite>  - the conditioning variables that the directly passed to the evidential network
<cite>model_indices</cite>      - the ground-truth, one-hot encoded model indices sampled from the model prior</p></li>
<li><p><strong>n_samples</strong> (<em>int</em>) – Number of samples to obtain from the approximate posterior</p></li>
<li><p><strong>to_numpy</strong> (<em>bool</em><em>, </em><em>default: True</em>) – Flag indicating whether to return the samples as a np.array or a tf.Tensor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>pm_samples</strong> – The posterior draws from the Dirichlet distribution, shape (num_samples, num_batch, num_models)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor or np.array</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedModelComparison.uncertainty_score">
<span class="sig-name descname"><span class="pre">uncertainty_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedModelComparison.uncertainty_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedModelComparison.uncertainty_score" title="Permalink to this definition"></a></dt>
<dd><p>Computes the uncertainy score according to sum(alphas) / num_models.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosterior">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.amortizers.</span></span><span class="sig-name descname"><span class="pre">AmortizedPosterior</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosterior"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosterior" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code>, <a class="reference internal" href="#bayesflow.amortizers.AmortizedTarget" title="bayesflow.amortizers.AmortizedTarget"><code class="xref py py-class docutils literal notranslate"><span class="pre">AmortizedTarget</span></code></a></p>
<p>A wrapper to connect an inference network for parameter estimation with an optional summary network
as in the original BayesFlow set-up described in the paper:</p>
<p>[1] Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., &amp; Köthe, U. (2020).
BayesFlow: Learning complex stochastic models with invertible neural networks.
IEEE Transactions on Neural Networks and Learning Systems.</p>
<p>But also allowing for augmented functionality, such as model misspecification detection in summary space:</p>
<p>[2] Schmitt, M., Bürkner, P. C., Köthe, U., &amp; Radev, S. T. (2022).
Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks
arXiv preprint arXiv:2112.08866.</p>
<p>And learning of fat-tailed posteriors with a Student-t latent pushforward density:</p>
<p>[3] Jaini, P., Kobyzev, I., Yu, Y., &amp; Brubaker, M. (2020, November).
Tails of lipschitz triangular flows.
In International Conference on Machine Learning (pp. 4673-4681). PMLR.</p>
<p>Serves as in interface for learning <code class="docutils literal notranslate"><span class="pre">p(parameters</span> <span class="pre">|</span> <span class="pre">data,</span> <span class="pre">context).</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosterior.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_summary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosterior.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosterior.call" title="Permalink to this definition"></a></dt>
<dd><p>Performs a forward pass through the summary and inference network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dict</strong> (<em>dict</em>) – Input dictionary containing the following mandatory keys, if DEFAULT keys unchanged:
<cite>parameters</cite>         - the latent model parameters over which a condition density is learned
<cite>summary_conditions</cite> - the conditioning variables (including data) that are first passed through a summary network
<cite>direct_conditions</cite>  - the conditioning variables that the directly passed to the inference network</p></li>
<li><p><strong>return_summary</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – A flag which determines whether the learnable data summaries (representations) are returned or not.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>net_out or (net_out, summary_out)</strong> – the outputs of <code class="docutils literal notranslate"><span class="pre">inference_net(theta,</span> <span class="pre">summary_net(x,</span> <span class="pre">c_s),</span> <span class="pre">c_d)</span></code>, usually a latent variable and
log(det(Jacobian)), that is a tuple <code class="docutils literal notranslate"><span class="pre">(z,</span> <span class="pre">log_det_J)</span> <span class="pre">or</span> <span class="pre">(sum_outputs,</span> <span class="pre">(z,</span> <span class="pre">log_det_J))</span> <span class="pre">if</span>
<span class="pre">return_summary</span> <span class="pre">is</span> <span class="pre">set</span> <span class="pre">to</span> <span class="pre">True</span> <span class="pre">and</span> <span class="pre">a</span> <span class="pre">summary</span> <span class="pre">network</span> <span class="pre">is</span> <span class="pre">defined.</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple of tf.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosterior.call_loop">
<span class="sig-name descname"><span class="pre">call_loop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_summary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosterior.call_loop"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosterior.call_loop" title="Permalink to this definition"></a></dt>
<dd><p>Performs a forward pass through the summary and inference network given a list of dicts
with the appropriate entries (i.e., as used for the standard call method).</p>
<p>This method is useful when GPU memory is limited or data sets have a different (non-Tensor) structure.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_list</strong> (<em>list of dicts</em><em>, </em><em>where each dict contains the following mandatory keys</em><em>, </em><em>if DEFAULT keys unchanged:</em>) – <cite>parameters</cite>         - the latent model parameters over which a condition density is learned
<cite>summary_conditions</cite> - the conditioning variables (including data) that are first passed through a summary network
<cite>direct_conditions</cite>  - the conditioning variables that the directly passed to the inference network</p></li>
<li><p><strong>return_summary</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – A flag which determines whether the learnable data summaries (representations) are returned or not.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>net_out or (net_out, summary_out)</strong> – the outputs of <code class="docutils literal notranslate"><span class="pre">inference_net(theta,</span> <span class="pre">summary_net(x,</span> <span class="pre">c_s),</span> <span class="pre">c_d)</span></code>, usually a latent variable and
log(det(Jacobian)), that is a tuple <code class="docutils literal notranslate"><span class="pre">(z,</span> <span class="pre">log_det_J)</span> <span class="pre">or</span> <span class="pre">(sum_outputs,</span> <span class="pre">(z,</span> <span class="pre">log_det_J))</span> <span class="pre">if</span>
<span class="pre">return_summary</span> <span class="pre">is</span> <span class="pre">set</span> <span class="pre">to</span> <span class="pre">True</span> <span class="pre">and</span> <span class="pre">a</span> <span class="pre">summary</span> <span class="pre">network</span> <span class="pre">is</span> <span class="pre">defined.</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple of tf.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosterior.compute_loss">
<span class="sig-name descname"><span class="pre">compute_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosterior.compute_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosterior.compute_loss" title="Permalink to this definition"></a></dt>
<dd><p>Computes the loss of the posterior amortizer given an input dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input_dict</strong> (<em>dict</em>) – Input dictionary containing the following mandatory keys:
<cite>parameters</cite>         - the latent model parameters over which a condition density is learned
<cite>summary_conditions</cite> - the conditioning variables that are first passed through a summary network
<cite>direct_conditions</cite>  - the conditioning variables that the directly passed to the inference network</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>total_loss</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor of shape (1,) - the total computed loss given input variables</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosterior.log_posterior">
<span class="sig-name descname"><span class="pre">log_posterior</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosterior.log_posterior"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosterior.log_posterior" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the approximate log-posterior of targets given conditional variables via
the change-of-variable formula for a conditional normalizing flow.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dict</strong> (<em>dict</em>) – Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged:
<cite>parameters</cite>         : the latent model parameters over which a conditional density (i.e., a posterior) is learned
<cite>summary_conditions</cite> : the conditioning variables (including data) that are first passed through a summary network
<cite>direct_conditions</cite>  : the conditioning variables that are directly passed to the inference network</p></li>
<li><p><strong>to_numpy</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – Flag indicating whether to return the lpdf values as a <cite>np.array</cite> or a <cite>tf.Tensor</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>log_post</strong> – the approximate log-posterior density of each each parameter</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor of shape (batch_size, n_obs)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosterior.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosterior.log_prob"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosterior.log_prob" title="Permalink to this definition"></a></dt>
<dd><p>Identical to <cite>log_posterior(input_dict, to_numpy, **kwargs)</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosterior.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosterior.sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosterior.sample" title="Permalink to this definition"></a></dt>
<dd><p>Generates random draws from the approximate posterior given a dictionary with conditonal variables.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dict</strong> (<em>dict</em>) – Input dictionary containing the following mandatory keys, if DEFAULT KEYS unchanged:
<cite>summary_conditions</cite> : the conditioning variables (including data) that are first passed through a summary network
<cite>direct_conditions</cite>  : the conditioning variables that the directly passed to the inference network</p></li>
<li><p><strong>n_samples</strong> (<em>int</em>) – The number of posterior draws (samples) to obtain from the approximate posterior</p></li>
<li><p><strong>to_numpy</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – Flag indicating whether to return the samples as a <cite>np.array</cite> or a <cite>tf.Tensor</cite>.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Additional keyword arguments passed to the networks</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>post_samples</strong> – the sampled parameters per data set</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor or np.ndarray of shape (n_data_sets, n_samples, n_params)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosterior.sample_loop">
<span class="sig-name descname"><span class="pre">sample_loop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosterior.sample_loop"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosterior.sample_loop" title="Permalink to this definition"></a></dt>
<dd><p>Generates random draws from the approximate posterior given a list of dicts with conditonal variables.
Useful when GPU memory is limited or data sets have a different (non-Tensor) structure.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_list</strong> (<em>list of dictionaries</em><em>, </em><em>each dictionary having the following mandatory keys</em><em>, </em><em>if DEFAULT KEYS unchanged:</em>) – <cite>summary_conditions</cite> : the conditioning variables (including data) that are first passed through a summary network
<cite>direct_conditions</cite>  : the conditioning variables that the directly passed to the inference network</p></li>
<li><p><strong>n_samples</strong> (<em>int</em>) – The number of posterior draws (samples) to obtain from the approximate posterior</p></li>
<li><p><strong>to_numpy</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – Flag indicating whether to return the samples as a <cite>np.darray</cite> or a <cite>tf.Tensor</cite></p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Additional keyword arguments passed to the networks</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>post_samples</strong> – the sampled parameters per data set</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor or np.ndarray of shape (n_datasets, n_samples, n_params)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosteriorLikelihood">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.amortizers.</span></span><span class="sig-name descname"><span class="pre">AmortizedPosteriorLikelihood</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosteriorLikelihood"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosteriorLikelihood" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code>, <a class="reference internal" href="#bayesflow.amortizers.AmortizedTarget" title="bayesflow.amortizers.AmortizedTarget"><code class="xref py py-class docutils literal notranslate"><span class="pre">AmortizedTarget</span></code></a></p>
<p>An interface for jointly learning a surrogate model of the simulator and an approximate
posterior given a generative model.</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosteriorLikelihood.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosteriorLikelihood.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosteriorLikelihood.call" title="Permalink to this definition"></a></dt>
<dd><p>Performs a forward pass through both amortizers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input_dict</strong> (<em>dict</em>) – Input dictionary containing the following mandatory keys:
<cite>posterior_inputs</cite>  - The input dictionary for the amortized posterior
<cite>likelihood_inputs</cite> - The input dictionary for the amortized likelihood</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>(post_out, lik_out)</strong> – The outputs of the posterior and likelihood networks given input variables.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosteriorLikelihood.compute_loss">
<span class="sig-name descname"><span class="pre">compute_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosteriorLikelihood.compute_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosteriorLikelihood.compute_loss" title="Permalink to this definition"></a></dt>
<dd><p>Computes the loss of the join amortizer by summing the corresponding amortized posterior
and likelihood losses.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input_dict</strong> (<em>dict</em>) – <dl class="simple">
<dt>Nested input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged::</dt><dd><p><cite>posterior_inputs</cite>  - The input dictionary for the amortized posterior
<cite>likelihood_inputs</cite> - The input dictionary for the amortized likelihood</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>total_losses</strong> – A dictionary with keys <cite>Post.Loss</cite> and <cite>Lik.Loss</cite> containing the individual losses for the
two amortizers.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosteriorLikelihood.log_likelihood">
<span class="sig-name descname"><span class="pre">log_likelihood</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosteriorLikelihood.log_likelihood"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosteriorLikelihood.log_likelihood" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the approximate log-likelihood of data given conditional variables via
the change-of-variable formula for conditional normalizing flows.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dict</strong> (<em>dict</em>) – <p>Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged:</p>
<p><cite>observables</cite> - the variables over which a condition density is learned (i.e., the observables)
<cite>conditions</cite>  - the conditioning variables that are directly passed to the inference network</p>
<p>OR a nested dictionary with key <cite>likelihood_inputs</cite> containing the above input dictionary</p>
</p></li>
<li><p><strong>to_numpy</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – Flag indicating whether to return the samples as a <cite>np.array</cite> or a <cite>tf.Tensor</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>log_lik</strong> – the approximate log-likelihood of each data point in each data set</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor of shape (batch_size, n_obs)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosteriorLikelihood.log_posterior">
<span class="sig-name descname"><span class="pre">log_posterior</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosteriorLikelihood.log_posterior"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosteriorLikelihood.log_posterior" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the approximate log-posterior of targets given conditional variables via
the change-of-variable formula for conditional normalizing flows.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input_dict</strong> (<em>dict</em>) – <p>Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged:</p>
<p><cite>parameters</cite>         - the latent generative model parameters over which a condition density is learned
<cite>summary_conditions</cite> - the conditioning variables that are first passed through a summary network
<cite>direct_conditions</cite>  - the conditioning variables that the directly passed to the inference network</p>
<p>OR a nested dictionary with key <cite>posterior_inputs</cite> containing the above input dictionary</p>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>log_post</strong> – the approximate log-likelihood of each data point in each data set</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor of shape (batch_size, n_obs)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosteriorLikelihood.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosteriorLikelihood.log_prob"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosteriorLikelihood.log_prob" title="Permalink to this definition"></a></dt>
<dd><p>Identical to calling separate <cite>log_likelihood()</cite> and <cite>log_posterior()</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><strong>out_dict</strong> (dict with keys <cite>log_posterior</cite> and <cite>log_likelihood</cite> corresponding)</p></li>
<li><p><em>to the computed log_pdfs of the approximate posterior and likelihood.</em></p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosteriorLikelihood.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_post_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_lik_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosteriorLikelihood.sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosteriorLikelihood.sample" title="Permalink to this definition"></a></dt>
<dd><p>Identical to calling <cite>sample_parameters()</cite> and <cite>sample_data()</cite> separately.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><strong>out_dict</strong> (dict with keys <cite>posterior_samples</cite> and <cite>likelihood_samples</cite> corresponding)</p></li>
<li><p>to the <cite>n_samples</cite> from the approximate posterior and likelihood, respectively</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosteriorLikelihood.sample_data">
<span class="sig-name descname"><span class="pre">sample_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosteriorLikelihood.sample_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosteriorLikelihood.sample_data" title="Permalink to this definition"></a></dt>
<dd><p>Generates <cite>n_samples</cite> random draws from the surrogate likelihood given input conditions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dict</strong> (<em>dict</em>) – <p>Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged:</p>
<p><cite>conditions</cite> - the conditioning variables that the directly passed to the inference network</p>
<p>OR a nested dictionary with key <cite>likelihood_inputs</cite> containing the above input dictionary</p>
</p></li>
<li><p><strong>n_samples</strong> (<em>int</em>) – The number of posterior samples to obtain from the approximate posterior</p></li>
<li><p><strong>to_numpy</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – Flag indicating whether to return the samples as a <cite>np.array</cite> or a <cite>tf.Tensor</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>lik_samples</strong> – Simulated observables from the surrogate likelihood.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor or np.ndarray of shape (n_datasets, n_samples, None)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedPosteriorLikelihood.sample_parameters">
<span class="sig-name descname"><span class="pre">sample_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_numpy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedPosteriorLikelihood.sample_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedPosteriorLikelihood.sample_parameters" title="Permalink to this definition"></a></dt>
<dd><p>Generates random draws from the approximate posterior given conditonal variables.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dict</strong> (<em>dict</em>) – <p>Input dictionary containing the following mandatory keys, if DEFAULT KEYS unchanged:</p>
<p><cite>summary_conditions</cite> : the conditioning variables (including data) that are first passed through a summary network
<cite>direct_conditions</cite>  : the conditioning variables that the directly passed to the inference network</p>
<p>OR a nested dictionary with key <cite>posterior_inputs</cite> containing the above input dictionary</p>
</p></li>
<li><p><strong>n_samples</strong> (<em>int</em>) – The number of posterior samples to obtain from the approximate posterior</p></li>
<li><p><strong>to_numpy</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – Boolean flag indicating whether to return the samples as a <cite>np.array</cite> or a <cite>tf.Tensor</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>post_samples</strong> – the sampled parameters per data set</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor or np.ndarray of shape (n_datasets, n_samples, n_params)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedTarget">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.amortizers.</span></span><span class="sig-name descname"><span class="pre">AmortizedTarget</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedTarget"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedTarget" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">ABC</span></code></p>
<p>An abstract interface for an amortized learned distribution. Children should
implement the following public methods:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">compute_loss(self,</span> <span class="pre">input_dict,</span> <span class="pre">**kwargs)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample(input_dict,</span> <span class="pre">**kwargs)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_prob(input_dict,</span> <span class="pre">**kwargs)</span></code></p></li>
</ol>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedTarget.compute_loss">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">compute_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedTarget.compute_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedTarget.compute_loss" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedTarget.log_prob">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedTarget.log_prob"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedTarget.log_prob" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.amortizers.AmortizedTarget.sample">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#AmortizedTarget.sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.AmortizedTarget.sample" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.amortizers.SingleModelAmortizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.amortizers.</span></span><span class="sig-name descname"><span class="pre">SingleModelAmortizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/amortizers.html#SingleModelAmortizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.amortizers.SingleModelAmortizer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#bayesflow.amortizers.AmortizedPosterior" title="bayesflow.amortizers.AmortizedPosterior"><code class="xref py py-class docutils literal notranslate"><span class="pre">AmortizedPosterior</span></code></a></p>
<p>Deprecated class for amortizer posterior estimation.</p>
</dd></dl>

</section>
<section id="module-bayesflow.computational_utilities">
<span id="bayesflow-computational-utilities-module"></span><h2>bayesflow.computational_utilities module<a class="headerlink" href="#module-bayesflow.computational_utilities" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.computational_utilities.expected_calibration_error">
<span class="sig-prename descclassname"><span class="pre">bayesflow.computational_utilities.</span></span><span class="sig-name descname"><span class="pre">expected_calibration_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">m_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">m_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">15</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/computational_utilities.html#expected_calibration_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.computational_utilities.expected_calibration_error" title="Permalink to this definition"></a></dt>
<dd><p>Estimates the calibration error of a model comparison network.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Make sure that <code class="docutils literal notranslate"><span class="pre">m_true</span></code> are <strong>one-hot encoded</strong> classes!</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>m_true</strong> (<em>np.array</em><em> or </em><em>list</em>) – True model indices</p></li>
<li><p><strong>m_pred</strong> (<em>np.array</em><em> or </em><em>list</em>) – Predicted model indices</p></li>
<li><p><strong>n_bins</strong> (<em>int</em><em>, </em><em>default: 15</em>) – Number of bins for plot</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>#TODO</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.computational_utilities.gaussian_kernel_matrix">
<span class="sig-prename descclassname"><span class="pre">bayesflow.computational_utilities.</span></span><span class="sig-name descname"><span class="pre">gaussian_kernel_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigmas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/computational_utilities.html#gaussian_kernel_matrix"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.computational_utilities.gaussian_kernel_matrix" title="Permalink to this definition"></a></dt>
<dd><p>Computes a Gaussian radial basis functions (RBFs) between the samples of x and y.</p>
<p>We create a sum of multiple Gaussian kernels each having a width <span class="math notranslate nohighlight">\(\sigma_i\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>tf.Tensor of shape</em><em> (</em><em>num_draws_x</em><em>, </em><em>num_features</em><em>)</em>) – Comprises <cite>num_draws_x</cite> Random draws from the “source” distribution <cite>P</cite>.</p></li>
<li><p><strong>y</strong> (<em>tf.Tensor of shape</em><em> (</em><em>num_draws_y</em><em>, </em><em>num_features</em><em>)</em>) – Comprises <cite>num_draws_y</cite> Random draws from the “source” distribution <cite>Q</cite>.</p></li>
<li><p><strong>sigmas</strong> (<em>list</em><em>(</em><em>float</em><em>)</em><em>, </em><em>optional</em><em>, </em><em>default: None</em>) – List which denotes the widths of each of the gaussians in the kernel.
If <cite>sigmas is None</cite>, a default range will be used, contained in <cite>bayesflow.default_settings.MMD_BANDWIDTH_LIST</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>kernel</strong> – The kernel matrix between pairs from <cite>x</cite> and <cite>y</cite>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor of shape (num_draws_x, num_draws_y)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.computational_utilities.get_coverage_probs">
<span class="sig-prename descclassname"><span class="pre">bayesflow.computational_utilities.</span></span><span class="sig-name descname"><span class="pre">get_coverage_probs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">u</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/computational_utilities.html#get_coverage_probs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.computational_utilities.get_coverage_probs" title="Permalink to this definition"></a></dt>
<dd><p>Vectorized function to compute the minimal coverage probability for uniform
ECDFs given evaluation points z and a sample of samples u.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>z</strong> (<em>np.ndarray of shape</em><em> (</em><em>num_points</em><em>, </em><em>)</em>) – The vector of evaluation points.</p></li>
<li><p><strong>u</strong> (<em>np.ndarray of shape</em><em> (</em><em>num_simulations</em><em>, </em><em>num_samples</em><em>)</em>) – The matrix of simulated draws (samples) from U(0, 1)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.computational_utilities.inverse_multiquadratic_kernel_matrix">
<span class="sig-prename descclassname"><span class="pre">bayesflow.computational_utilities.</span></span><span class="sig-name descname"><span class="pre">inverse_multiquadratic_kernel_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigmas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/computational_utilities.html#inverse_multiquadratic_kernel_matrix"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.computational_utilities.inverse_multiquadratic_kernel_matrix" title="Permalink to this definition"></a></dt>
<dd><p>Computes an inverse multiquadratic RBF between the samples of x and y.</p>
<p>We create a sum of multiple IM-RBF kernels each having a width <span class="math notranslate nohighlight">\(\sigma_i\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>tf.Tensor of shape</em><em> (</em><em>num_draws_x</em><em>, </em><em>num_features</em><em>)</em>) – Comprises <cite>num_draws_x</cite> Random draws from the “source” distribution <cite>P</cite>.</p></li>
<li><p><strong>y</strong> (<em>tf.Tensor of shape</em><em> (</em><em>num_draws_y</em><em>, </em><em>num_features</em><em>)</em>) – Comprises <cite>num_draws_y</cite> Random draws from the “source” distribution <cite>Q</cite>.</p></li>
<li><p><strong>sigmas</strong> (<em>list</em><em>(</em><em>float</em><em>)</em><em>, </em><em>optional</em><em>, </em><em>default: None</em>) – List which denotes the widths of each of the gaussians in the kernel.
If <cite>sigmas is None</cite>, a default range will be used, contained in <cite>bayesflow.default_settings.MMD_BANDWIDTH_LIST</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>kernel</strong> – The kernel matrix between pairs from <cite>x</cite> and <cite>y</cite>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor of shape (num_draws_x, num_draws_y)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.computational_utilities.maximum_mean_discrepancy">
<span class="sig-prename descclassname"><span class="pre">bayesflow.computational_utilities.</span></span><span class="sig-name descname"><span class="pre">maximum_mean_discrepancy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gaussian'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mmd_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minimum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/computational_utilities.html#maximum_mean_discrepancy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.computational_utilities.maximum_mean_discrepancy" title="Permalink to this definition"></a></dt>
<dd><p>Computes the MMD given a particular choice of kernel.</p>
<p>For details, consult Gretton et al. (2012):
<a class="reference external" href="https://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf">https://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source_samples</strong> (<em>tf.Tensor of shape</em><em> (</em><em>N</em><em>, </em><em>num_features</em><em>)</em>) – An array of <cite>N</cite> random draws from the “source” distribution.</p></li>
<li><p><strong>target_samples</strong> (<em>tf.Tensor of shape</em><em>  (</em><em>M</em><em>, </em><em>num_features</em><em>)</em>) – An array of <cite>M</cite> random draws from the “target” distribution.</p></li>
<li><p><strong>kernel</strong> (<em>str in</em><em> (</em><em>'gaussian'</em><em>, </em><em>'inverse_multiquadratic'</em><em>)</em><em>, </em><em>optional</em><em>, </em><em>default: 'gaussian'</em>) – The kernel to use for computing the distance between pairs of random draws.</p></li>
<li><p><strong>mmd_weight</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default: 1.0</em>) – The weight of the MMD value.</p></li>
<li><p><strong>minimum</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default: 0.0</em>) – The lower bound of the MMD value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss_value</strong> – A scalar Maximum Mean Discrepancy, shape (,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.computational_utilities.mmd_kernel">
<span class="sig-prename descclassname"><span class="pre">bayesflow.computational_utilities.</span></span><span class="sig-name descname"><span class="pre">mmd_kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/computational_utilities.html#mmd_kernel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.computational_utilities.mmd_kernel" title="Permalink to this definition"></a></dt>
<dd><p>Computes the estimator of the Maximum Mean Discrepancy (MMD) between two samples: x and y.</p>
<p>Maximum Mean Discrepancy (MMD) is a distance-measure between random draws from
the distributions <cite>x ~ P</cite> and <cite>y ~ Q</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>tf.Tensor of shape</em><em> (</em><em>N</em><em>, </em><em>num_features</em><em>)</em>) – An array of <cite>N</cite> random draws from the “source” distribution <cite>x ~ P</cite>.</p></li>
<li><p><strong>y</strong> (<em>tf.Tensor of shape</em><em> (</em><em>M</em><em>, </em><em>num_features</em><em>)</em>) – An array of <cite>M</cite> random draws from the “target” distribution <cite>y ~ Q</cite>.</p></li>
<li><p><strong>kernel</strong> (<em>callable</em>) – A function which computes the distance between pairs of samples.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – The statistically biased squared maximum mean discrepancy (MMD) value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor of shape (,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.computational_utilities.mmd_kernel_unbiased">
<span class="sig-prename descclassname"><span class="pre">bayesflow.computational_utilities.</span></span><span class="sig-name descname"><span class="pre">mmd_kernel_unbiased</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/computational_utilities.html#mmd_kernel_unbiased"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.computational_utilities.mmd_kernel_unbiased" title="Permalink to this definition"></a></dt>
<dd><p>Computes the unbiased estimator of the Maximum Mean Discrepancy (MMD) between two samples: x and y.
Maximum Mean Discrepancy (MMD) is a distance-measure between the samples of the distributions <cite>x ~ P</cite> and <cite>y ~ Q</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>tf.Tensor of shape</em><em> (</em><em>N</em><em>, </em><em>num_features</em><em>)</em>) – An array of <cite>N</cite> random draws from the “source” distribution <cite>x ~ P</cite>.</p></li>
<li><p><strong>y</strong> (<em>tf.Tensor of shape</em><em> (</em><em>M</em><em>, </em><em>num_features</em><em>)</em>) – An array of <cite>M</cite> random draws from the “target” distribution <cite>y ~ Q</cite>.</p></li>
<li><p><strong>kernel</strong> (<em>callable</em>) – A function which computes the distance between pairs of random draws from <cite>x</cite> and <cite>y</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – The statistically unbiaserd squared maximum mean discrepancy (MMD) value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor of shape (,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.computational_utilities.simultaneous_ecdf_bands">
<span class="sig-prename descclassname"><span class="pre">bayesflow.computational_utilities.</span></span><span class="sig-name descname"><span class="pre">simultaneous_ecdf_bands</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_points</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_simulations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.95</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_num_points</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/computational_utilities.html#simultaneous_ecdf_bands"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.computational_utilities.simultaneous_ecdf_bands" title="Permalink to this definition"></a></dt>
<dd><p>Computes the simultaneous ECDF bands through simulation according to
the algorithm described in Section 2.2:</p>
<p><a class="reference external" href="https://link.springer.com/content/pdf/10.1007/s11222-022-10090-6.pdf">https://link.springer.com/content/pdf/10.1007/s11222-022-10090-6.pdf</a></p>
<p>Depends on the vectorized utility function <cite>get_coverage_probs(z, u)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_samples</strong> (<em>int</em>) – The sample size used for computing the ECDF. Will equal to the number of posterior
samples when used for calibrarion. Corresponds to <cite>N</cite> in the paper above.</p></li>
<li><p><strong>num_points</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: None</em>) – The number of evaluation points on the interval (0, 1). Defaults to <cite>num_points = num_samples</cite> if
not explicitly specified. Correspond to <cite>K</cite> in the paper above.</p></li>
<li><p><strong>num_simulations</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 1000</em>) – The number of samples of size <cite>n_samples</cite> to simulate for determining the simultaneous CIs.</p></li>
<li><p><strong>confidence</strong> (<em>float in</em><em> (</em><em>0</em><em>, </em><em>1</em><em>)</em><em>, </em><em>optional</em><em>, </em><em>default: 0.95</em>) – The confidence level, <cite>confidence = 1 - alpha</cite> specifies the width of the confidence interval.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default: 1e-5</em>) – Small number to add to the lower and subtract from the upper bound of the interval [0, 1]
to avoid edge artefacts. No need to touch this.</p></li>
<li><p><strong>max_num_points</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 1000</em>) – Upper bound on <cite>num_points</cite>. Saves computation time when <cite>num_samples</cite> is large.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the evaluation points, the lower, and the upper confidence bands, respectively.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(alpha, z, L, U) - tuple of scalar and three arrays of size (num_samples,) containing the confidence level as well as</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-bayesflow.configuration">
<span id="bayesflow-configuration-module"></span><h2>bayesflow.configuration module<a class="headerlink" href="#module-bayesflow.configuration" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.configuration.DefaultJointConfigurator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.configuration.</span></span><span class="sig-name descname"><span class="pre">DefaultJointConfigurator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">default_float_type=&lt;class</span> <span class="pre">'numpy.float32'&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/configuration.html#DefaultJointConfigurator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.configuration.DefaultJointConfigurator" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Fallback class for a generic configrator for joint posterior and likelihood approximation.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.configuration.DefaultLikelihoodConfigurator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.configuration.</span></span><span class="sig-name descname"><span class="pre">DefaultLikelihoodConfigurator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">default_float_type=&lt;class</span> <span class="pre">'numpy.float32'&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/configuration.html#DefaultLikelihoodConfigurator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.configuration.DefaultLikelihoodConfigurator" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Fallback class for a generic configrator for amortized likelihood approximation.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.configuration.DefaultModelComparisonConfigurator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.configuration.</span></span><span class="sig-name descname"><span class="pre">DefaultModelComparisonConfigurator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_models</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_float_type=&lt;class</span> <span class="pre">'numpy.float32'&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/configuration.html#DefaultModelComparisonConfigurator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.configuration.DefaultModelComparisonConfigurator" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Fallback class for a default configurator for amortized model comparison.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.configuration.DefaultPosteriorConfigurator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.configuration.</span></span><span class="sig-name descname"><span class="pre">DefaultPosteriorConfigurator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">default_float_type=&lt;class</span> <span class="pre">'numpy.float32'&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/configuration.html#DefaultPosteriorConfigurator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.configuration.DefaultPosteriorConfigurator" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Fallback class for a generic configrator for amortized posterior approximation.</p>
</dd></dl>

</section>
<section id="module-bayesflow.coupling_networks">
<span id="bayesflow-coupling-networks-module"></span><h2>bayesflow.coupling_networks module<a class="headerlink" href="#module-bayesflow.coupling_networks" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.coupling_networks.AffineCouplingLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.coupling_networks.</span></span><span class="sig-name descname"><span class="pre">AffineCouplingLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/coupling_networks.html#AffineCouplingLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.coupling_networks.AffineCouplingLayer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code></p>
<p>Implements a conditional version of the INN coupling layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.coupling_networks.AffineCouplingLayer.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_or_z</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">condition</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inverse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/coupling_networks.html#AffineCouplingLayer.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.coupling_networks.AffineCouplingLayer.call" title="Permalink to this definition"></a></dt>
<dd><p>Performs one pass through a the affine coupling layer (either inverse or forward).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target_or_z</strong> (<em>tf.Tensor</em>) – The estimation quantites of interest or latent representations z ~ p(z), shape (batch_size, …)</p></li>
<li><p><strong>condition</strong> (<em>tf.Tensor</em><em> or </em><em>None</em>) – The conditioning data of interest, for instance, x = summary_fun(x), shape (batch_size, …).
If <cite>condition is None</cite>, then the layer recuces to an unconditional ACL.</p></li>
<li><p><strong>inverse</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – Flag indicating whether to run the block forward or backward.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>(z, log_det_J)</strong> (<em>tuple(tf.Tensor, tf.Tensor)</em>) – If inverse=False: The transformed input and the corresponding Jacobian of the transformation,
z shape: (batch_size, inp_dim), log_det_J shape: (batch_size, )</p></li>
<li><p><strong>target</strong> (<em>tf.Tensor</em>) – If inverse=True: The back-transformed z, shape (batch_size, inp_dim)</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If <code class="docutils literal notranslate"><span class="pre">inverse=False</span></code>, the return is <code class="docutils literal notranslate"><span class="pre">(z,</span> <span class="pre">log_det_J)</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">inverse=True</span></code>, the return is <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.coupling_networks.AffineCouplingLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">condition</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/coupling_networks.html#AffineCouplingLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.coupling_networks.AffineCouplingLayer.forward" title="Permalink to this definition"></a></dt>
<dd><p>Performs a forward pass through a coupling layer with an optinal <cite>Permutation</cite> and <cite>ActNorm</cite> layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target</strong> (<em>tf.Tensor</em>) – The estimation quantities of interest, for instance, parameter vector of shape (batch_size, theta_dim)</p></li>
<li><p><strong>condition</strong> (<em>tf.Tensor</em><em> or </em><em>None</em>) – The conditioning vector of interest, for instance, x = summary(x), shape (batch_size, summary_dim)
If <cite>None</cite>, transformation amounts to unconditional estimation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>(z, log_det_J)</strong> – The transformed input and the corresponding Jacobian of the transformation.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple(tf.Tensor, tf.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.coupling_networks.AffineCouplingLayer.inverse">
<span class="sig-name descname"><span class="pre">inverse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">condition</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/coupling_networks.html#AffineCouplingLayer.inverse"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.coupling_networks.AffineCouplingLayer.inverse" title="Permalink to this definition"></a></dt>
<dd><p>Performs an inverse pass through a coupling layer with an optinal <cite>Permutation</cite> and <cite>ActNorm</cite> layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>z</strong> (<em>tf.Tensor</em>) – latent variables z ~ p(z), shape (batch_size, theta_dim)</p></li>
<li><p><strong>condition</strong> (<em>tf.Tensor</em><em> or </em><em>None</em>) – The conditioning vector of interest, for instance, x = summary(x), shape (batch_size, summary_dim).
If <cite>None</cite>, transformation amounts to unconditional estimation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>target</strong> – The back-transformed latent variable z.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-bayesflow.default_settings">
<span id="bayesflow-default-settings-module"></span><h2>bayesflow.default_settings module<a class="headerlink" href="#module-bayesflow.default_settings" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.default_settings.MetaDictSetting">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.default_settings.</span></span><span class="sig-name descname"><span class="pre">MetaDictSetting</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">meta_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mandatory_fields</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/default_settings.html#MetaDictSetting"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.default_settings.MetaDictSetting" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#bayesflow.default_settings.Setting" title="bayesflow.default_settings.Setting"><code class="xref py py-class docutils literal notranslate"><span class="pre">Setting</span></code></a></p>
<p>Implements an interface for a default meta_dict with optional mandatory fields.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.default_settings.Setting">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.default_settings.</span></span><span class="sig-name descname"><span class="pre">Setting</span></span><a class="reference internal" href="_modules/bayesflow/default_settings.html#Setting"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.default_settings.Setting" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">ABC</span></code></p>
<p>Abstract base class for settings. It’s here to potentially extend the setting functionality in future.</p>
</dd></dl>

</section>
<section id="module-bayesflow.diagnostics">
<span id="bayesflow-diagnostics-module"></span><h2>bayesflow.diagnostics module<a class="headerlink" href="#module-bayesflow.diagnostics" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.diagnostics.plot_calibration_curves">
<span class="sig-prename descclassname"><span class="pre">bayesflow.diagnostics.</span></span><span class="sig-name descname"><span class="pre">plot_calibration_curves</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">m_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">m_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">font_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fig_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(12,</span> <span class="pre">4)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/diagnostics.html#plot_calibration_curves"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.diagnostics.plot_calibration_curves" title="Permalink to this definition"></a></dt>
<dd><p>Plots the calibration curves and the ECE for a model comparison problem. Depends on the
<cite>expected_calibration_error</cite> function for computing the ECE.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>TODO</strong> – </p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.diagnostics.plot_latent_space_2d">
<span class="sig-prename descclassname"><span class="pre">bayesflow.diagnostics.</span></span><span class="sig-name descname"><span class="pre">plot_latent_space_2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">height</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'#8f2727'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/diagnostics.html#plot_latent_space_2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.diagnostics.plot_latent_space_2d" title="Permalink to this definition"></a></dt>
<dd><p>Creates pairplots for the latent space learned by the inference network. Enables
visual inspection of the the latent space and whether its structrue corresponds to the
one enforced by the optimization criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>z_samples</strong> (<em>np.ndarray</em><em> or </em><em>tf.Tensor of shape</em><em> (</em><em>n_sim</em><em>, </em><em>n_params</em><em>)</em>) – The latent samples computed through a forward pass of the inference network.</p></li>
<li><p><strong>height</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default: 2.5</em>) – The height of the pair plot.</p></li>
<li><p><strong>color</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>defailt : '#8f2727'</em>) – The color of the plot</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Additional keyword arguments passed to the sns.PairGrid constructor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>plt.Figure - the figure instance for optional saving</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.diagnostics.plot_losses">
<span class="sig-prename descclassname"><span class="pre">bayesflow.diagnostics.</span></span><span class="sig-name descname"><span class="pre">plot_losses</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">history</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fig_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'#8f2727'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label_fontsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">14</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title_fontsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/diagnostics.html#plot_losses"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.diagnostics.plot_losses" title="Permalink to this definition"></a></dt>
<dd><p>A generic helper function to plot the losses of a series of training epochs and runs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>history</strong> (<em>pd.DataFrame</em><em> or </em><em>bayesflow.LossHistory object</em>) – The (plottable) history as returned by a train_[…] method of a <cite>Trainer</cite> instance.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>plt.Figure - the figure instance for optional saving</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.diagnostics.plot_posterior_2d">
<span class="sig-prename descclassname"><span class="pre">bayesflow.diagnostics.</span></span><span class="sig-name descname"><span class="pre">plot_posterior_2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">posterior_draws</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_draws</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">height</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">legend_fontsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">14</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">post_color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'#8f2727'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gray'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">post_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.7</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/diagnostics.html#plot_posterior_2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.diagnostics.plot_posterior_2d" title="Permalink to this definition"></a></dt>
<dd><p>Generates a bivariate pairplot given posterior draws and optional prior or prior draws.</p>
<dl class="simple">
<dt>posterior_draws<span class="classifier">np.ndarray of shape (n_post_draws, n_params)</span></dt><dd><p>The posterior draws obtained for a SINGLE observed data set.</p>
</dd>
<dt>prior<span class="classifier">bayesflow.forward_inference.Prior instance or None, optional, default: None</span></dt><dd><p>The optional prior object having an input-output signature as given by ayesflow.forward_inference.Prior</p>
</dd>
<dt>prior_draws<span class="classifier">np.ndarray of shape (n_prior_draws, n_params) or None, optonal (default: None)</span></dt><dd><p>The optional prior draws obtained from the prior. If both prior and prior_draws are provided, prior_draws
will be used.</p>
</dd>
<dt>param_names<span class="classifier">list or None, optional, default: None</span></dt><dd><p>The parameter names for nice plot titles. Inferred if None</p>
</dd>
<dt>height<span class="classifier">float, optional, default: 3.</span></dt><dd><p>The height of the pairplot.</p>
</dd>
<dt>legend_fontsize<span class="classifier">int, optional, default: 14</span></dt><dd><p>The font size of the legend text.</p>
</dd>
<dt>post_color<span class="classifier">str, optional, default: ‘#8f2727’</span></dt><dd><p>The color for the posterior histograms and KDEs.</p>
</dd>
<dt>priors_color<span class="classifier">str, optional, default: gray</span></dt><dd><p>The color for the optional prior histograms and KDEs.</p>
</dd>
<dt>post_alpha<span class="classifier">float in [0, 1], optonal, default: 0.9</span></dt><dd><p>The opacity of the posterior plots.</p>
</dd>
<dt>prior_alpha<span class="classifier">float in [0, 1], optonal, default: 0.7</span></dt><dd><p>The opacity of the prior plots.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>f</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>plt.Figure - the figure instance for optional saving</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>AssertionError</strong> – If the shape of posterior_draws is not 2-dimensional.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.diagnostics.plot_prior2d">
<span class="sig-prename descclassname"><span class="pre">bayesflow.diagnostics.</span></span><span class="sig-name descname"><span class="pre">plot_prior2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prior</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">height</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'#8f2727'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/diagnostics.html#plot_prior2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.diagnostics.plot_prior2d" title="Permalink to this definition"></a></dt>
<dd><p>Creates pairplots for a given joint prior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prior</strong> (<em>callable</em>) – The prior object which takes a single integer argument and generates random draws.</p></li>
<li><p><strong>param_names</strong> (<em>list of str</em><em> or </em><em>None</em><em>, </em><em>optional</em><em>, </em><em>default None</em>) – An optional list of strings which</p></li>
<li><p><strong>n_samples</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 1000</em>) – The number of random draws from the joint prior</p></li>
<li><p><strong>height</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default: 2.5</em>) – The height of the pair plot</p></li>
<li><p><strong>color</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>defailt : '#8f2727'</em>) – The color of the plot</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Additional keyword arguments passed to the sns.PairGrid constructor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>plt.Figure - the figure instance for optional saving</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.diagnostics.plot_recovery">
<span class="sig-prename descclassname"><span class="pre">bayesflow.diagnostics.</span></span><span class="sig-name descname"><span class="pre">plot_recovery</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">post_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">point_agg=&lt;function</span> <span class="pre">mean&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">uncertainty_agg=&lt;function</span> <span class="pre">std&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_names=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fig_size=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label_fontsize=14</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title_fontsize=16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_fontsize=16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_corr=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_r2=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color='#8f2727'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_col=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_row=None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/diagnostics.html#plot_recovery"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.diagnostics.plot_recovery" title="Permalink to this definition"></a></dt>
<dd><p>Creates and plots publication-ready recovery plot with true vs. point estimate + uncertainty.
The point estimate can be controlled with the <cite>point_agg</cite> argument, and the uncertainty estimate
can be controlled with the <cite>uncertainty_agg</cite> argument.</p>
<p>This plot yields the same information as the “posterior z-score”:</p>
<p><a class="reference external" href="https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html">https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html</a></p>
<p>Important: Posterior aggregates play no special role in Bayesian inference and should only
be used heuristically. For instanec, in the case of multi-modal posteriors, common point
estimates, such as mean, (geometric) median, or maximum a posteriori (MAP) mean nothing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>post_samples</strong> (<em>np.ndarray of shape</em><em> (</em><em>n_data_sets</em><em>, </em><em>n_post_draws</em><em>, </em><em>n_params</em><em>)</em>) – The posterior draws obtained from n_data_sets</p></li>
<li><p><strong>prior_samples</strong> (<em>np.ndarray of shape</em><em> (</em><em>n_data_sets</em><em>, </em><em>n_params</em><em>)</em>) – The prior draws (true parameters) obtained for generating the n_data_sets</p></li>
<li><p><strong>point_agg</strong> (<em>callable</em><em>, </em><em>optional</em><em>, </em><em>default: np.mean</em>) – The function to apply to the posterior draws to get a point estimate for each marginal.</p></li>
<li><p><strong>uncertainty_agg</strong> (<em>callable</em><em> or </em><em>None</em><em>, </em><em>optional</em><em>, </em><em>default: np.std</em>) – The function to apply to the posterior draws to get an uncertainty estimate.
If <cite>None</cite> provided, a simple scatter will be plotted.</p></li>
<li><p><strong>param_names</strong> (<em>list</em><em> or </em><em>None</em><em>, </em><em>optional</em><em>, </em><em>default: None</em>) – The parameter names for nice plot titles. Inferred if None</p></li>
<li><p><strong>fig_size</strong> (<em>tuple</em><em> or </em><em>None</em><em>, </em><em>optional</em><em>, </em><em>default : None</em>) – The figure size passed to the matplotlib constructor. Inferred if None.</p></li>
<li><p><strong>label_fontsize</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 14</em>) – The font size of the y-label text</p></li>
<li><p><strong>title_fontsize</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 16</em>) – The font size of the title text</p></li>
<li><p><strong>metric_fontsize</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 16</em>) – The font size of the goodness-of-fit metric (if provided)</p></li>
<li><p><strong>add_corr</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – A flag for adding correlation between true and estimates to the plot.</p></li>
<li><p><strong>add_r2</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – A flag for adding R^2 between true and estimates to the plot.</p></li>
<li><p><strong>color</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default: '#8f2727'</em>) – The color for the true vs. estimated scatter points and errobars.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>plt.Figure - the figure instance for optional saving</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference internal" href="#bayesflow.exceptions.ShapeError" title="bayesflow.exceptions.ShapeError"><strong>ShapeError</strong></a> – If there is a deviation form the expected shapes of <cite>post_samples</cite> and <cite>prior_samples</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.diagnostics.plot_sbc_ecdf">
<span class="sig-prename descclassname"><span class="pre">bayesflow.diagnostics.</span></span><span class="sig-name descname"><span class="pre">plot_sbc_ecdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">post_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">difference</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stacked</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fig_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label_fontsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">14</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">legend_fontsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">14</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title_fontsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_ecdf_color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'#a34f4f'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'grey'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/diagnostics.html#plot_sbc_ecdf"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.diagnostics.plot_sbc_ecdf" title="Permalink to this definition"></a></dt>
<dd><p>Creates the empirical CDFs for each marginal rank distribution and plots it against
a uniform ECDF. ECDF simultaneous bands are drawn using simulations from the uniform. Inspired by:</p>
<p>[1] Säilynoja, T., Bürkner, P. C., &amp; Vehtari, A. (2022). Graphical test for discrete uniformity and
its applications in goodness-of-fit evaluation and multiple sample comparison. Statistics and Computing,
32(2), 1-21. <a class="reference external" href="https://arxiv.org/abs/2103.10522">https://arxiv.org/abs/2103.10522</a></p>
<p>For models with many parameters, use <cite>stacked=True</cite> to obtain an idea of the overall calibration
of a posterior approximator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>post_samples</strong> (<em>np.ndarray of shape</em><em> (</em><em>n_data_sets</em><em>, </em><em>n_post_draws</em><em>, </em><em>n_params</em><em>)</em>) – The posterior draws obtained from n_data_sets</p></li>
<li><p><strong>prior_samples</strong> (<em>np.ndarray of shape</em><em> (</em><em>n_data_sets</em><em>, </em><em>n_params</em><em>)</em>) – The prior draws obtained for generating n_data_sets</p></li>
<li><p><strong>difference</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – If <cite>True</cite>, plots the ECDF difference. Enables a more dynamic visualization range.</p></li>
<li><p><strong>stacked</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – If <cite>True</cite>, all ECDFs will be plotted on the same plot. If <cite>False</cite>, each ECDF will
have its own subplot, similar to the behavior of <cite>plot_sbc_histograms</cite>.</p></li>
<li><p><strong>param_names</strong> (<em>list</em><em> or </em><em>None</em><em>, </em><em>optional</em><em>, </em><em>default: None</em>) – The parameter names for nice plot titles. Inferred if None. Only relevant if <cite>stacked=False</cite>.</p></li>
<li><p><strong>fig_size</strong> (<em>tuple</em><em> or </em><em>None</em><em>, </em><em>optional</em><em>, </em><em>default: None</em>) – The figure size passed to the matplotlib constructor. Inferred if None.</p></li>
<li><p><strong>label_fontsize</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 14</em>) – The font size of the y-label and y-label texts</p></li>
<li><p><strong>legend_fontsize</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 14</em>) – The font size of the legend text</p></li>
<li><p><strong>title_fontsize</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 16</em>) – The font size of the title text. Only relevant if <cite>stacked=False</cite></p></li>
<li><p><strong>rank_ecdf_color</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default: '#a34f4f'</em>) – The color to use for the rank ECDFs</p></li>
<li><p><strong>fill_color</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default: 'grey'</em>) – The color of the fill arguments.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em><em>, </em><em>default: {}</em>) – Keyword arguments can be passed to control the behavior of ECDF simultaneous band computation
through the <cite>ecdf_bands_kwargs</cite> dictionary. See <cite>simultaneous_ecdf_bands</cite> for keyword arguments</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>plt.Figure - the figure instance for optional saving</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference internal" href="#bayesflow.exceptions.ShapeError" title="bayesflow.exceptions.ShapeError"><strong>ShapeError</strong></a> – If there is a deviation form the expected shapes of <cite>post_samples</cite> and <cite>prior_samples</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.diagnostics.plot_sbc_histograms">
<span class="sig-prename descclassname"><span class="pre">bayesflow.diagnostics.</span></span><span class="sig-name descname"><span class="pre">plot_sbc_histograms</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">post_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fig_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">binomial_interval</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label_fontsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">14</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title_fontsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hist_color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'#a34f4f'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/diagnostics.html#plot_sbc_histograms"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.diagnostics.plot_sbc_histograms" title="Permalink to this definition"></a></dt>
<dd><p>Creates and plots publication-ready histograms of rank statistics for simulation-based calibration
(SBC) checks according to:</p>
<p>[1] Talts, S., Betancourt, M., Simpson, D., Vehtari, A., &amp; Gelman, A. (2018).
Validating Bayesian inference algorithms with simulation-based calibration.
arXiv preprint arXiv:1804.06788.</p>
<p>Any deviation from uniformity indicates miscalibration and thus poor convergence
of the networks or poor combination between generative model / networks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>post_samples</strong> (<em>np.ndarray of shape</em><em> (</em><em>n_data_sets</em><em>, </em><em>n_post_draws</em><em>, </em><em>n_params</em><em>)</em>) – The posterior draws obtained from n_data_sets</p></li>
<li><p><strong>prior_samples</strong> (<em>np.ndarray of shape</em><em> (</em><em>n_data_sets</em><em>, </em><em>n_params</em><em>)</em>) – The prior draws obtained for generating n_data_sets</p></li>
<li><p><strong>param_names</strong> (<em>list</em><em> or </em><em>None</em><em>, </em><em>optional</em><em>, </em><em>default: None</em>) – The parameter names for nice plot titles. Inferred if None</p></li>
<li><p><strong>fig_size</strong> (<em>tuple</em><em> or </em><em>None</em><em>, </em><em>optional</em><em>, </em><em>default : None</em>) – The figure size passed to the matplotlib constructor. Inferred if None.</p></li>
<li><p><strong>num_bins</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 10</em>) – The number of bins to use for each marginal histogram</p></li>
<li><p><strong>binomial_interval</strong> (<em>float in</em><em> (</em><em>0</em><em>, </em><em>1</em><em>)</em><em>, </em><em>optional</em><em>, </em><em>default: 0.95</em>) – The width of the confidence interval for the binomial distribution</p></li>
<li><p><strong>label_fontsize</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 14</em>) – The font size of the y-label text</p></li>
<li><p><strong>title_fontsize</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 16</em>) – The font size of the title text</p></li>
<li><p><strong>hist_color</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default '#a34f4f'</em>) – The color to use for the histogram body</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>plt.Figure - the figure instance for optional saving</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference internal" href="#bayesflow.exceptions.ShapeError" title="bayesflow.exceptions.ShapeError"><strong>ShapeError</strong></a> – If there is a deviation form the expected shapes of <cite>post_samples</cite> and <cite>prior_samples</cite>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-bayesflow.exceptions">
<span id="bayesflow-exceptions-module"></span><h2>bayesflow.exceptions module<a class="headerlink" href="#module-bayesflow.exceptions" title="Permalink to this heading"></a></h2>
<dl class="py exception">
<dt class="sig sig-object py" id="bayesflow.exceptions.ConfigurationError">
<em class="property"><span class="pre">exception</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.exceptions.</span></span><span class="sig-name descname"><span class="pre">ConfigurationError</span></span><a class="reference internal" href="_modules/bayesflow/exceptions.html#ConfigurationError"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.exceptions.ConfigurationError" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Exception</span></code></p>
<p>Class for error in model configuration, e.g. in meta dict</p>
</dd></dl>

<dl class="py exception">
<dt class="sig sig-object py" id="bayesflow.exceptions.InferenceError">
<em class="property"><span class="pre">exception</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.exceptions.</span></span><span class="sig-name descname"><span class="pre">InferenceError</span></span><a class="reference internal" href="_modules/bayesflow/exceptions.html#InferenceError"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.exceptions.InferenceError" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Exception</span></code></p>
<p>Class for error in forward/inverse pass of a neural components.</p>
</dd></dl>

<dl class="py exception">
<dt class="sig sig-object py" id="bayesflow.exceptions.LossError">
<em class="property"><span class="pre">exception</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.exceptions.</span></span><span class="sig-name descname"><span class="pre">LossError</span></span><a class="reference internal" href="_modules/bayesflow/exceptions.html#LossError"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.exceptions.LossError" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Exception</span></code></p>
<p>Class for error in applying loss.</p>
</dd></dl>

<dl class="py exception">
<dt class="sig sig-object py" id="bayesflow.exceptions.OperationNotSupportedError">
<em class="property"><span class="pre">exception</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.exceptions.</span></span><span class="sig-name descname"><span class="pre">OperationNotSupportedError</span></span><a class="reference internal" href="_modules/bayesflow/exceptions.html#OperationNotSupportedError"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.exceptions.OperationNotSupportedError" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Exception</span></code></p>
<p>Class for error that occurs when an operation is demanded but not supported,
e.g. when a trainer is initialized without generative model but the user demands it to simulate data.</p>
</dd></dl>

<dl class="py exception">
<dt class="sig sig-object py" id="bayesflow.exceptions.ShapeError">
<em class="property"><span class="pre">exception</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.exceptions.</span></span><span class="sig-name descname"><span class="pre">ShapeError</span></span><a class="reference internal" href="_modules/bayesflow/exceptions.html#ShapeError"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.exceptions.ShapeError" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Exception</span></code></p>
<p>Class for error in expected shappes.</p>
</dd></dl>

<dl class="py exception">
<dt class="sig sig-object py" id="bayesflow.exceptions.SimulationError">
<em class="property"><span class="pre">exception</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.exceptions.</span></span><span class="sig-name descname"><span class="pre">SimulationError</span></span><a class="reference internal" href="_modules/bayesflow/exceptions.html#SimulationError"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.exceptions.SimulationError" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Exception</span></code></p>
<p>Class for an error in simulation.</p>
</dd></dl>

<dl class="py exception">
<dt class="sig sig-object py" id="bayesflow.exceptions.SummaryStatsError">
<em class="property"><span class="pre">exception</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.exceptions.</span></span><span class="sig-name descname"><span class="pre">SummaryStatsError</span></span><a class="reference internal" href="_modules/bayesflow/exceptions.html#SummaryStatsError"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.exceptions.SummaryStatsError" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Exception</span></code></p>
<p>Class for error in summary statistics.</p>
</dd></dl>

</section>
<section id="module-bayesflow.helper_classes">
<span id="bayesflow-helper-classes-module"></span><h2>bayesflow.helper_classes module<a class="headerlink" href="#module-bayesflow.helper_classes" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.helper_classes.LossHistory">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.helper_classes.</span></span><span class="sig-name descname"><span class="pre">LossHistory</span></span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#LossHistory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.LossHistory" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Helper class to keep track of losses during training.</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.LossHistory.add_entry">
<span class="sig-name descname"><span class="pre">add_entry</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">current_loss</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#LossHistory.add_entry"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.LossHistory.add_entry" title="Permalink to this definition"></a></dt>
<dd><p>Adds loss entry for current epoch into internal memory data structure.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bayesflow.helper_classes.LossHistory.file_name">
<span class="sig-name descname"><span class="pre">file_name</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'history'</span></em><a class="headerlink" href="#bayesflow.helper_classes.LossHistory.file_name" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.LossHistory.flush">
<span class="sig-name descname"><span class="pre">flush</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#LossHistory.flush"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.LossHistory.flush" title="Permalink to this definition"></a></dt>
<dd><p>Returns current history and removes all existing loss history.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.LossHistory.get_copy">
<span class="sig-name descname"><span class="pre">get_copy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#LossHistory.get_copy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.LossHistory.get_copy" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.LossHistory.get_plottable">
<span class="sig-name descname"><span class="pre">get_plottable</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#LossHistory.get_plottable"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.LossHistory.get_plottable" title="Permalink to this definition"></a></dt>
<dd><p>Returns the losses as a nicely formatted pandas DataFrame.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.LossHistory.get_running_losses">
<span class="sig-name descname"><span class="pre">get_running_losses</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#LossHistory.get_running_losses"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.LossHistory.get_running_losses" title="Permalink to this definition"></a></dt>
<dd><p>Compute and return running means of the losses for current epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.LossHistory.load_from_file">
<span class="sig-name descname"><span class="pre">load_from_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#LossHistory.load_from_file"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.LossHistory.load_from_file" title="Permalink to this definition"></a></dt>
<dd><p>Loads the most recent saved <cite>LossHistory</cite> object from <cite>file_path</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.LossHistory.save_to_file">
<span class="sig-name descname"><span class="pre">save_to_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_to_keep</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#LossHistory.save_to_file"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.LossHistory.save_to_file" title="Permalink to this definition"></a></dt>
<dd><p>Saves a <cite>LossHistory</cite> object to a pickled dictionary in file_path.
If max_to_keep saved loss history files are found in file_path, the oldest is deleted before a new one is saved.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.LossHistory.start_new_run">
<span class="sig-name descname"><span class="pre">start_new_run</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#LossHistory.start_new_run"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.LossHistory.start_new_run" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="bayesflow.helper_classes.LossHistory.total_loss">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">total_loss</span></span><a class="headerlink" href="#bayesflow.helper_classes.LossHistory.total_loss" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.helper_classes.MemoryReplayBuffer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.helper_classes.</span></span><span class="sig-name descname"><span class="pre">MemoryReplayBuffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">capacity_in_batches</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#MemoryReplayBuffer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.MemoryReplayBuffer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Implements a memory replay buffer for simulation-based inference.</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.MemoryReplayBuffer.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#MemoryReplayBuffer.sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.MemoryReplayBuffer.sample" title="Permalink to this definition"></a></dt>
<dd><p>Samples <cite>batch_size</cite> number of parameter vectors and simulations from buffer.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>forward_dict</strong> – The (raw or configured) outputs of the forward model.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.MemoryReplayBuffer.store">
<span class="sig-name descname"><span class="pre">store</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">forward_dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#MemoryReplayBuffer.store"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.MemoryReplayBuffer.store" title="Permalink to this definition"></a></dt>
<dd><p>Stores simulation outputs, if internal buffer is not full.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>forward_dict</strong> (<em>dict</em>) – The confogired outputs of the forward model.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.helper_classes.RegressionLRAdjuster">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.helper_classes.</span></span><span class="sig-name descname"><span class="pre">RegressionLRAdjuster</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">period</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wait_between_fits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tolerance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">0.05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cooldown_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_resets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#RegressionLRAdjuster"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.RegressionLRAdjuster" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>This class will compute the slope of the loss trajectory and inform learning rate decay.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="bayesflow.helper_classes.RegressionLRAdjuster.file_name">
<span class="sig-name descname"><span class="pre">file_name</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'lr_adjuster'</span></em><a class="headerlink" href="#bayesflow.helper_classes.RegressionLRAdjuster.file_name" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.RegressionLRAdjuster.get_slope">
<span class="sig-name descname"><span class="pre">get_slope</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">losses</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#RegressionLRAdjuster.get_slope"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.RegressionLRAdjuster.get_slope" title="Permalink to this definition"></a></dt>
<dd><p>Fits a Huber regression on the provided loss trajectory or returns <cite>None</cite> if
not enough data points present.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.RegressionLRAdjuster.load_from_file">
<span class="sig-name descname"><span class="pre">load_from_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#RegressionLRAdjuster.load_from_file"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.RegressionLRAdjuster.load_from_file" title="Permalink to this definition"></a></dt>
<dd><p>Loads the saved LRAdjuster object from file_path.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.RegressionLRAdjuster.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#RegressionLRAdjuster.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.RegressionLRAdjuster.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets all stateful variables in preparation for a new start.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.RegressionLRAdjuster.save_to_file">
<span class="sig-name descname"><span class="pre">save_to_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#RegressionLRAdjuster.save_to_file"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.RegressionLRAdjuster.save_to_file" title="Permalink to this definition"></a></dt>
<dd><p>Saves the state parameters of a RegressionLRAdjuster object to a pickled dictionary in file_path.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.helper_classes.SimulationDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.helper_classes.</span></span><span class="sig-name descname"><span class="pre">SimulationDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">forward_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#SimulationDataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.SimulationDataset" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Helper class to create a tensorflow.data.Dataset which parses simulation dictionaries
and returns simulation dictionaries as expected by BayesFlow amortizers.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.helper_classes.SimulationMemory">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.helper_classes.</span></span><span class="sig-name descname"><span class="pre">SimulationMemory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stores_raw</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">capacity_in_batches</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#SimulationMemory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.SimulationMemory" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Helper class to keep track of a pre-determined number of simulations during training.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="bayesflow.helper_classes.SimulationMemory.file_name">
<span class="sig-name descname"><span class="pre">file_name</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'memory'</span></em><a class="headerlink" href="#bayesflow.helper_classes.SimulationMemory.file_name" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.SimulationMemory.get_memory">
<span class="sig-name descname"><span class="pre">get_memory</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#SimulationMemory.get_memory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.SimulationMemory.get_memory" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.SimulationMemory.is_full">
<span class="sig-name descname"><span class="pre">is_full</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#SimulationMemory.is_full"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.SimulationMemory.is_full" title="Permalink to this definition"></a></dt>
<dd><p>Returns True if the buffer is full, otherwise False.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.SimulationMemory.load_from_file">
<span class="sig-name descname"><span class="pre">load_from_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#SimulationMemory.load_from_file"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.SimulationMemory.load_from_file" title="Permalink to this definition"></a></dt>
<dd><p>Loads the saved <cite>SimulationMemory</cite> object from file_path.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.SimulationMemory.save_to_file">
<span class="sig-name descname"><span class="pre">save_to_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#SimulationMemory.save_to_file"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.SimulationMemory.save_to_file" title="Permalink to this definition"></a></dt>
<dd><p>Saves a <cite>SimulationMemory</cite> object to a pickled dictionary in file_path.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_classes.SimulationMemory.store">
<span class="sig-name descname"><span class="pre">store</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">forward_dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_classes.html#SimulationMemory.store"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_classes.SimulationMemory.store" title="Permalink to this definition"></a></dt>
<dd><p>Stores simulation outputs in <cite>forward_dict</cite>, if internal buffer is not full.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>forward_dict</strong> (<em>dict</em>) – The configured outputs of the forward model.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-bayesflow.helper_functions">
<span id="bayesflow-helper-functions-module"></span><h2>bayesflow.helper_functions module<a class="headerlink" href="#module-bayesflow.helper_functions" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.helper_functions.backprop_step">
<span class="sig-prename descclassname"><span class="pre">bayesflow.helper_functions.</span></span><span class="sig-name descname"><span class="pre">backprop_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amortizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_functions.html#backprop_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_functions.backprop_step" title="Permalink to this definition"></a></dt>
<dd><p>Computes the loss of the provided amortizer given an input dictionary and applies gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dict</strong> (<em>dict</em>) – The configured output of the genrative model</p></li>
<li><p><strong>amortizer</strong> (<em>tf.keras.Model</em>) – The custom amortizer. Needs to implement a compute_loss method.</p></li>
<li><p><strong>optimizer</strong> (<em>tf.keras.optimizers.Optimizer</em>) – The optimizer used to update the amortizer’s parameters.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em>) – Optional keyword arguments passed to the network’s compute_loss method</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – The outputs of the compute_loss() method of the amortizer comprising all
loss components, such as divergences or regularization.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.helper_functions.build_meta_dict">
<span class="sig-prename descclassname"><span class="pre">bayesflow.helper_functions.</span></span><span class="sig-name descname"><span class="pre">build_meta_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">user_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_setting</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#bayesflow.default_settings.MetaDictSetting" title="bayesflow.default_settings.MetaDictSetting"><span class="pre">MetaDictSetting</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span></span></span><a class="reference internal" href="_modules/bayesflow/helper_functions.html#build_meta_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_functions.build_meta_dict" title="Permalink to this definition"></a></dt>
<dd><p>Integrates a user-defined dictionary into a default dictionary.</p>
<p>Takes a user-defined dictionary and a default dictionary.</p>
<ol class="arabic simple">
<li><p>Scan the <cite>user_dict</cite> for violations by unspecified mandatory fields.</p></li>
<li><p>Merge <cite>user_dict</cite> entries into the <cite>default_dict</cite>. Considers nested dict structure.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>user_dict</strong> (<em>dict</em>) – The user’s dictionary</p></li>
<li><p><strong>default_setting</strong> (<a class="reference internal" href="#bayesflow.default_settings.MetaDictSetting" title="bayesflow.default_settings.MetaDictSetting"><em>MetaDictSetting</em></a>) – <p>The specified default setting with attributes:</p>
<ul>
<li><p><cite>meta_dict</cite>: dictionary with default values.</p></li>
<li><p><cite>mandatory_fields</cite>: list(str) keys that need to be specified by the <cite>user_dict</cite></p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>merged_dict</strong> – Merged dictionary.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.helper_functions.check_posterior_prior_shapes">
<span class="sig-prename descclassname"><span class="pre">bayesflow.helper_functions.</span></span><span class="sig-name descname"><span class="pre">check_posterior_prior_shapes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">post_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_samples</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_functions.html#check_posterior_prior_shapes"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_functions.check_posterior_prior_shapes" title="Permalink to this definition"></a></dt>
<dd><p>Checks requirements for the shapes of posterior and prior draws as
necessitated by most diagnostic functions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>post_samples</strong> (<em>np.ndarray of shape</em><em> (</em><em>n_data_sets</em><em>, </em><em>n_post_draws</em><em>, </em><em>n_params</em><em>)</em>) – The posterior draws obtained from n_data_sets</p></li>
<li><p><strong>prior_samples</strong> (<em>np.ndarray of shape</em><em> (</em><em>n_data_sets</em><em>, </em><em>n_params</em><em>)</em>) – The prior draws obtained for generating n_data_sets</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference internal" href="#bayesflow.exceptions.ShapeError" title="bayesflow.exceptions.ShapeError"><strong>ShapeError</strong></a> – If there is a deviation form the expected shapes of <cite>post_samples</cite> and <cite>prior_samples</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.helper_functions.extract_current_lr">
<span class="sig-prename descclassname"><span class="pre">bayesflow.helper_functions.</span></span><span class="sig-name descname"><span class="pre">extract_current_lr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_functions.html#extract_current_lr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_functions.extract_current_lr" title="Permalink to this definition"></a></dt>
<dd><p>Extracts current learning rate from <cite>optimizer</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optimizer</strong> (instance of subclass of <cite>tf.keras.optimizers.Optimizer</cite>) – Optimizer to extract the learning rate from</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>current_lr</strong> – Current learning rate, or <cite>None</cite> if it can’t be determined</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.float or NoneType</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.helper_functions.format_loss_string">
<span class="sig-prename descclassname"><span class="pre">bayesflow.helper_functions.</span></span><span class="sig-name descname"><span class="pre">format_loss_string</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ep</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">it</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">avg_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">slope</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ep_str</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Epoch'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">it_str</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Iter'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scalar_loss_str</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Loss'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_functions.html#format_loss_string"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_functions.format_loss_string" title="Permalink to this definition"></a></dt>
<dd><p>Prepare loss string for displaying on progress bar.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.helper_functions.merge_left_into_right">
<span class="sig-prename descclassname"><span class="pre">bayesflow.helper_functions.</span></span><span class="sig-name descname"><span class="pre">merge_left_into_right</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">left_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">right_dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_functions.html#merge_left_into_right"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_functions.merge_left_into_right" title="Permalink to this definition"></a></dt>
<dd><p>Function to merge nested dict <cite>left_dict</cite> into nested dict <cite>right_dict</cite>.</p>
</dd></dl>

</section>
<section id="module-bayesflow.helper_networks">
<span id="bayesflow-helper-networks-module"></span><h2>bayesflow.helper_networks module<a class="headerlink" href="#module-bayesflow.helper_networks" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.helper_networks.ActNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.helper_networks.</span></span><span class="sig-name descname"><span class="pre">ActNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_networks.html#ActNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_networks.ActNorm" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code></p>
<p>Implements an Activation Normalization (ActNorm) Layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_networks.ActNorm.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inverse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_networks.html#ActNorm.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_networks.ActNorm.call" title="Permalink to this definition"></a></dt>
<dd><p>Performs one pass through the actnorm layer (either inverse or forward) and normalizes
the last axis of <cite>target</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target</strong> (<em>tf.Tensor of shape</em><em> (</em><em>batch_size</em><em>, </em><em>...</em><em>)</em>) – the target variables of interest, i.e., parameters for posterior estimation</p></li>
<li><p><strong>inverse</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – Flag indicating whether to run the block forward or backwards</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>(z, log_det_J)</strong> (<em>tuple(tf.Tensor, tf.Tensor)</em>) – If inverse=False: The transformed input and the corresponding Jacobian of the transformation,
v shape: (batch_size, inp_dim), log_det_J shape: (,)</p></li>
<li><p><strong>target</strong> (<em>tf.Tensor</em>) – If inverse=True: The inversly transformed targets, shape == target.shape</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If <code class="docutils literal notranslate"><span class="pre">inverse=False</span></code>, the return is <code class="docutils literal notranslate"><span class="pre">(z,</span> <span class="pre">log_det_J)</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">inverse=True</span></code>, the return is <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.helper_networks.DenseCouplingNet">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.helper_networks.</span></span><span class="sig-name descname"><span class="pre">DenseCouplingNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_networks.html#DenseCouplingNet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_networks.DenseCouplingNet" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code></p>
<p>Implements a conditional version of a standard fully connected (FC) network.
Would also work as an unconditional estimator.</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_networks.DenseCouplingNet.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">condition</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_networks.html#DenseCouplingNet.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_networks.DenseCouplingNet.call" title="Permalink to this definition"></a></dt>
<dd><p>Concatenates target and condition and performs a forward pass through the coupling net.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target</strong> (<em>tf.Tensor</em>) – The split estimation quntities, for instance, parameters <span class="math notranslate nohighlight">\(\theta \sim p(\theta)\)</span> of interest, shape (batch_size, …)</p></li>
<li><p><strong>condition</strong> (<em>tf.Tensor</em><em> or </em><em>None</em>) – the conditioning vector of interest, for instance <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">summary(x)</span></code>, shape (batch_size, summary_dim)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.helper_networks.EquivariantModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.helper_networks.</span></span><span class="sig-name descname"><span class="pre">EquivariantModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_networks.html#EquivariantModule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_networks.EquivariantModule" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code></p>
<p>Implements an equivariant module performing an equivariant transform.</p>
<p>For details and justification, see:</p>
<p><a class="reference external" href="https://www.jmlr.org/papers/volume21/19-322/19-322.pdf">https://www.jmlr.org/papers/volume21/19-322/19-322.pdf</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_networks.EquivariantModule.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_networks.html#EquivariantModule.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_networks.EquivariantModule.call" title="Permalink to this definition"></a></dt>
<dd><p>Performs the forward pass of a learnable equivariant transform.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>tf.Tensor</em>) – Input of shape (batch_size, N, x_dim)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong> – Output of shape (batch_size, N, equiv_dim)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.helper_networks.InvariantModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.helper_networks.</span></span><span class="sig-name descname"><span class="pre">InvariantModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_networks.html#InvariantModule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_networks.InvariantModule" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code></p>
<p>Implements an invariant module performing a permutation-invariant transform.</p>
<p>For details and rationale, see:</p>
<p><a class="reference external" href="https://www.jmlr.org/papers/volume21/19-322/19-322.pdf">https://www.jmlr.org/papers/volume21/19-322/19-322.pdf</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_networks.InvariantModule.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_networks.html#InvariantModule.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_networks.InvariantModule.call" title="Permalink to this definition"></a></dt>
<dd><p>Performs the forward pass of a learnable invariant transform.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>tf.Tensor</em>) – Input of shape (batch_size, N, x_dim)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong> – Output of shape (batch_size, out_dim)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.helper_networks.MultiConv1D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.helper_networks.</span></span><span class="sig-name descname"><span class="pre">MultiConv1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_networks.html#MultiConv1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_networks.MultiConv1D" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code></p>
<p>Implements an inception-inspired 1D convolutional layer using different kernel sizes.</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_networks.MultiConv1D.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_networks.html#MultiConv1D.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_networks.MultiConv1D.call" title="Permalink to this definition"></a></dt>
<dd><p>Performs a forward pass through the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>tf.Tensor</em>) – Input of shape (batch_size, n_time_steps, n_time_series)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong> – Output of shape (batch_size, n_time_steps, n_filters)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.helper_networks.Permutation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.helper_networks.</span></span><span class="sig-name descname"><span class="pre">Permutation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_networks.html#Permutation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_networks.Permutation" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code></p>
<p>Implements a layer to permute the inputs entering a (conditional) coupling layer. Uses
fixed permutations, as these perform equally well compared to learned permutations.</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.helper_networks.Permutation.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inverse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/helper_networks.html#Permutation.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.helper_networks.Permutation.call" title="Permalink to this definition"></a></dt>
<dd><p>Permutes a batch of target vectors over the last axis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target</strong> (<em>tf.Tensor of shape</em><em> (</em><em>batch_size</em><em>, </em><em>...</em><em>)</em>) – The target vector to be permuted over its last axis.</p></li>
<li><p><strong>inverse</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – Controls if the current pass is forward (<code class="docutils literal notranslate"><span class="pre">inverse=False</span></code>) or inverse (<code class="docutils literal notranslate"><span class="pre">inverse=True</span></code>).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong> – The (un-)permuted target vector.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor of the same shape as <cite>target</cite>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-bayesflow.inference_networks">
<span id="bayesflow-inference-networks-module"></span><h2>bayesflow.inference_networks module<a class="headerlink" href="#module-bayesflow.inference_networks" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.inference_networks.EvidentialNetwork">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.inference_networks.</span></span><span class="sig-name descname"><span class="pre">EvidentialNetwork</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/inference_networks.html#EvidentialNetwork"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.inference_networks.EvidentialNetwork" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code></p>
<p>Implements a network whose outputs are the concentration parameters of a Dirichlet density.</p>
<p>Follows ideas from:</p>
<p>[1] Radev, S. T., D’Alessandro, M., Mertens, U. K., Voss, A., Köthe, U., &amp; Bürkner, P. C. (2021).
Amortized Bayesian model comparison with evidential deep learning.
IEEE Transactions on Neural Networks and Learning Systems.</p>
<p>[2] Sensoy, M., Kaplan, L., &amp; Kandemir, M. (2018).
Evidential deep learning to quantify classification uncertainty.
Advances in neural information processing systems, 31.</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.inference_networks.EvidentialNetwork.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">condition</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/inference_networks.html#EvidentialNetwork.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.inference_networks.EvidentialNetwork.call" title="Permalink to this definition"></a></dt>
<dd><p>Computes evidences for model comparison given a batch of data and optional concatenated context,
typically passed through a summayr network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>condition</strong> (<em>tf.Tensor of shape</em><em> (</em><em>batch_size</em><em>, </em><em>...</em><em>)</em>) – The input variables used for determining <cite>p(model | condition)</cite></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>evidence</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor of shape (batch_size, num_models) – the learned model evidences</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.inference_networks.EvidentialNetwork.create_config">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">create_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/inference_networks.html#EvidentialNetwork.create_config"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.inference_networks.EvidentialNetwork.create_config" title="Permalink to this definition"></a></dt>
<dd><p>“Used to create the settings dictionary for the internal networks of the invertible
network. Will fill in missing</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.inference_networks.EvidentialNetwork.evidence">
<span class="sig-name descname"><span class="pre">evidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">condition</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/inference_networks.html#EvidentialNetwork.evidence"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.inference_networks.EvidentialNetwork.evidence" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.inference_networks.EvidentialNetwork.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">condition</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/inference_networks.html#EvidentialNetwork.sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.inference_networks.EvidentialNetwork.sample" title="Permalink to this definition"></a></dt>
<dd><p>Samples posterior model probabilities from the higher-order Dirichlet density.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>condition</strong> (<em>tf.Tensor</em>) – The summary of the observed (or simulated) data, shape (n_data_sets, …)</p></li>
<li><p><strong>n_samples</strong> (<em>int</em>) – Number of samples to obtain from the approximate posterior</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>pm_samples</strong> – The posterior draws from the Dirichlet distribution, shape (num_samples, num_batch, num_models)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor or np.array</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.inference_networks.InvertibleNetwork">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.inference_networks.</span></span><span class="sig-name descname"><span class="pre">InvertibleNetwork</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/inference_networks.html#InvertibleNetwork"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.inference_networks.InvertibleNetwork" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code></p>
<p>Implements a chain of conditional invertible coupling layers for conditional density estimation.</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.inference_networks.InvertibleNetwork.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">targets</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">condition</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inverse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/inference_networks.html#InvertibleNetwork.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.inference_networks.InvertibleNetwork.call" title="Permalink to this definition"></a></dt>
<dd><p>Performs one pass through an invertible chain (either inverse or forward).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>targets</strong> (<em>tf.Tensor</em>) – The estimation quantities of interest, shape (batch_size, …)</p></li>
<li><p><strong>condition</strong> (<em>tf.Tensor</em>) – The conditional data x, shape (batch_size, summary_dim)</p></li>
<li><p><strong>inverse</strong> (<em>bool</em><em>, </em><em>default: False</em>) – Flag indicating whether to run the chain forward or backwards</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>(z, log_det_J)</strong> (<em>tuple(tf.Tensor, tf.Tensor)</em>) – If inverse=False: The transformed input and the corresponding Jacobian of the transformation,
v shape: (batch_size, …), log_det_J shape: (batch_size, …)</p></li>
<li><p><strong>target</strong> (<em>tf.Tensor</em>) – If inverse=True: The transformed out, shape (batch_size, …)</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If <code class="docutils literal notranslate"><span class="pre">inverse=False</span></code>, the return is <code class="docutils literal notranslate"><span class="pre">(z,</span> <span class="pre">log_det_J)</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">inverse=True</span></code>, the return is <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.inference_networks.InvertibleNetwork.create_config">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">create_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/inference_networks.html#InvertibleNetwork.create_config"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.inference_networks.InvertibleNetwork.create_config" title="Permalink to this definition"></a></dt>
<dd><p>“Used to create the settings dictionary for the internal networks of the invertible
network. Will fill in missing</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.inference_networks.InvertibleNetwork.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">targets</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">condition</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/inference_networks.html#InvertibleNetwork.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.inference_networks.InvertibleNetwork.forward" title="Permalink to this definition"></a></dt>
<dd><p>Performs a forward pass though the chain.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.inference_networks.InvertibleNetwork.inverse">
<span class="sig-name descname"><span class="pre">inverse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">condition</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/inference_networks.html#InvertibleNetwork.inverse"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.inference_networks.InvertibleNetwork.inverse" title="Permalink to this definition"></a></dt>
<dd><p>Performs a reverse pass through the chain. Assumes that it is only used
in inference mode, so <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> contains <code class="docutils literal notranslate"><span class="pre">training=False</span></code>.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-bayesflow.losses">
<span id="bayesflow-losses-module"></span><h2>bayesflow.losses module<a class="headerlink" href="#module-bayesflow.losses" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.losses.kl_dirichlet">
<span class="sig-prename descclassname"><span class="pre">bayesflow.losses.</span></span><span class="sig-name descname"><span class="pre">kl_dirichlet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/losses.html#kl_dirichlet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.losses.kl_dirichlet" title="Permalink to this definition"></a></dt>
<dd><p>Computes the KL divergence between a Dirichlet distribution with parameter vector alpha and a uniform Dirichlet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_indices</strong> (<em>tf.Tensor of shape</em><em> (</em><em>batch_size</em><em>, </em><em>n_models</em><em>)</em>) – one-hot-encoded true model indices</p></li>
<li><p><strong>alpha</strong> (<em>tf.Tensor of shape</em><em> (</em><em>batch_size</em><em>, </em><em>n_models</em><em>)</em>) – positive network outputs in <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">+inf]</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>kl</strong> – A single scalar representing <span class="math notranslate nohighlight">\(D_{KL}(\mathrm{Dir}(\alpha) | \mathrm{Dir}(1,1,\ldots,1) )\)</span>, shape (,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.losses.kl_latent_space_gaussian">
<span class="sig-prename descclassname"><span class="pre">bayesflow.losses.</span></span><span class="sig-name descname"><span class="pre">kl_latent_space_gaussian</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_det_J</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/losses.html#kl_latent_space_gaussian"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.losses.kl_latent_space_gaussian" title="Permalink to this definition"></a></dt>
<dd><p>Computes the Kullback-Leibler divergence between true and approximate
posterior assuming a Gaussian latent space as a source distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>z</strong> (<em>tf.Tensor of shape</em><em> (</em><em>batch_size</em><em>, </em><em>...</em><em>)</em>) – The (latent transformed) target variables</p></li>
<li><p><strong>log_det_J</strong> (<em>tf.Tensor of shape</em><em> (</em><em>batch_size</em><em>, </em><em>...</em><em>)</em>) – The logartihm of the Jacobian determinant of the transformation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – A single scalar value representing the KL loss, shape (,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Parameter estimation</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">kl_latent_space_gaussian</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">log_det_J</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.losses.kl_latent_space_student">
<span class="sig-prename descclassname"><span class="pre">bayesflow.losses.</span></span><span class="sig-name descname"><span class="pre">kl_latent_space_student</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">z</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_det_J</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/losses.html#kl_latent_space_student"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.losses.kl_latent_space_student" title="Permalink to this definition"></a></dt>
<dd><p>Computes the Kullback-Leibler divergence between true and approximate
posterior assuming latent student t-distribution as a source distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>v</strong> (<em>tf Tensor of shape</em><em> (</em><em>batch_size</em><em>, </em><em>...</em><em>)</em>) – The degrees of freedom of the latent student t-distribution</p></li>
<li><p><strong>z</strong> (<em>tf.Tensor of shape</em><em> (</em><em>batch_size</em><em>, </em><em>...</em><em>)</em>) – The (latent transformed) target variables</p></li>
<li><p><strong>log_det_J</strong> (<em>tf.Tensor of shape</em><em> (</em><em>batch_size</em><em>, </em><em>...</em><em>)</em>) – The logartihm of the Jacobian determinant of the transformation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – A single scalar value representing the KL loss, shape (,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.losses.log_loss">
<span class="sig-prename descclassname"><span class="pre">bayesflow.losses.</span></span><span class="sig-name descname"><span class="pre">log_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/losses.html#log_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.losses.log_loss" title="Permalink to this definition"></a></dt>
<dd><p>Computes the logloss given output probs and true model indices m_true.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_indices</strong> (<em>tf.Tensor of shape</em><em> (</em><em>batch_size</em><em>, </em><em>n_models</em><em>)</em>) – one-hot-encoded true model indices</p></li>
<li><p><strong>alpha</strong> (<em>tf.Tensor of shape</em><em> (</em><em>batch_size</em><em>, </em><em>n_models</em><em>)</em>) – positive network outputs in <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">+inf]</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – A single scalar Monte-Carlo approximation of the log-loss, shape (,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="bayesflow.losses.mmd_summary_space">
<span class="sig-prename descclassname"><span class="pre">bayesflow.losses.</span></span><span class="sig-name descname"><span class="pre">mmd_summary_space</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">summary_outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">z_dist=&lt;function</span> <span class="pre">random_normal&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel='gaussian'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/losses.html#mmd_summary_space"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.losses.mmd_summary_space" title="Permalink to this definition"></a></dt>
<dd><p>Computes the MMD(p(summary_otuputs) | z_dist) to re-shape the summary network outputs in
an information-preserving manner.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>summary_outputs</strong> (<em>tf Tensor of shape</em><em> (</em><em>batch_size</em><em>, </em><em>...</em><em>)</em>) – The outputs of the summary network.</p></li>
<li><p><strong>z_dist</strong> (<em>callable</em><em>, </em><em>default tf.random.normal</em>) – The latent data distribution towards which the summary outputs are optimized.</p></li>
<li><p><strong>kernel</strong> (<em>str in</em><em> (</em><em>'gaussian'</em><em>, </em><em>'inverse_multiquadratic'</em><em>)</em><em>, </em><em>default 'gaussian'</em>) – The kernel function to use for MMD computation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-bayesflow.mcmc">
<span id="bayesflow-mcmc-module"></span><h2>bayesflow.mcmc module<a class="headerlink" href="#module-bayesflow.mcmc" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.mcmc.MCMCSurrogateLikelihood">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.mcmc.</span></span><span class="sig-name descname"><span class="pre">MCMCSurrogateLikelihood</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">amortized_likelihood</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">configurator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">likelihood_postprocessor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_postprocessor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/mcmc.html#MCMCSurrogateLikelihood"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.mcmc.MCMCSurrogateLikelihood" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>An interface to provide likelihood evaluation and gradient estimation of a pre-trained
<code class="docutils literal notranslate"><span class="pre">AmortizedLikelihood</span></code> instance, which can be used in tandem with (HMC)-MCMC, as implemented,
for instance, in <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code>.</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.mcmc.MCMCSurrogateLikelihood.log_likelihood">
<span class="sig-name descname"><span class="pre">log_likelihood</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/mcmc.html#MCMCSurrogateLikelihood.log_likelihood"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.mcmc.MCMCSurrogateLikelihood.log_likelihood" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the approximate log-likelihood of targets given conditional variables.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>configurator</strong> (<em>The parameters as expected by configurator. For the default</em>) – </p>
</dd>
</dl>
<p>:param :
:param the first parameter has to be a dictionary containing the following mandatory keys:
:param :
:param if DEFAULT_KEYS unchanged: <code class="docutils literal notranslate"><span class="pre">observables</span></code> - the variables over which a condition density is learned (i.e., the observables)</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">conditions</span></code>  - the conditioning variables that the directly passed to the inference network</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>out</strong> – The output as returned by <code class="docutils literal notranslate"><span class="pre">likelihood_postprocessor</span></code>. For the default postprocessor,
this is the total log-likelihood given by the sum of all log-likelihood values.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.mcmc.MCMCSurrogateLikelihood.log_likelihood_grad">
<span class="sig-name descname"><span class="pre">log_likelihood_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/mcmc.html#MCMCSurrogateLikelihood.log_likelihood_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.mcmc.MCMCSurrogateLikelihood.log_likelihood_grad" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the gradient of the surrogate likelihood with respect to
every parameter in <code class="docutils literal notranslate"><span class="pre">conditions</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>configurator</strong> (<em>The parameters as expected by configurator. For the default</em>) – </p>
</dd>
</dl>
<p>:param :
:param the first parameter has to be a dictionary containing the following mandatory keys:
:param :
:param if <code class="docutils literal notranslate"><span class="pre">DEFAULT_KEYS</span></code> unchanged: <code class="docutils literal notranslate"><span class="pre">observables</span></code> - the variables over which a condition density is learned (i.e., the observables)</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">conditions</span></code>  - the conditioning variables that the directly passed to the inference network</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>out</strong> – The output as returned by <code class="docutils literal notranslate"><span class="pre">grad_postprocessor</span></code>. For the default postprocessor,
this is an array containing the derivative with respect to each value in <code class="docutils literal notranslate"><span class="pre">conditions</span></code>
as returned by <code class="docutils literal notranslate"><span class="pre">configurator</span></code>.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.mcmc.PyMCSurrogateLikelihood">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.mcmc.</span></span><span class="sig-name descname"><span class="pre">PyMCSurrogateLikelihood</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">amortized_likelihood</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">observables</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">configurator=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">likelihood_postprocessor=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_postprocessor=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_pymc_type=&lt;class</span> <span class="pre">'numpy.float64'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_tf_type=&lt;class</span> <span class="pre">'numpy.float32'&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/mcmc.html#PyMCSurrogateLikelihood"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.mcmc.PyMCSurrogateLikelihood" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Op</span></code>, <a class="reference internal" href="#bayesflow.mcmc.MCMCSurrogateLikelihood" title="bayesflow.mcmc.MCMCSurrogateLikelihood"><code class="xref py py-class docutils literal notranslate"><span class="pre">MCMCSurrogateLikelihood</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.mcmc.PyMCSurrogateLikelihood.grad">
<span class="sig-name descname"><span class="pre">grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/mcmc.html#PyMCSurrogateLikelihood.grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.mcmc.PyMCSurrogateLikelihood.grad" title="Permalink to this definition"></a></dt>
<dd><p>Aggregates gradients with respect to <code class="docutils literal notranslate"><span class="pre">inputs</span></code> (typically the parameter vector)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>The input variables.</em>) – </p></li>
<li><p><strong>output_grads</strong> (<em>The gradients of the output variables.</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>grads</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>The gradients with respect to each <code class="docutils literal notranslate"><span class="pre">Variable</span></code> in <code class="docutils literal notranslate"><span class="pre">inputs</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bayesflow.mcmc.PyMCSurrogateLikelihood.itypes">
<span class="sig-name descname"><span class="pre">itypes</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Type</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">[TensorType(float64,</span> <span class="pre">(None,))]</span></em><a class="headerlink" href="#bayesflow.mcmc.PyMCSurrogateLikelihood.itypes" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bayesflow.mcmc.PyMCSurrogateLikelihood.otypes">
<span class="sig-name descname"><span class="pre">otypes</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Type</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">[TensorType(float64,</span> <span class="pre">())]</span></em><a class="headerlink" href="#bayesflow.mcmc.PyMCSurrogateLikelihood.otypes" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.mcmc.PyMCSurrogateLikelihood.perform">
<span class="sig-name descname"><span class="pre">perform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">node</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/mcmc.html#PyMCSurrogateLikelihood.perform"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.mcmc.PyMCSurrogateLikelihood.perform" title="Permalink to this definition"></a></dt>
<dd><p>Computes the log-likelihood of <code class="docutils literal notranslate"><span class="pre">inputs</span></code> (typically the parameter vector of a model).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>node</strong> (The symbolic <code class="docutils literal notranslate"><span class="pre">aesara.graph.basic.Apply</span></code> node that represents this computation.) – </p></li>
<li><p><strong>inputs</strong> (<em>Immutable sequence of non-symbolic/numeric inputs. These are the values of each</em>) – <code class="docutils literal notranslate"><span class="pre">Variable</span></code> in <code class="docutils literal notranslate"><span class="pre">node.inputs</span></code>.</p></li>
<li><p><strong>outputs</strong> (<em>List of mutable single-element lists</em><em> (</em><em>do not change the length of these lists</em><em>)</em><em>.</em>) – Each sub-list corresponds to value of each <code class="docutils literal notranslate"><span class="pre">Variable</span></code> in <code class="docutils literal notranslate"><span class="pre">node.outputs</span></code>.
The primary purpose of this method is to set the values of these sub-lists.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-bayesflow.networks">
<span id="bayesflow-networks-module"></span><h2>bayesflow.networks module<a class="headerlink" href="#module-bayesflow.networks" title="Permalink to this heading"></a></h2>
<p>Meta-module for easy access of different neural network architecture interfaces</p>
</section>
<section id="module-bayesflow.simulation">
<span id="bayesflow-simulation-module"></span><h2>bayesflow.simulation module<a class="headerlink" href="#module-bayesflow.simulation" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.simulation.ContextGenerator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.simulation.</span></span><span class="sig-name descname"><span class="pre">ContextGenerator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batchable_context_fun</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">callable</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_batchable_context_fun</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">callable</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_non_batchable_for_batchable</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/simulation.html#ContextGenerator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.simulation.ContextGenerator" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Basic interface for a simulation module responsible for generating variables over which
we want to amortize during simulation-based training, but do not want to perform inference on.
Both priors and simulators in a generative framework can have their own context generators,
depending on the particular modeling goals.</p>
<p>The interface distinguishes between two types of context: batchable and non-batchable.</p>
<ul class="simple">
<li><p>Batchable context variables differ for each simulation in each training batch</p></li>
<li><p>Non-batchable context varibales stay the same for each simulation in a batch, but differ across batches</p></li>
</ul>
<p>Examples for batchable context variables include experimental design variables, design matrices, etc.
Examples for non-batchable context variables include the number of observations in an experiment, positional
encodings, time indices, etc.</p>
<p>While the latter can also be considered batchable in principle, batching them would require non-Tensor
(i.e., non-rectangular) data structures, which usually means inefficient computations.</p>
<p>Example for a simulation context which will generate a random number of observations between 1 and 100 for
each training batch:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gen</span> <span class="o">=</span> <span class="n">ContextGenerator</span><span class="p">(</span><span class="n">non_batchable_context_fun</span><span class="o">=</span><span class="k">lambda</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">))</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.simulation.ContextGenerator.batchable_context">
<span class="sig-name descname"><span class="pre">batchable_context</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/simulation.html#ContextGenerator.batchable_context"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.simulation.ContextGenerator.batchable_context" title="Permalink to this definition"></a></dt>
<dd><p>Generates ‘batch_size’ context variables given optional arguments.
Return type is a list of context variables.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.simulation.ContextGenerator.generate_context">
<span class="sig-name descname"><span class="pre">generate_context</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/simulation.html#ContextGenerator.generate_context"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.simulation.ContextGenerator.generate_context" title="Permalink to this definition"></a></dt>
<dd><p>Creates a dictionary with batchable and non batchable context.</p>
<blockquote>
<div><p>Parameters</p>
</div></blockquote>
<dl class="simple">
<dt>batch_size<span class="classifier">int</span></dt><dd><p>The batch_size argument used for batchable context.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><strong>context_dict</strong> (<em>dictionary</em>) – A dictionary with context variables with the following keys, if default keys not changed:
<code class="docutils literal notranslate"><span class="pre">batchable_context</span></code> : value
<code class="docutils literal notranslate"><span class="pre">non_batchable_context</span></code> : value</p></li>
<li><p>Note, that the values of the context variables will be <code class="docutils literal notranslate"><span class="pre">None</span></code>, if the</p></li>
<li><p><em>corresponding context-generating functions have not been provided when</em></p></li>
<li><p><em>initializing this object.</em></p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.simulation.ContextGenerator.non_batchable_context">
<span class="sig-name descname"><span class="pre">non_batchable_context</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/simulation.html#ContextGenerator.non_batchable_context"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.simulation.ContextGenerator.non_batchable_context" title="Permalink to this definition"></a></dt>
<dd><p>Generates a context variable shared across simulations in a given batch, given optional arguments.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.simulation.GenerativeModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.simulation.</span></span><span class="sig-name descname"><span class="pre">GenerativeModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prior</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">callable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">simulator</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">callable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_test</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_is_batched</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">simulator_is_batched</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'anonymous'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/simulation.html#GenerativeModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.simulation.GenerativeModel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Basic interface for a generative model in a simulation-based context.
Generally, a generative model consists of two mandatory components:</p>
<ul class="simple">
<li><p>Prior : A randomized function returning random parameter draws from a prior distribution;</p></li>
<li><p>Simulator : A function which transforms the parameters into observables in a non-deterministic manner.</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.simulation.GenerativeModel.plot_pushforward">
<span class="sig-name descname"><span class="pre">plot_pushforward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameter_draws</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">funcs_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">funcs_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_raw_sims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/simulation.html#GenerativeModel.plot_pushforward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.simulation.GenerativeModel.plot_pushforward" title="Permalink to this definition"></a></dt>
<dd><p>Creates simulations from parameter_draws (generated from self.prior if they are not passed as an argument)
and plots visualizations for them.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameter_draws</strong> (<em>numpy ndarray of the shape</em><em> (</em><em>batch_size</em><em>, </em><em>parameter_values</em><em>)</em>) – A sample of parameters. May be drawn from either the prior (which is also the default behavior if no input is specified)
or from the posterior to do a prior/posterior pushforward.</p></li>
<li><p><strong>funcs_list</strong> (<em>list of callables</em>) – A list of functions that can be used to aggregate simulation data (map a single simulation to a single real value).
The default behavior without user input is to use numpy’s mean and standard deviation functions.</p></li>
<li><p><strong>funcs_labels</strong> (<em>list of strings</em>) – A list of labels for the functions in funcs_list.
The default behavior without user input is to call the functions “Aggregator function 1, Aggregator function 2, etc.”</p></li>
<li><p><strong>batch_size</strong> (<em>integer</em>) – The number of prior draws to generate (and then create and visualizes simulations from)</p></li>
<li><p><strong>show_raw_sims</strong> (<em>boolean</em>) – Flag determining whether or not a plot of 49 raw (i.e. unaggregated) simulations is generated.
Useful for very general data exploration.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>parameters_draws</strong> (<em>numpy ndarray</em>) – The parameters provided by the user or generated internally.</p></li>
<li><p><strong>simulations</strong> (<em>numpy ndarray</em>) – The simulations generated from parameter_draws (or prior draws generated on the fly)</p></li>
<li><p><strong>aggregated_data</strong> (<em>list of numpy 1d arrays</em>) – Arrays generated from the simulations with the functions in funcs_list</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.simulation.GenerativeModel.presimulate_and_save">
<span class="sig-name descname"><span class="pre">presimulate_and_save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">folder_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_iterations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_limit</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">iterations_per_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extend_from</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/simulation.html#GenerativeModel.presimulate_and_save"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.simulation.GenerativeModel.presimulate_and_save" title="Permalink to this definition"></a></dt>
<dd><p>Simulates a dataset for single-pass offline training (called via the train_from_presimulation method
of the Trainer class in the trainers.py script).</p>
<p>One of the following pairs of parameters has to be provided:</p>
<ul class="simple">
<li><p>(iterations_per_epoch, epochs),</p></li>
<li><p>(total_iterations, iterations_per_epoch)</p></li>
<li><p>(total_iterations, epochs)</p></li>
</ul>
<p>Providing all three of the parameters in these pairs leads to a consistency check, since
incompatible combinations are possible.
<cite>memory_limit</cite> is an upper bound on the size of individual files; this can be useful to avoid running out of RAM during training.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.simulation.MultiGenerativeModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.simulation.</span></span><span class="sig-name descname"><span class="pre">MultiGenerativeModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">generative_models</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'equal'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/simulation.html#MultiGenerativeModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.simulation.MultiGenerativeModel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Basic interface for multiple generative models in a simulation-based context.
A <code class="docutils literal notranslate"><span class="pre">MultiveGenerativeModel</span></code> instance consists of a list of <code class="docutils literal notranslate"><span class="pre">GenerativeModel</span></code> instances
and a prior distribution over candidate models defined by a list of probabilities.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.simulation.Prior">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.simulation.</span></span><span class="sig-name descname"><span class="pre">Prior</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_prior_fun</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">callable</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_fun</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">callable</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_generator</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">callable</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/simulation.html#Prior"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.simulation.Prior" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Basic interface for a simulation module responsible for generating random draws from a
prior distribution.</p>
<p>The prior functions should return a np.array of simulation parameters which will be internally used
by the GenerativeModel interface for simulations.</p>
<p>An optional context generator (i.e., an instance of ContextGenerator) or a user-defined callable object
implementing the following two methods can be provided:
- context_generator.batchable_context(batch_size)
- context_generator.non_batchable_context()</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.simulation.Prior.estimate_means_and_stds">
<span class="sig-name descname"><span class="pre">estimate_means_and_stds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_draws</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/simulation.html#Prior.estimate_means_and_stds"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.simulation.Prior.estimate_means_and_stds" title="Permalink to this definition"></a></dt>
<dd><p>Estimates prior means and stds given n_draws from the prior, useful
for z-standardization of the prior draws.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_draws</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default = 1000</em><em>)</em>) – The number of random draws to obtain from the joint prior.</p></li>
<li><p><strong>*args</strong> (<em>tuple</em>) – Optional positional arguments passed to the generator functions.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em>) – Optional keyword arguments passed to the generator functions.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The estimated means and stds of the joint prior.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(prior_means, prior_stds) - tuple of np.ndarrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.simulation.Prior.logpdf">
<span class="sig-name descname"><span class="pre">logpdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prior_draws</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/simulation.html#Prior.logpdf"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.simulation.Prior.logpdf" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.simulation.Prior.plot_prior2d">
<span class="sig-name descname"><span class="pre">plot_prior2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/simulation.html#Prior.plot_prior2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.simulation.Prior.plot_prior2d" title="Permalink to this definition"></a></dt>
<dd><p>Generates a 2D plot representing bivariate prior ditributions. Uses the function
<a href="#id1"><span class="problematic" id="id2">`</span></a>bayesflow.diagnostics.plot_prior2d() internally for generating the plot.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>**kwargs</strong> (<em>dict</em>) – Optional keyword arguments passed to the <cite>plot_prior2d</cite> function.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>plt.Figure - the figure instance for optional saving</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.simulation.Simulator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.simulation.</span></span><span class="sig-name descname"><span class="pre">Simulator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_simulator_fun</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">simulator_fun</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/simulation.html#Simulator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.simulation.Simulator" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Basic interface for a simulation module responsible for generating randomized simulations given a prior
parameter distribution and optional context variables, given a user-provided simulation function.</p>
<p>The user-provided simulator functions should return a np.array of synthetic data which will be used internally
by the GenerativeModel interface for simulations.</p>
<p>An optional context generator (i.e., an instance of ContextGenerator) or a user-defined callable object
implementing the following two methods can be provided:
- context_generator.batchable_context(batch_size)
- context_generator.non_batchable_context()</p>
</dd></dl>

</section>
<section id="module-bayesflow.summary_networks">
<span id="bayesflow-summary-networks-module"></span><h2>bayesflow.summary_networks module<a class="headerlink" href="#module-bayesflow.summary_networks" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.summary_networks.InvariantNetwork">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.summary_networks.</span></span><span class="sig-name descname"><span class="pre">InvariantNetwork</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/summary_networks.html#InvariantNetwork"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.summary_networks.InvariantNetwork" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code></p>
<p>Implements a deep permutation-invariant network according to [1] and [2].</p>
<p>[1] Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., &amp; Smola, A. J. (2017).
Deep sets. Advances in neural information processing systems, 30.</p>
<p>[2] Bloem-Reddy, B., &amp; Teh, Y. W. (2020).
Probabilistic Symmetries and Invariant Neural Networks.
J. Mach. Learn. Res., 21, 90-1.</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.summary_networks.InvariantNetwork.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/summary_networks.html#InvariantNetwork.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.summary_networks.InvariantNetwork.call" title="Permalink to this definition"></a></dt>
<dd><p>Performs the forward pass of a learnable deep invariant transformation consisting of
a sequence of equivariant transforms followed by an invariant transform.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>tf.Tensor</em>) – Input of shape (batch_size, n_obs, data_dim)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong> – Output of shape (batch_size, out_dim)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.summary_networks.SequentialNetwork">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.summary_networks.</span></span><span class="sig-name descname"><span class="pre">SequentialNetwork</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/summary_networks.html#SequentialNetwork"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.summary_networks.SequentialNetwork" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code></p>
<p>Implements a sequence of <cite>MultiConv1D</cite> layers followed by an LSTM network.</p>
<p>For details and rationale, see [1]:</p>
<p>[1] Radev, S. T., Graw, F., Chen, S., Mutters, N. T., Eichel, V. M., Bärnighausen, T., &amp; Köthe, U. (2021).
OutbreakFlow: Model-based Bayesian inference of disease outbreak dynamics with invertible neural networks
and its application to the COVID-19 pandemics in Germany.
PLoS computational biology, 17(10), e1009472.</p>
<p><a class="reference external" href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009472">https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009472</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.summary_networks.SequentialNetwork.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/summary_networks.html#SequentialNetwork.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.summary_networks.SequentialNetwork.call" title="Permalink to this definition"></a></dt>
<dd><p>Performs a forward pass through the network by first passing <cite>x</cite> through the sequence of
multi-convolutional layers and then applying the LSTM network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>tf.Tensor</em>) – Input of shape (batch_size, n_time_steps, n_time_series)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong> – Output of shape (batch_size, summary_dim)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.summary_networks.SplitNetwork">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.summary_networks.</span></span><span class="sig-name descname"><span class="pre">SplitNetwork</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/summary_networks.html#SplitNetwork"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.summary_networks.SplitNetwork" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code></p>
<p>Implements a vertical stack of networks and concatenates their individual outputs. Allows for splitting
of data to provide an individual network for each split of the data.</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.summary_networks.SplitNetwork.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/summary_networks.html#SplitNetwork.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.summary_networks.SplitNetwork.call" title="Permalink to this definition"></a></dt>
<dd><p>Performs a forward pass through the subnetworks and concatenates their output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>tf.Tensor</em>) – Input of shape (batch_size, n_obs, data_dim)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong> – Output of shape (batch_size, out_dim)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-bayesflow.trainers">
<span id="bayesflow-trainers-module"></span><h2>bayesflow.trainers module<a class="headerlink" href="#module-bayesflow.trainers" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.trainers.</span></span><span class="sig-name descname"><span class="pre">Trainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">amortizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generative_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">configurator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_to_keep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_checks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/trainers.html#Trainer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>This class connects a generative model (or, already simulated data from a model) with
a configurator and a neural inference architecture for amortized inference (amortizer). A Trainer
instance is responsible for optimizing the amortizer via various forms of simulation-based training.</p>
<p>At the very minimum, the trainer must be initialized with an <code class="docutils literal notranslate"><span class="pre">amortizer</span></code> instance, which is capable
of processing the (configured) outputs of a generative model. A <code class="docutils literal notranslate"><span class="pre">configurator</span></code> will then process
the outputs of the generative model and convert them into suitable inputs for the amortizer. Users
can choose from a palette of default configurators or create their own configurators, essentially
building a modularized pipeline GenerativeModel -&gt; Configurator -&gt; Amortizer. Most complex models
wtill require custom configurators.</p>
<p>Currently, the trainer supports the following simulation-based training regimes, based on efficiency
considerations:</p>
<ul>
<li><dl>
<dt>Online training</dt><dd><p>Usage:
&gt;&gt;&gt; trainer.train_online(epochs, iterations_per_epoch, batch_size, <a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs)</p>
<p>This training regime is optimal for fast generative models which can efficiently simulated data on-the-fly.
In order for this training regime to be efficient, on-the-fly batch simulations should not take longer than 2-3 seconds.</p>
</dd>
</dl>
</li>
<li><dl>
<dt>Experience replay training</dt><dd><p>Usage:
&gt;&gt;&gt; trainer.train_experience_replay(epochs, iterations_per_epoch, batch_size, <a href="#id5"><span class="problematic" id="id6">**</span></a>kwargs)</p>
<p>This training regime is also good for fast generative models capable of efficiently simulating data on-the-fly.
Compare to pure online training, this training will keep an experience replay buffer from which simulations
are randomly sampled, so the networks will likely see some simulations multiple times.</p>
</dd>
</dl>
</li>
<li><dl>
<dt>Round-based training</dt><dd><p>Usage:
&gt;&gt;&gt; trainer.train_rounds(rounds, sim_per_round, epochs, batch_size, <a href="#id7"><span class="problematic" id="id8">**</span></a>kwargs)</p>
<p>This training regime is optimal for slow, but still reasonably performant generative models.
In order for this training regime to be efficient, on-the-fly batch simulations should not take longer than one 2-3 minutes.</p>
<p>Important: overfitting presents a danger when using small numbers of simulated data sets, so it is recommended to use
some amount of regularization for the neural amortizer(s).</p>
</dd>
</dl>
</li>
<li><dl>
<dt>Offline taining</dt><dd><p>Usage:
&gt;&gt;&gt; trainer.train_offline(simulations_dict, epochs, batch_size, <a href="#id9"><span class="problematic" id="id10">**</span></a>kwargs)</p>
<p>This training regime is optimal for very slow, external simulators, which take several minutes for a single simulation.
It assumes that all training data has been already simulated and stored on disk.</p>
<p>Important: overfitting presents a danger when using a small simulated data set, so it is recommended to use
some amount of regularization for the neural amortizer(s).</p>
</dd>
</dl>
</li>
</ul>
<p>Note: For extremely slow simulators (i.e., more than an hour of a single simulation), the BayesFlow framework
might not be the ideal choice and should probably be considered in combination with a black-box surrogate optimization method,
such as Bayesian optimization.</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.diagnose_latent2d">
<span class="sig-name descname"><span class="pre">diagnose_latent2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/trainers.html#Trainer.diagnose_latent2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.diagnose_latent2d" title="Permalink to this definition"></a></dt>
<dd><p>Performs visual pre-inference diagnostics of latent space on either provided validation data
(new simulations) or internal simulation memory.
If <code class="docutils literal notranslate"><span class="pre">inputs</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">None</span></code>, then diagnostics will be performed on the inputs, regardless
whether the <code class="docutils literal notranslate"><span class="pre">simulation_memory</span></code> of the trainer is empty or not. If <code class="docutils literal notranslate"><span class="pre">inputs</span> <span class="pre">is</span> <span class="pre">None</span></code>, then
the trainer will try to access is memory or raise a <code class="docutils literal notranslate"><span class="pre">ConfigurationError</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>None</em><em>, </em><em>list</em><em> or </em><em>dict</em><em>, </em><em>optional</em><em> (</em><em>default - None</em><em>)</em>) – The optional inputs to use</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Optional keyword arguments, which could be:
<code class="docutils literal notranslate"><span class="pre">conf_args</span></code>  - optional keyword arguments passed to the configurator
<code class="docutils literal notranslate"><span class="pre">net_args</span></code>   - optional keyword arguments passed to the amortizer
<code class="docutils literal notranslate"><span class="pre">plot_args</span></code>  - optional keyword arguments passed to <code class="docutils literal notranslate"><span class="pre">plot_latent_space_2d</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>losses</strong> – A dictionary storing the losses across epochs and iterations</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict(ep_num : list(losses))</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.diagnose_sbc_histograms">
<span class="sig-name descname"><span class="pre">diagnose_sbc_histograms</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/trainers.html#Trainer.diagnose_sbc_histograms"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.diagnose_sbc_histograms" title="Permalink to this definition"></a></dt>
<dd><p>Performs visual pre-inference diagnostics via simulation-based calibration (SBC)
(new simulations) or internal simulation memory.
If <code class="docutils literal notranslate"><span class="pre">inputs</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">None</span></code>, then diagnostics will be performed on the inputs, regardless
whether the <code class="docutils literal notranslate"><span class="pre">simulation_memory</span></code> of the trainer is empty or not. If <code class="docutils literal notranslate"><span class="pre">inputs</span> <span class="pre">is</span> <span class="pre">None</span></code>, then
the trainer will try to access is memory or raise a <code class="docutils literal notranslate"><span class="pre">ConfigurationError</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>None</em><em>, </em><em>list</em><em> or </em><em>dict</em><em>, </em><em>optional</em><em> (</em><em>default - None</em><em>)</em>) – The optional inputs to use</p></li>
<li><p><strong>n_samples</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default - None</em><em>)</em>) – The number of posterior samples to draw for each simulated data set.
If None, the number will be heuristically determined so n_sim / n_draws ~= 20</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Optional keyword arguments, which could be:
<code class="docutils literal notranslate"><span class="pre">conf_args</span></code>  - optional keyword arguments passed to the configurator
<cite>net_args`</cite>   - optional keyword arguments passed to the amortizer
<code class="docutils literal notranslate"><span class="pre">plot_args</span></code>  - optional keyword arguments passed to <code class="docutils literal notranslate"><span class="pre">plot_sbc</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>losses</strong> – A dictionary storing the losses across epochs and iterations</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict(ep_num : list(losses))</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.load_pretrained_network">
<span class="sig-name descname"><span class="pre">load_pretrained_network</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/trainers.html#Trainer.load_pretrained_network"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.load_pretrained_network" title="Permalink to this definition"></a></dt>
<dd><p>Attempts to load a pre-trained network if checkpoint path is provided and a checkpoint manager exists.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.train_experience_replay">
<span class="sig-name descname"><span class="pre">train_experience_replay</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epochs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">iterations_per_epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_checkpoint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer_capacity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optional_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_autograph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/trainers.html#Trainer.train_experience_replay"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.train_experience_replay" title="Permalink to this definition"></a></dt>
<dd><p>Trains the network(s) via experience replay using a memory replay buffer, as utilized
in reinforcement learning. Additional keyword arguments are passed to the generative mode,
configurator, and amortizer. Read below for signature.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epochs</strong> (<em>int</em>) – Number of epochs (and number of times a checkpoint is stored)</p></li>
<li><p><strong>iterations_per_epoch</strong> (<em>int</em>) – Number of batch simulations to perform per epoch</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Number of simulations to perform at each backpropagation step.</p></li>
<li><p><strong>save_checkpoint</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – A flag to decide whether to save checkpoints after each epoch,
if a <code class="docutils literal notranslate"><span class="pre">checkpoint_path</span></code> provided during initialization, otherwise ignored.</p></li>
<li><p><strong>optimizer</strong> (<em>tf.keras.optimizer.Optimizer</em><em> or </em><em>None</em>) – Optimizer for the neural network. <code class="docutils literal notranslate"><span class="pre">None</span></code> will result in <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code>
using a learning rate of 5e-4 and a cosine decay from 5e-4 to 0. A custom optimizer
will override default learning rate and schedule settings.</p></li>
<li><p><strong>reuse_optimizer</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – A flag indicating whether the optimizer instance should be treated as persistent or not.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the optimizer and its states are not stored after training has finished.
Otherwise, the optimizer will be stored as <code class="docutils literal notranslate"><span class="pre">self.optimizer</span></code> and re-used in further training runs.</p></li>
<li><p><strong>buffer_capacity</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 1000</em>) – Max number of batches to store in buffer. For instance, if <code class="docutils literal notranslate"><span class="pre">batch_size=32</span></code>
and <code class="docutils literal notranslate"><span class="pre">capacity_in_batches=1000</span></code>, then the buffer will hold a maximum of
32 * 1000 = 32000 simulations. Be careful with memory!</p></li>
<li><p><strong>optional_stopping</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – Whether to use optional stopping or not during training. Could speed up training.</p></li>
<li><p><strong>use_autograph</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – Whether to use autograph for the backprop step. Could lead to enourmous speed-ups but
could also be harder to debug.
Important! Argument will be ignored if buffer has previously been initialized!</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em><em>, </em><em>default: {}</em>) – Optional keyword arguments, which can be:
<code class="docutils literal notranslate"><span class="pre">model_args</span></code> - optional keyword arguments passed to the generative model
<code class="docutils literal notranslate"><span class="pre">conf_args</span></code>  - optional keyword arguments passed to the configurator
<code class="docutils literal notranslate"><span class="pre">net_args</span></code>   - optional keyword arguments passed to the amortizer</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>losses</strong> – A dictionary or a data frame storing the losses across epochs and iterations.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> or <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.train_from_presimulation">
<span class="sig-name descname"><span class="pre">train_from_presimulation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">presimulation_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_checkpoint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_loader</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optional_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_autograph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/trainers.html#Trainer.train_from_presimulation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.train_from_presimulation" title="Permalink to this definition"></a></dt>
<dd><p>Trains an amortizer via a modified form of offline training.</p>
<p>Like regular offline training, it assumes that parameters, data and optional context have already
been simulated (i.e., forward inference has been performed).</p>
<p>Also like regular offline training, it is faster than online training in scenarios where simulations are slow.
Unlike regular offline training, it uses each batch from the presimulated dataset only once during training.
A larger presimulated dataset is therefore required than for offline training, and the increase in speed
gained by loading simulations instead of generating them on the fly comes at a cost:
a large presimulated dataset takes up a large amount of hard drive space.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>presimulation_path</strong> (<em>str</em>) – <p>File path to the folder containing the files from the precomputed simulation.
Ideally generated using a GenerativeModel’s presimulate_and_save method, otherwise must match
the structure produced by that method:</p>
<p>Each file contains the data for one epoch (i.e. a number of batches), and must be compatible
with the custom_loader provided.
The custom_loader must read each file into a collection (either a dictionary or a list) of simulation_dict objects.
This is easily achieved with the pickle library: if the files were generated from collections of simulation_dict objects
using pickle.dump, the _default_loader (default for custom_load) will load them using pickle.load.
Training parameters like number of iterations and batch size are inferred from the files during training.</p>
</p></li>
<li><p><strong>optimizer</strong> (<em>tf.keras.optimizer.Optimizer</em>) – Optimizer for the neural network training. Since for this training, it is impossible to guess the number of
iterations beforehead, an optimizer must be provided.</p></li>
<li><p><strong>save_checkpoint</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default : True</em>) – Determines whether to save checkpoints after each epoch,
if a checkpoint_path provided during initialization, otherwise ignored.</p></li>
<li><p><strong>max_epochs</strong> (<em>int</em><em> or </em><em>None</em><em>, </em><em>optional</em><em>, </em><em>default: None</em>) – An optional parameter to limit the number of epochs.</p></li>
<li><p><strong>reuse_optimizer</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – A flag indicating whether the optimizer instance should be treated as persistent or not.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the optimizer and its states are not stored after training has finished.
Otherwise, the optimizer will be stored as <code class="docutils literal notranslate"><span class="pre">self.optimizer</span></code> and re-used in further training runs.</p></li>
<li><p><strong>custom_loader</strong> (<em>callable</em><em>, </em><em>optional</em><em>, </em><em>default: self._default_loader</em>) – Must take a string file_path as an input and output a collection (dictionary or list) of simulation_dict objects.
A simulation_dict has the keys
- <code class="docutils literal notranslate"><span class="pre">prior_non_batchable_context</span></code>,
- <code class="docutils literal notranslate"><span class="pre">prior_batchable_context</span></code>,
- <code class="docutils literal notranslate"><span class="pre">prior_draws</span></code>,
- <code class="docutils literal notranslate"><span class="pre">sim_non_batchable_context</span></code>,
- <code class="docutils literal notranslate"><span class="pre">sim_batchable_context</span></code>,
- <code class="docutils literal notranslate"><span class="pre">sim_data</span></code>.
<code class="docutils literal notranslate"><span class="pre">prior_draws</span></code> and <code class="docutils literal notranslate"><span class="pre">sim_data</span></code> must have actual data as values, the rest are optional.</p></li>
<li><p><strong>optional_stopping</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – Whether to use optional stopping or not during training. Could speed up training.</p></li>
<li><p><strong>use_autograph</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – Whether to use autograph for the backprop step. Could lead to enourmous speed-ups but
could also be harder to debug.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Optional keyword arguments, which can be:
<code class="docutils literal notranslate"><span class="pre">conf_args</span></code>  - optional keyword arguments passed to the configurator
<code class="docutils literal notranslate"><span class="pre">net_args</span></code>   - optional keyword arguments passed to the amortizer</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>losses</strong> – A dictionary or a data frame storing the losses across epochs and iterations</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> or <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.train_offline">
<span class="sig-name descname"><span class="pre">train_offline</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">simulations_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_checkpoint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optional_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_autograph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/trainers.html#Trainer.train_offline"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.train_offline" title="Permalink to this definition"></a></dt>
<dd><p>Trains an amortizer via offline learning. Assume parameters, data and optional
context have already been simulated (i.e., forward inference has been performed).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>simulations_dict</strong> (<em>dict</em>) – A dictionaty containing the simulated data / context, if using the default keys,
the method expects at least the mandatory keys <code class="docutils literal notranslate"><span class="pre">sim_data</span></code> and <code class="docutils literal notranslate"><span class="pre">prior_draws</span></code> to be present</p></li>
<li><p><strong>epochs</strong> (<em>int</em>) – Number of epochs (and number of times a checkpoint is stored)</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Number of simulations to perform at each backpropagation step</p></li>
<li><p><strong>save_checkpoint</strong> (<em>bool</em><em> (</em><em>default - True</em><em>)</em>) – Determines whether to save checkpoints after each epoch,
if a checkpoint_path provided during initialization, otherwise ignored.</p></li>
<li><p><strong>optimizer</strong> (<em>tf.keras.optimizer.Optimizer</em><em> or </em><em>None</em>) – Optimizer for the neural network. <code class="docutils literal notranslate"><span class="pre">None</span></code> will result in <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code>
using a learning rate of 5e-4 and a cosine decay from 5e-4 to 0. A custom optimizer
will override default learning rate and schedule settings.</p></li>
<li><p><strong>reuse_optimizer</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – A flag indicating whether the optimizer instance should be treated as persistent or not.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the optimizer and its states are not stored after training has finished.
Otherwise, the optimizer will be stored as <code class="docutils literal notranslate"><span class="pre">self.optimizer</span></code> and re-used in further training runs.</p></li>
<li><p><strong>optional_stopping</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – Whether to use optional stopping or not during training. Could speed up training.</p></li>
<li><p><strong>use_autograph</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – Whether to use autograph for the backprop step. Could lead to enourmous speed-ups but
could also be harder to debug.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Optional keyword arguments, which can be:
<code class="docutils literal notranslate"><span class="pre">model_args</span></code> - optional keyword arguments passed to the generative model
<code class="docutils literal notranslate"><span class="pre">conf_args</span></code>  - optional keyword arguments passed to the configurator
<code class="docutils literal notranslate"><span class="pre">net_args</span></code>   - optional keyword arguments passed to the amortizer</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>losses</strong> – A dictionary or a data frame storing the losses across epochs and iterations</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> or <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.train_online">
<span class="sig-name descname"><span class="pre">train_online</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epochs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">iterations_per_epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_checkpoint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optional_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_autograph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/trainers.html#Trainer.train_online"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.train_online" title="Permalink to this definition"></a></dt>
<dd><p>Trains an amortizer via online learning. Additional keyword arguments
are passed to the generative mode, configurator, and amortizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epochs</strong> (<em>int</em>) – Number of epochs (and number of times a checkpoint is stored)</p></li>
<li><p><strong>iterations_per_epoch</strong> (<em>int</em>) – Number of batch simulations to perform per epoch</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Number of simulations to perform at each backprop step</p></li>
<li><p><strong>save_checkpoint</strong> (<em>bool</em><em> (</em><em>default - True</em><em>)</em>) – A flag to decide whether to save checkpoints after each epoch,
if a checkpoint_path provided during initialization, otherwise ignored.</p></li>
<li><p><strong>optimizer</strong> (<em>tf.keras.optimizer.Optimizer</em><em> or </em><em>None</em>) – Optimizer for the neural network. <code class="docutils literal notranslate"><span class="pre">None</span></code> will result in <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code>
using a learning rate of 5e-4 and a cosine decay from 5e-4 to 0. A custom optimizer
will override default learning rate and schedule settings.</p></li>
<li><p><strong>reuse_optimizer</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – A flag indicating whether the optimizer instance should be treated as persistent or not.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the optimizer and its states are not stored after training has finished.
Otherwise, the optimizer will be stored as <a href="#id11"><span class="problematic" id="id12">``</span></a>self.optimizer` and re-used in further training runs.</p></li>
<li><p><strong>optional_stopping</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – Whether to use optional stopping or not during training. Could speed up training.</p></li>
<li><p><strong>use_autograph</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – Whether to use autograph for the backprop step. Could lead to enourmous speed-ups but
could also be harder to debug.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Optional keyword arguments, which can be:
<code class="docutils literal notranslate"><span class="pre">model_args</span></code> - optional keyword arguments passed to the generative model
<code class="docutils literal notranslate"><span class="pre">conf_args</span></code>  - optional keyword arguments passed to the configurator
<code class="docutils literal notranslate"><span class="pre">net_args</span></code>   - optional keyword arguments passed to the amortizer</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>losses</strong> – A dictionary storing the losses across epochs and iterations</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict or pandas.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.train_rounds">
<span class="sig-name descname"><span class="pre">train_rounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rounds</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sim_per_round</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_checkpoint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optional_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_autograph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/trainers.html#Trainer.train_rounds"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.train_rounds" title="Permalink to this definition"></a></dt>
<dd><p>Trains an amortizer via round-based learning. In each round, <code class="docutils literal notranslate"><span class="pre">sim_per_round</span></code> data sets
are simulated from the generative model and added to the data sets simulated in previous
round. Then, the networks are trained for <code class="docutils literal notranslate"><span class="pre">epochs</span></code> on the augmented set of data sets.</p>
<p>Important: Training time will increase from round to round, since the number of simulations
increases correspondingly. The final round will then train the networks on <code class="docutils literal notranslate"><span class="pre">rounds</span> <span class="pre">*</span> <span class="pre">sim_per_round</span></code>
data sets, so make sure this number does not eat up all available memory.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rounds</strong> (<em>int</em>) – Number of rounds to perform (outer loop)</p></li>
<li><p><strong>sim_per_round</strong> (<em>int</em>) – Number of simulations per round</p></li>
<li><p><strong>epochs</strong> (<em>int</em>) – Number of epochs (and number of times a checkpoint is stored, inner loop) within a round.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Number of simulations to use at each backpropagation step</p></li>
<li><p><strong>save_checkpoint</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>(</em><em>default - True</em><em>)</em>) – A flag to decide whether to save checkpoints after each epoch,
if a checkpoint_path provided during initialization, otherwise ignored.</p></li>
<li><p><strong>optimizer</strong> (<em>tf.keras.optimizer.Optimizer</em><em> or </em><em>None</em>) – Optimizer for the neural network training. <code class="docutils literal notranslate"><span class="pre">None</span></code> will result in <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code>
using a learning rate of 5e-4 and a cosine decay from 5e-4 to 0. A custom optimizer
will override default learning rate and schedule settings.</p></li>
<li><p><strong>reuse_optimizer</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – A flag indicating whether the optimizer instance should be treated as persistent or not.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the optimizer and its states are not stored after training has finished.
Otherwise, the optimizer will be stored as <code class="docutils literal notranslate"><span class="pre">self.optimizer</span></code> and re-used in further training runs.</p></li>
<li><p><strong>optional_stopping</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – Whether to use optional stopping or not during training. Could speed up training.</p></li>
<li><p><strong>use_autograph</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: True</em>) – Whether to use autograph for the backprop step. Could lead to enourmous speed-ups but
could also be harder to debug.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Optional keyword arguments, which can be:
<code class="docutils literal notranslate"><span class="pre">model_args</span></code> - optional keyword arguments passed to the generative model
<code class="docutils literal notranslate"><span class="pre">conf_args</span></code>  - optional keyword arguments passed to the configurator
<code class="docutils literal notranslate"><span class="pre">net_args</span></code>   - optional keyword arguments passed to the amortizer</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>losses</strong> – A dictionary or a data frame storing the losses across epochs and iterations</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> or <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-bayesflow.version">
<span id="bayesflow-version-module"></span><h2>bayesflow.version module<a class="headerlink" href="#module-bayesflow.version" title="Permalink to this heading"></a></h2>
</section>
<section id="module-bayesflow.wrappers">
<span id="bayesflow-wrappers-module"></span><h2>bayesflow.wrappers module<a class="headerlink" href="#module-bayesflow.wrappers" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.wrappers.SpectralNormalization">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.wrappers.</span></span><span class="sig-name descname"><span class="pre">SpectralNormalization</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/wrappers.html#SpectralNormalization"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.wrappers.SpectralNormalization" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Wrapper</span></code></p>
<p>Performs spectral normalization on neural network weights. Adapted from:</p>
<p><a class="reference external" href="https://www.tensorflow.org/addons/api_docs/python/tfa/layers/SpectralNormalization">https://www.tensorflow.org/addons/api_docs/python/tfa/layers/SpectralNormalization</a></p>
<p>This wrapper controls the Lipschitz constant of a layer by
constraining its spectral norm, which can stabilize the training of generative networks.</p>
<p>See Spectral Normalization for Generative Adversarial Networks](<a class="reference external" href="https://arxiv.org/abs/1802.05957">https://arxiv.org/abs/1802.05957</a>).</p>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.wrappers.SpectralNormalization.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/wrappers.html#SpectralNormalization.build"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.wrappers.SpectralNormalization.build" title="Permalink to this definition"></a></dt>
<dd><p>Build <cite>Layer</cite></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.wrappers.SpectralNormalization.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/wrappers.html#SpectralNormalization.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.wrappers.SpectralNormalization.call" title="Permalink to this definition"></a></dt>
<dd><p>Call <cite>Layer</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<em>tf.Tensor of shape</em><em> (</em><em>None</em><em>,</em><em>...</em><em>,</em><em>condition_dim + target_dim</em><em>)</em>) – The inputs to the corresponding layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.wrappers.SpectralNormalization.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/wrappers.html#SpectralNormalization.get_config"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.wrappers.SpectralNormalization.get_config" title="Permalink to this definition"></a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<p>Note that <cite>get_config()</cite> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.wrappers.SpectralNormalization.normalize_weights">
<span class="sig-name descname"><span class="pre">normalize_weights</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/bayesflow/wrappers.html#SpectralNormalization.normalize_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.wrappers.SpectralNormalization.normalize_weights" title="Permalink to this definition"></a></dt>
<dd><p>Generate spectral normalized weights.</p>
<p>This method will update the value of <cite>self.w</cite> with the
spectral normalized value, so that the layer is ready for <cite>call()</cite>.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-bayesflow">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-bayesflow" title="Permalink to this heading"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="modules.html" class="btn btn-neutral float-left" title="bayesflow" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="bayesflow.benchmarks.html" class="btn btn-neutral float-right" title="bayesflow.benchmarks package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Stefan T. Radev.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

</body>
</html>
