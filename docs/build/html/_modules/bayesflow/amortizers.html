<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>bayesflow.amortizers &mdash; BayesFlow beta documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> BayesFlow
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../modules.html">bayesflow</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">BayesFlow</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
      <li class="breadcrumb-item active">bayesflow.amortizers</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for bayesflow.amortizers</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) 2022 The BayesFlow Developers</span>

<span class="c1"># Permission is hereby granted, free of charge, to any person obtaining a copy</span>
<span class="c1"># of this software and associated documentation files (the &quot;Software&quot;), to deal</span>
<span class="c1"># in the Software without restriction, including without limitation the rights</span>
<span class="c1"># to use, copy, modify, merge, publish, distribute, sublicense, and/or sell</span>
<span class="c1"># copies of the Software, and to permit persons to whom the Software is</span>
<span class="c1"># furnished to do so, subject to the following conditions:</span>

<span class="c1"># The above copyright notice and this permission notice shall be included in all</span>
<span class="c1"># copies or substantial portions of the Software.</span>

<span class="c1"># THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</span>
<span class="c1"># IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,</span>
<span class="c1"># FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE</span>
<span class="c1"># AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER</span>
<span class="c1"># LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,</span>
<span class="c1"># OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE</span>
<span class="c1"># SOFTWARE.</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">bayesflow.exceptions</span> <span class="kn">import</span> <span class="n">ConfigurationError</span><span class="p">,</span> <span class="n">SummaryStatsError</span>
<span class="kn">from</span> <span class="nn">bayesflow.losses</span> <span class="kn">import</span> <span class="n">log_loss</span><span class="p">,</span> <span class="n">mmd_summary_space</span><span class="p">,</span> <span class="n">kl_dirichlet</span>
<span class="kn">from</span> <span class="nn">bayesflow.default_settings</span> <span class="kn">import</span> <span class="n">DEFAULT_KEYS</span>

<span class="kn">import</span> <span class="nn">tensorflow_probability</span> <span class="k">as</span> <span class="nn">tfp</span>

<span class="kn">from</span> <span class="nn">warnings</span> <span class="kn">import</span> <span class="n">warn</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>


<div class="viewcode-block" id="AmortizedTarget"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedTarget">[docs]</a><span class="k">class</span> <span class="nc">AmortizedTarget</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An abstract interface for an amortized learned distribution. Children should</span>
<span class="sd">    implement the following public methods:</span>

<span class="sd">    1. ``compute_loss(self, input_dict, **kwargs)``</span>
<span class="sd">    2. ``sample(input_dict, **kwargs)``</span>
<span class="sd">    3. ``log_prob(input_dict, **kwargs)``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">pass</span>

<div class="viewcode-block" id="AmortizedTarget.compute_loss"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedTarget.compute_loss">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">pass</span></div>

<div class="viewcode-block" id="AmortizedTarget.sample"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedTarget.sample">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">pass</span></div>

<div class="viewcode-block" id="AmortizedTarget.log_prob"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedTarget.log_prob">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">pass</span></div></div>


<div class="viewcode-block" id="AmortizedPosterior"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosterior">[docs]</a><span class="k">class</span> <span class="nc">AmortizedPosterior</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">,</span> <span class="n">AmortizedTarget</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A wrapper to connect an inference network for parameter estimation with an optional summary network</span>
<span class="sd">    as in the original BayesFlow set-up described in the paper:</span>

<span class="sd">    [1] Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., &amp; Köthe, U. (2020).</span>
<span class="sd">    BayesFlow: Learning complex stochastic models with invertible neural networks.</span>
<span class="sd">    IEEE Transactions on Neural Networks and Learning Systems.</span>

<span class="sd">    But also allowing for augmented functionality, such as model misspecification detection in summary space:</span>

<span class="sd">    [2] Schmitt, M., Bürkner, P. C., Köthe, U., &amp; Radev, S. T. (2022).</span>
<span class="sd">    Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks</span>
<span class="sd">    arXiv preprint arXiv:2112.08866.</span>

<span class="sd">    And learning of fat-tailed posteriors with a Student-t latent pushforward density:</span>

<span class="sd">    [3] Jaini, P., Kobyzev, I., Yu, Y., &amp; Brubaker, M. (2020, November).</span>
<span class="sd">    Tails of lipschitz triangular flows.</span>
<span class="sd">    In International Conference on Machine Learning (pp. 4673-4681). PMLR.</span>

<span class="sd">    Serves as in interface for learning ``p(parameters | data, context).``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_net</span><span class="p">,</span> <span class="n">summary_net</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">latent_dist</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">latent_is_dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                 <span class="n">summary_loss_fun</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initializes a composite neural network to represent an amortized approximate posterior.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        inference_net     : tf.keras.Model</span>
<span class="sd">            An (invertible) inference network which processes the outputs of a generative model </span>
<span class="sd">        summary_net       : tf.keras.Model or None, optional, default: None</span>
<span class="sd">            An optional summary network to compress non-vector data structures.</span>
<span class="sd">        latent_dist       : callable or None, optional, default: None</span>
<span class="sd">            The latent distribution towards which to optimize the networks. Defaults to</span>
<span class="sd">            a multivariate unit Gaussian.</span>
<span class="sd">        latent_is_dynamic : bool, optional, default: False</span>
<span class="sd">            If set to `True`, assumes that `latent_dist` is a function of the condtion and takes </span>
<span class="sd">            a different shape depending on the condition. Useful for more expressive transforms</span>
<span class="sd">            of complex distributions, such as fat-tailed or highly-multimodal distros. </span>
<span class="sd">            </span>
<span class="sd">            Important: In the case of dynamic latents, the user is responsible that the</span>
<span class="sd">            latent is appropriately parameterized! If not using `tensorflow_probability`,</span>
<span class="sd">            the `latent_dist` object needs to implement the following methods:</span>
<span class="sd">            - `latent_dist(x).log_prob(z)` and</span>
<span class="sd">            - `latent_dist(x).sample(n_samples)`</span>

<span class="sd">        summary_loss_fun  : callable or None, optional, default: None</span>
<span class="sd">            The loss function which accepts the outputs of the summary network. If None, no loss is provided.</span>
<span class="sd">        **kwargs          : dict, optional</span>
<span class="sd">            Additional keyword arguments passed to the __init__ method of a tf.keras.Model instance.</span>

<span class="sd">        Important</span>
<span class="sd">        ----------</span>
<span class="sd">        - If no `summary_net` is provided, then the output dictionary of your generative model should not contain</span>
<span class="sd">        any `sumamry_conditions`, i.e., `summary_conditions` should be set to None, otherwise these will be ignored.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">inference_net</span> <span class="o">=</span> <span class="n">inference_net</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">summary_net</span> <span class="o">=</span> <span class="n">summary_net</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_net</span><span class="o">.</span><span class="n">latent_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_is_dynamic</span> <span class="o">=</span> <span class="n">latent_is_dynamic</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">summary_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_determine_summary_loss</span><span class="p">(</span><span class="n">summary_loss_fun</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_determine_latent_dist</span><span class="p">(</span><span class="n">latent_dist</span><span class="p">)</span>

<div class="viewcode-block" id="AmortizedPosterior.call"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosterior.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">return_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a forward pass through the summary and inference network.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dict     : dict  </span>
<span class="sd">            Input dictionary containing the following mandatory keys, if DEFAULT keys unchanged:</span>
<span class="sd">            `parameters`         - the latent model parameters over which a condition density is learned</span>
<span class="sd">            `summary_conditions` - the conditioning variables (including data) that are first passed through a summary network</span>
<span class="sd">            `direct_conditions`  - the conditioning variables that the directly passed to the inference network</span>
<span class="sd">        return_summary : bool, optional, default: False</span>
<span class="sd">            A flag which determines whether the learnable data summaries (representations) are returned or not.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        net_out or (net_out, summary_out) : tuple of tf.Tensor</span>
<span class="sd">            the outputs of ``inference_net(theta, summary_net(x, c_s), c_d)``, usually a latent variable and</span>
<span class="sd">            log(det(Jacobian)), that is a tuple ``(z, log_det_J) or (sum_outputs, (z, log_det_J)) if</span>
<span class="sd">            return_summary is set to True and a summary network is defined.``</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Concatenate conditions, if given</span>
        <span class="n">summary_out</span><span class="p">,</span> <span class="n">full_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_summary_condition</span><span class="p">(</span>
            <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;summary_conditions&#39;</span><span class="p">]),</span> 
            <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;direct_conditions&#39;</span><span class="p">]),</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

        <span class="c1"># Compute output of inference net</span>
        <span class="n">net_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_net</span><span class="p">(</span><span class="n">input_dict</span><span class="p">[</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;parameters&#39;</span><span class="p">]],</span> <span class="n">full_cond</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="c1"># Return summary outputs or not, depending on parameter</span>
        <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">net_out</span><span class="p">,</span> <span class="n">summary_out</span>
        <span class="k">return</span> <span class="n">net_out</span></div>

<div class="viewcode-block" id="AmortizedPosterior.compute_loss"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosterior.compute_loss">[docs]</a>    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes the loss of the posterior amortizer given an input dictionary.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dict : dict  </span>
<span class="sd">            Input dictionary containing the following mandatory keys: </span>
<span class="sd">            `parameters`         - the latent model parameters over which a condition density is learned</span>
<span class="sd">            `summary_conditions` - the conditioning variables that are first passed through a summary network</span>
<span class="sd">            `direct_conditions`  - the conditioning variables that the directly passed to the inference network</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        total_loss : tf.Tensor of shape (1,) - the total computed loss given input variables</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="c1"># Get amortizer outputs</span>
        <span class="n">net_out</span><span class="p">,</span> <span class="n">sum_out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">return_summary</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">z</span><span class="p">,</span> <span class="n">log_det_J</span> <span class="o">=</span> <span class="n">net_out</span>

        <span class="c1"># Case summary loss should be computed</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sum_loss</span> <span class="o">=</span>  <span class="bp">self</span><span class="o">.</span><span class="n">summary_loss</span><span class="p">(</span><span class="n">sum_out</span><span class="p">)</span>
        <span class="c1"># Case no summary loss, simply add 0 for convenience</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sum_loss</span> <span class="o">=</span> <span class="mf">0.</span>
        
        <span class="c1"># Case dynamic latent space - function of summary conditions</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_is_dynamic</span><span class="p">:</span>
            <span class="n">logpdf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dist</span><span class="p">(</span><span class="n">sum_out</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="c1"># Case static latent space</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logpdf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        
        <span class="c1"># Compute and return total loss</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="o">-</span><span class="n">logpdf</span> <span class="o">-</span> <span class="n">log_det_J</span><span class="p">)</span> <span class="o">+</span> <span class="n">sum_loss</span>
        <span class="k">return</span> <span class="n">total_loss</span></div>

<div class="viewcode-block" id="AmortizedPosterior.call_loop"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosterior.call_loop">[docs]</a>    <span class="k">def</span> <span class="nf">call_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_list</span><span class="p">,</span> <span class="n">return_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a forward pass through the summary and inference network given a list of dicts </span>
<span class="sd">        with the appropriate entries (i.e., as used for the standard call method).</span>

<span class="sd">        This method is useful when GPU memory is limited or data sets have a different (non-Tensor) structure.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_list     : list of dicts, where each dict contains the following mandatory keys, if DEFAULT keys unchanged: </span>
<span class="sd">            `parameters`         - the latent model parameters over which a condition density is learned</span>
<span class="sd">            `summary_conditions` - the conditioning variables (including data) that are first passed through a summary network</span>
<span class="sd">            `direct_conditions`  - the conditioning variables that the directly passed to the inference network</span>
<span class="sd">        return_summary : bool, optional, default: False</span>
<span class="sd">            A flag which determines whether the learnable data summaries (representations) are returned or not.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        net_out or (net_out, summary_out) : tuple of tf.Tensor</span>
<span class="sd">            the outputs of ``inference_net(theta, summary_net(x, c_s), c_d)``, usually a latent variable and</span>
<span class="sd">            log(det(Jacobian)), that is a tuple ``(z, log_det_J) or (sum_outputs, (z, log_det_J)) if </span>
<span class="sd">            return_summary is set to True and a summary network is defined.`` </span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">forward_dict</span> <span class="ow">in</span> <span class="n">input_list</span><span class="p">:</span>
            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">(</span><span class="n">forward_dict</span><span class="p">,</span> <span class="n">return_summary</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
        <span class="n">net_out</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">o</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">net_out</span><span class="p">)</span></div>

<div class="viewcode-block" id="AmortizedPosterior.sample"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosterior.sample">[docs]</a>    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Generates random draws from the approximate posterior given a dictionary with conditonal variables.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dict  : dict  </span>
<span class="sd">            Input dictionary containing the following mandatory keys, if DEFAULT KEYS unchanged: </span>
<span class="sd">            `summary_conditions` : the conditioning variables (including data) that are first passed through a summary network</span>
<span class="sd">            `direct_conditions`  : the conditioning variables that the directly passed to the inference network</span>
<span class="sd">        n_samples   : int</span>
<span class="sd">            The number of posterior draws (samples) to obtain from the approximate posterior</span>
<span class="sd">        to_numpy    : bool, optional, default: True</span>
<span class="sd">            Flag indicating whether to return the samples as a `np.array` or a `tf.Tensor`.</span>
<span class="sd">        </span>
<span class="sd">        **kwargs    : dict, optional</span>
<span class="sd">            Additional keyword arguments passed to the networks</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        post_samples : tf.Tensor or np.ndarray of shape (n_data_sets, n_samples, n_params)</span>
<span class="sd">            the sampled parameters per data set</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Compute learnable summaries, if appropriate</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">conditions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_summary_condition</span><span class="p">(</span>
            <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;summary_conditions&#39;</span><span class="p">]),</span> 
            <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;direct_conditions&#39;</span><span class="p">]),</span> 
            <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

        <span class="c1"># Obtain number of data sets</span>
        <span class="n">n_data_sets</span> <span class="o">=</span> <span class="n">conditions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Obtain random draws from the approximate posterior given conditioning variables</span>
        <span class="c1"># Case dynamic, assume tensorflow_probability instance, so need to reshape output from</span>
        <span class="c1"># (n_samples, n_data_sets, latent_dim) to (n_data_sets, n_samples, latent_dim)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_is_dynamic</span><span class="p">:</span>
            <span class="n">z_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dist</span><span class="p">(</span><span class="n">conditions</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
            <span class="n">z_samples</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">z_samples</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="c1"># Case static latent - marginal samples from the specified dist</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">z_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_data_sets</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">))</span>

        <span class="c1"># Obtain random draws from the approximate posterior given conditioning variables</span>
        <span class="n">post_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_net</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">z_samples</span><span class="p">,</span> <span class="n">conditions</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Only return 2D array, if first dimensions is 1</span>
        <span class="k">if</span> <span class="n">post_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">post_samples</span> <span class="o">=</span> <span class="n">post_samples</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Return numpy version of tensor or tensor itself</span>
        <span class="k">if</span> <span class="n">to_numpy</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">post_samples</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">post_samples</span></div>

<div class="viewcode-block" id="AmortizedPosterior.sample_loop"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosterior.sample_loop">[docs]</a>    <span class="k">def</span> <span class="nf">sample_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_list</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Generates random draws from the approximate posterior given a list of dicts with conditonal variables.</span>
<span class="sd">        Useful when GPU memory is limited or data sets have a different (non-Tensor) structure. </span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_list  : list of dictionaries, each dictionary having the following mandatory keys, if DEFAULT KEYS unchanged: </span>
<span class="sd">            `summary_conditions` : the conditioning variables (including data) that are first passed through a summary network</span>
<span class="sd">            `direct_conditions`  : the conditioning variables that the directly passed to the inference network</span>
<span class="sd">        n_samples   : int</span>
<span class="sd">            The number of posterior draws (samples) to obtain from the approximate posterior</span>
<span class="sd">        to_numpy    : bool, optional, default: True</span>
<span class="sd">            Flag indicating whether to return the samples as a `np.darray` or a `tf.Tensor`</span>
<span class="sd">        **kwargs    : dict, optional</span>
<span class="sd">            Additional keyword arguments passed to the networks</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        post_samples : tf.Tensor or np.ndarray of shape (n_datasets, n_samples, n_params)</span>
<span class="sd">            the sampled parameters per data set</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">post_samples</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">input_dict</span> <span class="ow">in</span> <span class="n">input_list</span><span class="p">:</span>
            <span class="n">post_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">to_numpy</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">post_samples</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">post_samples</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></div>

<div class="viewcode-block" id="AmortizedPosterior.log_posterior"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosterior.log_posterior">[docs]</a>    <span class="k">def</span> <span class="nf">log_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculates the approximate log-posterior of targets given conditional variables via</span>
<span class="sd">        the change-of-variable formula for a conditional normalizing flow.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dict : dict  </span>
<span class="sd">            Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged: </span>
<span class="sd">            `parameters`         : the latent model parameters over which a conditional density (i.e., a posterior) is learned</span>
<span class="sd">            `summary_conditions` : the conditioning variables (including data) that are first passed through a summary network</span>
<span class="sd">            `direct_conditions`  : the conditioning variables that are directly passed to the inference network</span>
<span class="sd">        to_numpy  : bool, optional, default: True</span>
<span class="sd">            Flag indicating whether to return the lpdf values as a `np.array` or a `tf.Tensor`</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        log_post  : tf.Tensor of shape (batch_size, n_obs)</span>
<span class="sd">            the approximate log-posterior density of each each parameter </span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Compute learnable summaries, if appropriate</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">conditions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_summary_condition</span><span class="p">(</span>
            <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;summary_conditions&#39;</span><span class="p">]),</span> 
            <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;direct_conditions&#39;</span><span class="p">]),</span> 
            <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

        <span class="c1"># Forward pass through the network</span>
        <span class="n">z</span><span class="p">,</span> <span class="n">log_det_J</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_net</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;parameters&#39;</span><span class="p">]],</span> <span class="n">conditions</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="c1"># Compute approximate log posterior</span>
        <span class="c1"># Case dynamic latent - function of conditions</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_is_dynamic</span><span class="p">:</span>
            <span class="n">log_post</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dist</span><span class="p">(</span><span class="n">conditions</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_det_J</span>
        <span class="c1"># Case static latent - marginal samples from z</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">log_post</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_det_J</span>

        <span class="k">if</span> <span class="n">to_numpy</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">log_post</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">log_post</span></div>
    
<div class="viewcode-block" id="AmortizedPosterior.log_prob"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosterior.log_prob">[docs]</a>    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Identical to `log_posterior(input_dict, to_numpy, **kwargs)`.&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_posterior</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_compute_summary_condition</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">summary_conditions</span><span class="p">,</span> <span class="n">direct_conditions</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Determines how to concatenate the provided conditions.&quot;&quot;&quot;</span>

        <span class="c1"># Compute learnable summaries, if given</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_net</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sum_condition</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_net</span><span class="p">(</span><span class="n">summary_conditions</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sum_condition</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Concatenate learnable summaries with fixed summaries</span>
        <span class="k">if</span> <span class="n">sum_condition</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">direct_conditions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">full_cond</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">sum_condition</span><span class="p">,</span> <span class="n">direct_conditions</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">sum_condition</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">full_cond</span> <span class="o">=</span> <span class="n">sum_condition</span>
        <span class="k">elif</span> <span class="n">direct_conditions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">full_cond</span> <span class="o">=</span> <span class="n">direct_conditions</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">SummaryStatsError</span><span class="p">(</span><span class="s2">&quot;Could not concatenarte or determine conditioning inputs...&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sum_condition</span><span class="p">,</span> <span class="n">full_cond</span>

    <span class="k">def</span> <span class="nf">_determine_latent_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_dist</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Determines which latent distribution to use and defaults to unit normal if none provided.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">latent_dist</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">MultivariateNormalDiag</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">latent_dist</span>

    <span class="k">def</span> <span class="nf">_determine_summary_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss_fun</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Determines which summary loss to use if default `None` argument provided, otherwise return identity.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># If callable, return provided loss</span>
        <span class="k">if</span> <span class="n">loss_fun</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">callable</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">loss_fun</span>
        
        <span class="c1"># If string, check for MMD or mmd</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">str</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">loss_fun</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;mmd&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">mmd_summary_space</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;For now, only &#39;mmd&#39; is supported as a string argument for summary_loss_fun!&quot;</span><span class="p">)</span>
        <span class="c1"># Throw if loss type unexpected</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Could not infer summary_loss_fun, argument should be of type (None, callable, or str)!&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="AmortizedLikelihood"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedLikelihood">[docs]</a><span class="k">class</span> <span class="nc">AmortizedLikelihood</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">,</span> <span class="n">AmortizedTarget</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An interface for a surrogate model of a simulator, or an implicit likelihood</span>
<span class="sd">    ``p(data | parameters, context).``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">surrogate_net</span><span class="p">,</span> <span class="n">latent_dist</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initializes a composite neural architecture representing an amortized emulator </span>
<span class="sd">        for the simulator (i.e., the implicit likelihood model).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        surrogate_net : tf.keras.Model</span>
<span class="sd">            An (invertible) inference network which processes the outputs of the generative model.</span>
<span class="sd">        latent_dist       : callable or None, optional, default: None</span>
<span class="sd">            The latent distribution towards which to optimize the surrogate network outputs. Defaults to</span>
<span class="sd">            a multivariate unit Gaussian.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_net</span> <span class="o">=</span> <span class="n">surrogate_net</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_net</span><span class="o">.</span><span class="n">latent_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_determine_latent_dist</span><span class="p">(</span><span class="n">latent_dist</span><span class="p">)</span>

<div class="viewcode-block" id="AmortizedLikelihood.call"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedLikelihood.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a forward pass through the summary and inference network.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dict  : dict </span>
<span class="sd">            Input dictionary containing the following mandatory keys: </span>
<span class="sd">            `observables` - the observables over which a condition density is learned (i.e., the data)</span>
<span class="sd">            `conditions`  - the conditioning variables that the directly passed to the inference network</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        net_out</span>
<span class="sd">            the outputs of ``surrogate_net(theta, summary_net(x, c_s), c_d)``, usually a latent variable and</span>
<span class="sd">            log(det(Jacobian)), that is a tuple ``(z, log_det_J)``. </span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">net_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_net</span><span class="p">(</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;observables&#39;</span><span class="p">]],</span> 
            <span class="n">input_dict</span><span class="p">[</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;conditions&#39;</span><span class="p">]],</span> 
            <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">net_out</span></div>

<div class="viewcode-block" id="AmortizedLikelihood.call_loop"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedLikelihood.call_loop">[docs]</a>    <span class="k">def</span> <span class="nf">call_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_list</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a forward pass through the surrogate network given a list of dicts </span>
<span class="sd">        with the appropriate entries (i.e., as used for the standard call method).</span>

<span class="sd">        This method is useful when GPU memory is limited or data sets have a different (non-Tensor) structure.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_list     : list of dicts, where each dict contains the following mandatory keys, if DEFAULT keys unchanged: </span>
<span class="sd">            `observables` - the observables over which a condition density is learned (i.e., the data)</span>
<span class="sd">            `conditions`  - the conditioning variables that the directly passed to the inference network</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        net_out or (net_out, summary_out) : tuple of tf.Tensor</span>
<span class="sd">            the outputs of ``inference_net(theta, summary_net(x, c_s), c_d)``, usually a latent variable and</span>
<span class="sd">            log(det(Jacobian)), that is a tuple ``(z, log_det_J)``.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">forward_dict</span> <span class="ow">in</span> <span class="n">input_list</span><span class="p">:</span>
            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">(</span><span class="n">forward_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
        <span class="n">net_out</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">o</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">net_out</span><span class="p">)</span></div>

<div class="viewcode-block" id="AmortizedLikelihood.sample"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedLikelihood.sample">[docs]</a>    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Generates `n_samples` random draws from the surrogate likelihood given input conditions.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        input_dict   : dict  </span>
<span class="sd">            Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged: </span>
<span class="sd">            `conditions` - the conditioning variables that the directly passed to the inference network</span>
<span class="sd">        n_samples    : int</span>
<span class="sd">            The number of posterior samples to obtain from the approximate posterior</span>
<span class="sd">        to_numpy  : bool, optional, default: True</span>
<span class="sd">            Flag indicating whether to return the samples as a `np.array` or a `tf.Tensor`</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        lik_samples : tf.Tensor or np.ndarray of shape (n_datasets, n_samples, None)</span>
<span class="sd">            Simulated batch of observables from the surrogate likelihood.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Extract condition</span>
        <span class="n">conditions</span> <span class="o">=</span> <span class="n">input_dict</span><span class="p">[</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;conditions&#39;</span><span class="p">]]</span>

        <span class="c1"># Obtain number of data sets</span>
        <span class="n">n_data_sets</span> <span class="o">=</span> <span class="n">conditions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Obtain random draws from the surrogate likelihood given conditioning variables</span>
        <span class="n">z_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_data_sets</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">))</span>

        <span class="c1"># Obtain random draws from the surrogate likelihood given conditioning variables</span>
        <span class="n">lik_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_net</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">z_samples</span><span class="p">,</span> <span class="n">conditions</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Only return 2D array, if first dimensions is 1</span>
        <span class="k">if</span> <span class="n">lik_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">lik_samples</span> <span class="o">=</span> <span class="n">lik_samples</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">to_numpy</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">lik_samples</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">lik_samples</span></div>

<div class="viewcode-block" id="AmortizedLikelihood.sample_loop"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedLikelihood.sample_loop">[docs]</a>    <span class="k">def</span> <span class="nf">sample_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_list</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Generates random draws from the surrogate network given a list of dicts with conditonal variables.</span>
<span class="sd">        Useful when GPU memory is limited or data sets have a different (non-Tensor) structure. </span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_list   : list of dictionaries, each dictionary having the following mandatory keys, if DEFAULT KEYS unchanged: </span>
<span class="sd">            `conditions` - the conditioning variables that the directly passed to the inference network</span>
<span class="sd">        n_samples    : int</span>
<span class="sd">            The number of posterior draws (samples) to obtain from the approximate posterior</span>
<span class="sd">        to_numpy     : bool, optional, default: True</span>
<span class="sd">            Flag indicating whether to return the samples as a `np.array` or a `tf.Tensor`</span>
<span class="sd">        **kwargs     : dict, optional</span>
<span class="sd">            Additional keyword arguments passed to the networks</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        post_samples : tf.Tensor or np.ndarray of shape (n_data_sets, n_samples, data_dim)</span>
<span class="sd">            the sampled parameters per data set</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">post_samples</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">input_dict</span> <span class="ow">in</span> <span class="n">input_list</span><span class="p">:</span>
            <span class="n">post_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">to_numpy</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">post_samples</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">post_samples</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></div>

<div class="viewcode-block" id="AmortizedLikelihood.log_likelihood"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedLikelihood.log_likelihood">[docs]</a>    <span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculates the approximate log-likelihood of targets given conditional variables via</span>
<span class="sd">        the change-of-variable formula for a conditional normalizing flow.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dict : dict  </span>
<span class="sd">            Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged: </span>
<span class="sd">            `observables` - the variables over which a condition density is learned (i.e., the observables)</span>
<span class="sd">            `conditions`  - the conditioning variables that the directly passed to the inference network</span>
<span class="sd">        to_numpy   : bool, optional, default: True</span>
<span class="sd">            Boolean flag indicating whether to return the log-lik values as a `np.array` or a `tf.Tensor`</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        log_lik    : tf.Tensor of shape (batch_size, n_obs)</span>
<span class="sd">            the approximate log-likelihood of each data point in each data set</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Forward pass through the network</span>
        <span class="n">z</span><span class="p">,</span> <span class="n">log_det_J</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_net</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;observables&#39;</span><span class="p">]],</span> 
            <span class="n">input_dict</span><span class="p">[</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;conditions&#39;</span><span class="p">]],</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

        <span class="c1"># Compute approximate log likelihood</span>
        <span class="n">log_lik</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_det_J</span>

        <span class="c1"># Convert tensor to numpy array, if specified</span>
        <span class="k">if</span> <span class="n">to_numpy</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">log_lik</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">log_lik</span></div>

<div class="viewcode-block" id="AmortizedLikelihood.log_prob"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedLikelihood.log_prob">[docs]</a>    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Identical to `log_likelihood(input_dict, to_numpy, **kwargs)`.&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="AmortizedLikelihood.compute_loss"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedLikelihood.compute_loss">[docs]</a>    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes the loss of the amortized given input data provided in input_dict.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dict  : dict </span>
<span class="sd">            Input dictionary containing the following mandatory keys: </span>
<span class="sd">            `data`        - the observables over which a condition density is learned (i.e., the observables)</span>
<span class="sd">            `conditions`  - the conditioning variables that the directly passed to the inference network</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss        : tf.Tensor of shape (1,) - the total computed loss given input variables</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">z</span><span class="p">,</span> <span class="n">log_det_J</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_det_J</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span></div>

    <span class="k">def</span> <span class="nf">_determine_latent_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_dist</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Determines which latent distribution to use and defaults to unit normal if `None` provided.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">latent_dist</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">MultivariateNormalDiag</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">latent_dist</span></div>


<div class="viewcode-block" id="AmortizedPosteriorLikelihood"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosteriorLikelihood">[docs]</a><span class="k">class</span> <span class="nc">AmortizedPosteriorLikelihood</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">,</span> <span class="n">AmortizedTarget</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An interface for jointly learning a surrogate model of the simulator and an approximate</span>
<span class="sd">    posterior given a generative model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">amortized_posterior</span><span class="p">,</span> <span class="n">amortized_likelihood</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initializes a joint learner comprising an amortized posterior and an amortized emulator.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        amortized_posterior  : an instance of AmortizedPosterior or a custom tf.keras.Model</span>
<span class="sd">            The generative neural posterior approximator.</span>
<span class="sd">        amortized_likelihood : an instance of AmortizedLikelihood or a custom tf.keras.Model</span>
<span class="sd">            The generative neural likelihood approximator.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">amortized_posterior</span> <span class="o">=</span> <span class="n">amortized_posterior</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">amortized_likelihood</span> <span class="o">=</span> <span class="n">amortized_likelihood</span>

<div class="viewcode-block" id="AmortizedPosteriorLikelihood.call"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosteriorLikelihood.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Performs a forward pass through both amortizers.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dict  : dict </span>
<span class="sd">            Input dictionary containing the following mandatory keys: </span>
<span class="sd">            `posterior_inputs`  - The input dictionary for the amortized posterior</span>
<span class="sd">            `likelihood_inputs` - The input dictionary for the amortized likelihood</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        (post_out, lik_out) : tuple</span>
<span class="sd">            The outputs of the posterior and likelihood networks given input variables.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">post_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">amortized_posterior</span><span class="p">(</span><span class="n">input_dict</span><span class="p">[</span><span class="s1">&#39;posterior_inputs&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">lik_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">amortized_likelihood</span><span class="p">(</span><span class="n">input_dict</span><span class="p">[</span><span class="s1">&#39;likelihood_inputs&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">post_out</span><span class="p">,</span> <span class="n">lik_out</span></div>

<div class="viewcode-block" id="AmortizedPosteriorLikelihood.compute_loss"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosteriorLikelihood.compute_loss">[docs]</a>    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Computes the loss of the join amortizer by summing the corresponding amortized posterior </span>
<span class="sd">        and likelihood losses.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dict   : dict </span>
<span class="sd">            Nested input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged:: </span>
<span class="sd">            `posterior_inputs`  - The input dictionary for the amortized posterior</span>
<span class="sd">            `likelihood_inputs` - The input dictionary for the amortized likelihood</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        total_losses : dict</span>
<span class="sd">            A dictionary with keys `Post.Loss` and `Lik.Loss` containing the individual losses for the</span>
<span class="sd">            two amortizers.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">loss_post</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">amortized_posterior</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">input_dict</span><span class="p">[</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;posterior_inputs&#39;</span><span class="p">]],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">loss_lik</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">amortized_likelihood</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">input_dict</span><span class="p">[</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;likelihood_inputs&#39;</span><span class="p">]],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;Post.Loss&#39;</span><span class="p">:</span> <span class="n">loss_post</span><span class="p">,</span> <span class="s1">&#39;Lik.Loss&#39;</span><span class="p">:</span> <span class="n">loss_lik</span><span class="p">}</span></div>

<div class="viewcode-block" id="AmortizedPosteriorLikelihood.log_likelihood"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosteriorLikelihood.log_likelihood">[docs]</a>    <span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Calculates the approximate log-likelihood of data given conditional variables via</span>
<span class="sd">        the change-of-variable formula for conditional normalizing flows.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dict : dict  </span>
<span class="sd">            Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged:</span>

<span class="sd">            `observables` - the variables over which a condition density is learned (i.e., the observables)</span>
<span class="sd">            `conditions`  - the conditioning variables that are directly passed to the inference network</span>

<span class="sd">            OR a nested dictionary with key `likelihood_inputs` containing the above input dictionary</span>
<span class="sd">        to_numpy   : bool, optional, default: True</span>
<span class="sd">            Flag indicating whether to return the samples as a `np.array` or a `tf.Tensor`</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        log_lik     : tf.Tensor of shape (batch_size, n_obs)</span>
<span class="sd">            the approximate log-likelihood of each data point in each data set</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;likelihood_inputs&#39;</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">amortized_likelihood</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">(</span>
                <span class="n">input_dict</span><span class="p">[</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;likelihood_inputs&#39;</span><span class="p">]],</span> <span class="n">to_numpy</span><span class="o">=</span><span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">amortized_likelihood</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>
   
<div class="viewcode-block" id="AmortizedPosteriorLikelihood.log_posterior"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosteriorLikelihood.log_posterior">[docs]</a>    <span class="k">def</span> <span class="nf">log_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculates the approximate log-posterior of targets given conditional variables via</span>
<span class="sd">        the change-of-variable formula for conditional normalizing flows.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dict : dict  </span>
<span class="sd">            Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged:</span>

<span class="sd">            `parameters`         - the latent generative model parameters over which a condition density is learned</span>
<span class="sd">            `summary_conditions` - the conditioning variables that are first passed through a summary network</span>
<span class="sd">            `direct_conditions`  - the conditioning variables that the directly passed to the inference network</span>

<span class="sd">            OR a nested dictionary with key `posterior_inputs` containing the above input dictionary</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        log_post    : tf.Tensor of shape (batch_size, n_obs)</span>
<span class="sd">            the approximate log-likelihood of each data point in each data set</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;posterior_inputs&#39;</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">amortized_posterior</span><span class="o">.</span><span class="n">log_posterior</span><span class="p">(</span>
                <span class="n">input_dict</span><span class="p">[</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;posterior_inputs&#39;</span><span class="p">]],</span> <span class="n">to_numpy</span><span class="o">=</span><span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">amortized_posterior</span><span class="o">.</span><span class="n">log_posterior</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="AmortizedPosteriorLikelihood.log_prob"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosteriorLikelihood.log_prob">[docs]</a>    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Identical to calling separate `log_likelihood()` and `log_posterior()`.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        out_dict : dict with keys `log_posterior` and `log_likelihood` corresponding</span>
<span class="sd">        to the computed log_pdfs of the approximate posterior and likelihood.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">log_post</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_posterior</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">log_lik</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">out_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;log_posterior&#39;</span><span class="p">:</span> <span class="n">log_post</span><span class="p">,</span> <span class="s1">&#39;log_likelihood&#39;</span><span class="p">:</span> <span class="n">log_lik</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">out_dict</span></div>
   
<div class="viewcode-block" id="AmortizedPosteriorLikelihood.sample_data"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosteriorLikelihood.sample_data">[docs]</a>    <span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Generates `n_samples` random draws from the surrogate likelihood given input conditions.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        input_dict   : dict  </span>
<span class="sd">            Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged: </span>

<span class="sd">            `conditions` - the conditioning variables that the directly passed to the inference network</span>
<span class="sd">            </span>
<span class="sd">            OR a nested dictionary with key `likelihood_inputs` containing the above input dictionary</span>
<span class="sd">        n_samples    : int</span>
<span class="sd">            The number of posterior samples to obtain from the approximate posterior</span>
<span class="sd">        to_numpy     : bool, optional, default: True</span>
<span class="sd">            Flag indicating whether to return the samples as a `np.array` or a `tf.Tensor`</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        lik_samples : tf.Tensor or np.ndarray of shape (n_datasets, n_samples, None)</span>
<span class="sd">            Simulated observables from the surrogate likelihood.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;likelihood_inputs&#39;</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">amortized_likelihood</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
                <span class="n">input_dict</span><span class="p">[</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;likelihood_inputs&#39;</span><span class="p">]],</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">amortized_likelihood</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="AmortizedPosteriorLikelihood.sample_parameters"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosteriorLikelihood.sample_parameters">[docs]</a>    <span class="k">def</span> <span class="nf">sample_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Generates random draws from the approximate posterior given conditonal variables.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dict   : dict  </span>
<span class="sd">            Input dictionary containing the following mandatory keys, if DEFAULT KEYS unchanged: </span>

<span class="sd">            `summary_conditions` : the conditioning variables (including data) that are first passed through a summary network</span>
<span class="sd">            `direct_conditions`  : the conditioning variables that the directly passed to the inference network</span>

<span class="sd">            OR a nested dictionary with key `posterior_inputs` containing the above input dictionary</span>
<span class="sd">        n_samples    : int</span>
<span class="sd">            The number of posterior samples to obtain from the approximate posterior</span>
<span class="sd">        to_numpy     : bool, optional, default: True</span>
<span class="sd">            Boolean flag indicating whether to return the samples as a `np.array` or a `tf.Tensor`</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        post_samples : tf.Tensor or np.ndarray of shape (n_datasets, n_samples, n_params)</span>
<span class="sd">            the sampled parameters per data set</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="k">if</span> <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;posterior_inputs&#39;</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">amortized_posterior</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
                <span class="n">input_dict</span><span class="p">[</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;posterior_inputs&#39;</span><span class="p">]],</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">amortized_posterior</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="AmortizedPosteriorLikelihood.sample"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedPosteriorLikelihood.sample">[docs]</a>    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">n_post_samples</span><span class="p">,</span> <span class="n">n_lik_samples</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Identical to calling `sample_parameters()` and `sample_data()` separately.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        out_dict : dict with keys `posterior_samples` and `likelihood_samples` corresponding</span>
<span class="sd">        to the `n_samples` from the approximate posterior and likelihood, respectively</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">post_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_parameters</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">n_post_samples</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">lik_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_data</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">n_lik_samples</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">out_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;posterior_samples&#39;</span><span class="p">:</span> <span class="n">post_samples</span><span class="p">,</span> <span class="s1">&#39;likelihood_samples&#39;</span><span class="p">:</span> <span class="n">lik_samples</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">out_dict</span></div></div>


<div class="viewcode-block" id="AmortizedModelComparison"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedModelComparison">[docs]</a><span class="k">class</span> <span class="nc">AmortizedModelComparison</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An interface to connect an evidential network for Bayesian model comparison with an optional summary network,</span>
<span class="sd">    as described in the original paper on evidential neural networks for model comparison:</span>

<span class="sd">    [1] Radev, S. T., D&#39;Alessandro, M., Mertens, U. K., Voss, A., Köthe, U., &amp; Bürkner, P. C. (2021). </span>
<span class="sd">    Amortized bayesian model comparison with evidential deep learning. </span>
<span class="sd">    IEEE Transactions on Neural Networks and Learning Systems.</span>

<span class="sd">    Note: the original paper does not distinguish between the summary and the evidential networks, but</span>
<span class="sd">    treats them as a whole, with the appropriate architetcure dictated by the model application. For the</span>
<span class="sd">    sake of consistency, the BayesFlow library distinguisahes the two modules.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">evidence_net</span><span class="p">,</span> <span class="n">summary_net</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">loss_fun</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kl_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initializes a composite neural architecture for amortized bayesian model comparison.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        evidence_net      : tf.keras.Model</span>
<span class="sd">            A neural network which outputs model evidences. </span>
<span class="sd">        summary_net       : tf.keras.Model or None, optional, default: None</span>
<span class="sd">            An optional summary network</span>
<span class="sd">        loss_fun          : callable or None, optional, default: None</span>
<span class="sd">            The loss function which accepts the outputs of the amortizer. If None, the loss will be the log-loss.</span>
<span class="sd">        kl_weight         : callable or None, optional, defult: None</span>
<span class="sd">            The weight of the KL regularization, if None, no regualrization will be used.</span>
<span class="sd">            </span>
<span class="sd">        Important</span>
<span class="sd">        ----------</span>
<span class="sd">        - If no `summary_net` is provided, then the output dictionary of your generative model should not contain</span>
<span class="sd">        any `sumamry_conditions`, i.e., `summary_conditions` should be set to None, otherwise these will be ignored.</span>

<span class="sd">        - If no custom `loss_fun` is provided, the loss function will be the log loss for the means of a Dirichlet</span>
<span class="sd">        distribution, as described in:</span>

<span class="sd">        Radev, S. T., D&#39;Alessandro, M., Mertens, U. K., Voss, A., Köthe, U., &amp; Bürkner, P. C. (2021). </span>
<span class="sd">        Amortized bayesian model comparison with evidential deep learning. </span>
<span class="sd">        IEEE Transactions on Neural Networks and Learning Systems.</span>

<span class="sd">        - If no `kl_weight` is provided, no regularization (ground-trith preserving prior) will be used</span>
<span class="sd">        for detecting implausible observables during inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">evidence_net</span> <span class="o">=</span> <span class="n">evidence_net</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">summary_net</span> <span class="o">=</span> <span class="n">summary_net</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_determine_loss</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kl_weight</span> <span class="o">=</span> <span class="n">kl_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_models</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evidence_net</span><span class="o">.</span><span class="n">num_models</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">return_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Performs a forward pass through both networks.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dict     : dict </span>
<span class="sd">            Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged</span>
<span class="sd">            `summary_conditions` - the conditioning variables that are first passed through a summary network</span>
<span class="sd">            `direct_conditions`  - the conditioning variables that the directly passed to the evidential network</span>
<span class="sd">            `model_indices`      - the ground-truth, one-hot encoded model indices sampled from the model prior</span>
<span class="sd">        return_summary : bool, optional, default: False</span>
<span class="sd">            Indicates whether the summary network outputs are returned along the estimated evidences.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        net_out : tf.Tensor of shape (batch_size, num_models) or tuple of (net_out (batch_size, num_models), </span>
<span class="sd">                  summary_out (batch_size, summary_dim)), the latter being the summary network outputs, if</span>
<span class="sd">                  `return_summary` set to True.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">summary_out</span><span class="p">,</span> <span class="n">full_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_summary_condition</span><span class="p">(</span>
            <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;summary_conditions&#39;</span><span class="p">]),</span> 
            <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;direct_conditions&#39;</span><span class="p">]),</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

        <span class="n">net_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evidence_net</span><span class="p">(</span><span class="n">full_cond</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_summary</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">net_out</span>
        <span class="k">return</span> <span class="n">net_out</span><span class="p">,</span> <span class="n">summary_out</span>

<div class="viewcode-block" id="AmortizedModelComparison.compute_loss"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedModelComparison.compute_loss">[docs]</a>    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes the loss of the amortized model comparison instance.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dict  : dict </span>
<span class="sd">            Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged:: </span>
<span class="sd">            `summary_conditions` - the conditioning variables that are first passed through a summary network</span>
<span class="sd">            `direct_conditions`  - the conditioning variables that the directly passed to the evidence network</span>
<span class="sd">            `model_indices`      - the ground-truth, one-hot encoded model indices sampled from the model prior</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        total_loss  : tf.Tensor of shape (1,) - the total computed loss given input variables</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">input_dict</span><span class="p">[</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;model_indices&#39;</span><span class="p">]],</span> <span class="n">alphas</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kl_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">kl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kl_weight</span> <span class="o">*</span> <span class="n">kl_dirichlet</span><span class="p">(</span><span class="n">input_dict</span><span class="p">[</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;model_indices&#39;</span><span class="p">]],</span> <span class="n">alphas</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">loss</span> <span class="o">+</span> <span class="n">kl</span></div>

<div class="viewcode-block" id="AmortizedModelComparison.sample"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedModelComparison.sample">[docs]</a>    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Samples posterior model probabilities from the higher order Dirichlet density.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dict : dict</span>
<span class="sd">            Input dictionary containing the following mandatory keys, if DEFAULT_KEYS unchanged</span>
<span class="sd">            `summary_conditions` - the conditioning variables that are first passed through a summary network</span>
<span class="sd">            `direct_conditions`  - the conditioning variables that the directly passed to the evidential network</span>
<span class="sd">            `model_indices`      - the ground-truth, one-hot encoded model indices sampled from the model prior</span>
<span class="sd">        n_samples  : int</span>
<span class="sd">            Number of samples to obtain from the approximate posterior</span>
<span class="sd">        to_numpy   : bool, default: True</span>
<span class="sd">            Flag indicating whether to return the samples as a np.array or a tf.Tensor</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pm_samples : tf.Tensor or np.array</span>
<span class="sd">            The posterior draws from the Dirichlet distribution, shape (num_samples, num_batch, num_models)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">full_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_summary_condition</span><span class="p">(</span>
            <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;summary_conditions&#39;</span><span class="p">]),</span> 
            <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;direct_conditions&#39;</span><span class="p">]),</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">evidence_net</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">full_cond</span><span class="p">,</span> <span class="n">to_numpy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="AmortizedModelComparison.evidence"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedModelComparison.evidence">[docs]</a>    <span class="k">def</span> <span class="nf">evidence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes the evidence for the competing models given the data sets</span>
<span class="sd">        contained in `input_dict`.&quot;&quot;&quot;</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">full_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_summary_condition</span><span class="p">(</span>
            <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;summary_conditions&#39;</span><span class="p">]),</span> 
            <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;direct_conditions&#39;</span><span class="p">]),</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

        <span class="n">alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">full_cond</span><span class="p">,</span> <span class="n">return_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">to_numpy</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">alphas</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">alphas</span></div>

<div class="viewcode-block" id="AmortizedModelComparison.uncertainty_score"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.AmortizedModelComparison.uncertainty_score">[docs]</a>    <span class="k">def</span> <span class="nf">uncertainty_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes the uncertainy score according to sum(alphas) / num_models.&quot;&quot;&quot;</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">full_cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_summary_condition</span><span class="p">(</span>
            <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;summary_conditions&#39;</span><span class="p">]),</span> 
            <span class="n">input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DEFAULT_KEYS</span><span class="p">[</span><span class="s1">&#39;direct_conditions&#39;</span><span class="p">]),</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

        <span class="n">alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">full_cond</span><span class="p">,</span> <span class="n">return_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">evidence_net</span><span class="o">.</span><span class="n">num_models</span>
        <span class="k">if</span> <span class="n">to_numpy</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">u</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">u</span></div>

    <span class="k">def</span> <span class="nf">_compute_summary_condition</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">summary_conditions</span><span class="p">,</span> <span class="n">direct_conditions</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Determines how to concatenate the provided conditions.&quot;&quot;&quot;</span>

        <span class="c1"># Compute learnable summaries, if given</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_net</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sum_condition</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_net</span><span class="p">(</span><span class="n">summary_conditions</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sum_condition</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Concatenate learnable summaries with fixed summaries, this </span>
        <span class="k">if</span> <span class="n">sum_condition</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">direct_conditions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">full_cond</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">sum_condition</span><span class="p">,</span> <span class="n">direct_conditions</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">sum_condition</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">full_cond</span> <span class="o">=</span> <span class="n">sum_condition</span>
        <span class="k">elif</span> <span class="n">direct_conditions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">full_cond</span> <span class="o">=</span> <span class="n">direct_conditions</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">SummaryStatsError</span><span class="p">(</span><span class="s2">&quot;Could not concatenarte or determine conditioning inputs...&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sum_condition</span><span class="p">,</span> <span class="n">full_cond</span>

    <span class="k">def</span> <span class="nf">_determine_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss_fun</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Helper method to determine loss function to use.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">loss_fun</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">log_loss</span>
        <span class="k">elif</span> <span class="n">callable</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">loss_fun</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">ConfigurationError</span><span class="p">(</span><span class="s2">&quot;Loss function is neither default (`None`) not callable. Please provide a valid loss function!&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="SingleModelAmortizer"><a class="viewcode-back" href="../../bayesflow.html#bayesflow.amortizers.SingleModelAmortizer">[docs]</a><span class="k">class</span> <span class="nc">SingleModelAmortizer</span><span class="p">(</span><span class="n">AmortizedPosterior</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deprecated class for amortizer posterior estimation.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init_subclass__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1"> will be deprecated. Use `AmortizedPosterior` instead.&#39;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init_subclass__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1"> will be deprecated. Use `AmortizedPosterior` instead.&#39;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Stefan T. Radev.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>