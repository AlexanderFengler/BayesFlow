{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2df8c31",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Model-specification\" data-toc-modified-id=\"Model-specification-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model specification</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-loop\" data-toc-modified-id=\"Training-loop-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Training loop</a></span></li><li><span><a href=\"#Diagnostics\" data-toc-modified-id=\"Diagnostics-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Diagnostics</a></span></li><li><span><a href=\"#Inspecting-the-summary-space\" data-toc-modified-id=\"Inspecting-the-summary-space-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Inspecting the summary space</a></span></li></ul></li><li><span><a href=\"#Observed-Data:-Misspecification-Detection\" data-toc-modified-id=\"Observed-Data:-Misspecification-Detection-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Observed Data: Misspecification Detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualization-in-data-space\" data-toc-modified-id=\"Visualization-in-data-space-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Visualization in data space</a></span></li><li><span><a href=\"#Detecting-misspecification-in-summary-space\" data-toc-modified-id=\"Detecting-misspecification-in-summary-space-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Detecting misspecification in summary space</a></span></li></ul></li><li><span><a href=\"#Sensitivity-to-Misspecification\" data-toc-modified-id=\"Sensitivity-to-Misspecification-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Sensitivity to Misspecification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Computing-the-sensitivity\" data-toc-modified-id=\"Computing-the-sensitivity-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Computing the sensitivity</a></span></li><li><span><a href=\"#Plotting-the-results\" data-toc-modified-id=\"Plotting-the-results-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Plotting the results</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f53e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayesflow as bf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e175999-940f-4220-8696-7ba877600f89",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34cc2c7-65d4-43b5-b71b-0515443a1abb",
   "metadata": {},
   "source": [
    "Under certain regularity conditions, the theory on simulation-based inference assures that the neural posterior estimator $q_{\\phi}$ samples from the exact posterior $p(\\theta\\,|\\,x)$ after convergence.\n",
    "\n",
    "However, the neural posterior approximator is optimized with respect to the prior of the generative model which we specify for the training process.\n",
    "When the generative model at test time (the \"true data generating process\") deviates from the one used during training, the guarantees for the approximate neural posterior no longer hold and the approximate posterior samples can be wrong in essentially arbitrary ways.\n",
    "\n",
    "The precise definition of model misspecification in amortized inference, along with extensive implications and experiments, is available in the following pre-print: https://arxiv.org/abs/2112.08866"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3be58",
   "metadata": {},
   "source": [
    "![](../images/model_misspecification_amortized_sbi.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab21025-c5ad-406b-a7cb-d5c59d129601",
   "metadata": {},
   "source": [
    "# Model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb30c6ac",
   "metadata": {},
   "source": [
    "The general Bayesian forward model can be formulated as a two-step process:\n",
    "\n",
    "$$\n",
    "\\theta \\sim p(\\theta) \\qquad x\\sim p(x|\\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "We specify a fairly simple generative model where the means of a 2-dimensional Gaussian shall be estimated: $\\theta=\\mu=(\\mu_1, \\mu_2)$. Consequently, the likelihood $p(x|\\theta)$ is a Gaussian $\\mathcal{N}(\\mu, \\Sigma)$ with location $\\mu$ and covariance matrix $\\Sigma$. The prior distribution $p(\\theta)$ over the inference targets is again a Gaussian $\\mathcal{N}(\\mu_0, \\Sigma_0)$ with location $\\mu_0$ and covariance $\\Sigma_0$.\n",
    "\n",
    "Consequently, the forward model is\n",
    "\n",
    "$$\n",
    "\\mu\\sim\\mathcal{N}(\\mu_0, \\Sigma_0) \\qquad x\\sim\\mathcal{N}(\\mu, \\Sigma)\n",
    "$$\n",
    "\n",
    "with fixed parameters $\\mu_0, \\Sigma_0, \\Sigma$, while we desire posterior inference over the parameter $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de375a6",
   "metadata": {},
   "source": [
    "We choose $\\mu_0=0, \\Sigma_0=\\mathbb{I}, \\Sigma=\\mathbb{I}$ as fixed parameters of the generative model for training the neural posterior approximator. Each simulated data set contains $N=100$ observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35decbdc-2fcd-4afd-9c73-e96d7572ec53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prior(D=2, mu=0., sigma=1.0):\n",
    "    \"\"\"Gaussian prior random number generator.\"\"\"\n",
    "    return np.random.default_rng().normal(loc=mu, scale=sigma, size=D)\n",
    "\n",
    "def simulator(theta, n_obs=100, scale=1.0):\n",
    "    \"\"\"Gaussian likelihood random number generator\"\"\"\n",
    "    return np.random.default_rng().normal(loc=theta, scale=scale, size=(n_obs, theta.shape[0]))\n",
    "\n",
    "generative_model = bf.simulation.GenerativeModel(prior=prior, \n",
    "                                                 simulator=simulator, \n",
    "                                                 name=\"Generative Model: Training\",\n",
    "                                                 simulator_is_batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6c728",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f806c",
   "metadata": {},
   "source": [
    "We choose a `DeepSet` (1) to learn 2 summary statistics from the data, which are then passed to the posterior network and jointly optimized.\n",
    "The Inference network is a standard `InvertibleNetwork` with four coupling layers and the `AmortizedPosterior` combines the inference and summary networks. Since we desire model misspecification detection via a structured summary space (2), we select `summary_loss_fun=\"MMD\"` and the amortizer combines its losses correctly.\n",
    "Finally, the `trainer` wraps the generative model and the amortizer into a consistent object for training and subsequent sampling.\n",
    "\n",
    "(1) Zaheer et al (2017): https://arxiv.org/abs/1703.06114\n",
    "\n",
    "(2) Schmitt et al (2021): https://arxiv.org/abs/2112.08866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eff441",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_net = bf.networks.DeepSet(summary_dim=2)\n",
    "inference_net = bf.networks.InvertibleNetwork(num_params=2, num_coupling_layers=2)\n",
    "amortizer = bf.amortizers.AmortizedPosterior(inference_net, summary_net, summary_loss_fun=\"MMD\")\n",
    "trainer = bf.trainers.Trainer(generative_model=generative_model, amortizer=amortizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674126e",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65269680",
   "metadata": {},
   "source": [
    "Because the inference problem is simple, we just train for 15 epochs with 500 iterations per epoch and a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fc49d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = trainer.train_online(epochs=15, iterations_per_epoch=500, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbbdebb",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541347e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = bf.diagnostics.plot_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1cde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = trainer.diagnose_sbc_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb3cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sims = trainer.configurator(trainer.generative_model(200))\n",
    "posterior_draws = amortizer.sample(new_sims, n_samples=500)\n",
    "\n",
    "_ = bf.diagnostics.plot_recovery(posterior_draws, new_sims['parameters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3acc4",
   "metadata": {},
   "source": [
    "## Inspecting the summary space\n",
    "\n",
    "In fact, the summary space has essentially converged to a unit Gaussian for samples from the generative model which we used to train the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = trainer.configurator(trainer.generative_model(10000))\n",
    "summary_statistics = trainer.amortizer.summary_net(simulations['summary_conditions'])\n",
    "theta = simulations['parameters']\n",
    "\n",
    "_ = bf.diagnostics.plot_latent_space_2d(summary_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf961a09-c49f-4443-b1fb-584cdc80d7c8",
   "metadata": {},
   "source": [
    "# Observed Data: Misspecification Detection\n",
    "\n",
    "After assessing the converged neural posterior approximator's performance for the generative model from training, we will now perform inference on data from a different data generating process. In a real-life analysis, this would be the observed data $x_{\\text{obs}}$ from an experiment or study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0678795f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a727c31",
   "metadata": {},
   "source": [
    "For this illustration, we choose the prior scale $\\tau_0$ as the source of misspecification. That means that we observe 1000 data sets $\\{x_{\\text{obs}}^{(k)}\\}_{k=1}^{1000}$ from a generative model with prior scale $\\tau_0=4$. Consequently, the prior covariance is $4\\cdot\\mathbb{I}=\\begin{pmatrix}4&0\\\\0&4\\end{pmatrix}$. The remaining fixed parameters $\\mu_0$ and $\\Sigma$ are unaltered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633bfc4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prior_obs(D=2, mu=0.0, sigma=4.0):\n",
    "    \"\"\"Gaussian prior random number generator.\"\"\"\n",
    "    return np.random.default_rng().normal(loc=mu, scale=sigma, size=D)\n",
    "\n",
    "def simulator_obs(theta, n_obs=100, scale=1.0):\n",
    "    \"\"\"Gaussian likelihood random number generator\"\"\"\n",
    "    return np.random.default_rng().normal(loc=theta, scale=scale, size=(n_obs, theta.shape[0]))\n",
    "\n",
    "generative_model_obs = bf.simulation.GenerativeModel(prior=prior_obs, \n",
    "                                                     simulator=simulator_obs, \n",
    "                                                     name=\"Generative Model: Observed\",\n",
    "                                                     simulator_is_batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dacb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 simulated data sets from the well-specified model from training (for reference)\n",
    "simulations = trainer.configurator(trainer.generative_model(1000))\n",
    "x = simulations['summary_conditions']\n",
    "\n",
    "# 1000 \"observed\" data sets with different prior covariance\n",
    "simulations_obs = trainer.configurator(generative_model_obs(1000))\n",
    "x_obs = simulations_obs['summary_conditions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11508c3",
   "metadata": {},
   "source": [
    "## Visualization in data space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16aca3b",
   "metadata": {},
   "source": [
    "Let's visualize some of the data $x_{\\text{obs}}$ from that generative model. This plot lives on the data domain $\\mathbb{R}^2$ and depicts the data $x_{\\text{obs}}$. Each color is one data set $k=1,\\ldots,1000$, and all points of one color form the respective data set $x_{\\text{obs}}^{(k)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fdeb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_sets_visualization = 10\n",
    "colors = cm.viridis(np.linspace(0, 1, n_data_sets_visualization))\n",
    "indices = list(range(n_data_sets_visualization))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "for idx, c in zip(indices, colors):\n",
    "    ax1.scatter(x[idx, :, 0], x[idx, :, 1], color=c, alpha=0.7)\n",
    "    ax2.scatter(x_obs[idx, :, 0], x_obs[idx, :, 1], color=c, alpha=0.7)\n",
    "\n",
    "for ax in (ax1, ax2):\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(-10, 10)\n",
    "    ax.set_aspect('equal')\n",
    "    sns.despine()\n",
    "    \n",
    "ax1.set_title(\"data from well-specified model\")\n",
    "ax2.set_title(\"observed data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd544d",
   "metadata": {},
   "source": [
    "## Detecting misspecification in summary space\n",
    "\n",
    "As proposed in the paper (2), we will detect the deviating observed data as deviations in the structured summary space. Therefore, we compute the learned summary statistics of the well-specified data $h_{\\psi}(x)$ and for the observed data $h_{\\psi}(x_{\\text{obs}})$ by a simple pass through the trainer's summary network $h_{\\psi}$.\n",
    "\n",
    "\n",
    "(2) Schmitt et al (2021): https://arxiv.org/abs/2112.08866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85062ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_statistics = trainer.amortizer.summary_net(x)\n",
    "summary_statistics_obs = trainer.amortizer.summary_net(x_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3cbcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = cm.viridis(np.linspace(0.1, 0.9, 2))\n",
    "\n",
    "plt.scatter(summary_statistics_obs[:, 0], summary_statistics_obs[:, 1], color=colors[0], label=r\"observed: $h_{\\psi}(x_{obs})$\")\n",
    "plt.scatter(summary_statistics[:, 0], summary_statistics[:, 1], color=colors[1], label=\"well-specified: $h_{\\psi}(x)$\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.gca().set_aspect('equal')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bea4e7",
   "metadata": {},
   "source": [
    "This visual discrepancy can be quantified in many ways. In this case, we choose the *Maximum Mean Discrepancy*, more specifically its biased estimator (3), as implemented in `bayesflow.computational_utilities.maximum_mean_discrepancy`.\n",
    "The larger the MMD, the more do the samples deviate.\n",
    "\n",
    "(3) Gretton (2012): https://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c18862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesflow import computational_utilities\n",
    "\n",
    "mmd = computational_utilities.maximum_mean_discrepancy(s, s_obs)\n",
    "\n",
    "print(f\"Estimated MMD in summary space: {mmd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822fc97c-f4f4-49aa-8c2c-6d028b31ee9c",
   "metadata": {},
   "source": [
    "# Sensitivity to Misspecification\n",
    "\n",
    "The submodule `bayesflow.sensitivity` contains functions to analyze the sensitivity of a converged `Trainer` (i.e., the neural posterior estimator) to model misspecification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cf80ac",
   "metadata": {},
   "source": [
    "We start by redefining the generative model with the possibility to increase the model's misspecification through two settings `p1` and `p2`. Therefore, we define a function `generative_model_misspecified(p1, p2)`. The function takes the settings `p1` and `p2` as input and returns a (potentially misspecified) generative model.\n",
    "\n",
    "In our Gaussian example, we let `p1` control the prior location ($\\mu_0=\\mathtt{p1}$) while `p2` controls the scale of a diagonal covariance matrix $\\Sigma_0$ such that $\\Sigma_0=\\mathtt{p2}\\cdot\\mathbb{I}$. In this example, both settings cause prior misspecification. Inducing other types of misspecification (e.g., simulator or noise) follows the same principle.\n",
    "\n",
    "The consequence: If `p1=0` and `p2=1`, the `generative_model_misspecified` function yields the original well-specified model from training. For all other values for `p1` and `p2`, the resulting generative model differs.\n",
    "\n",
    "**Implementation details:** \n",
    "\n",
    "- The `partial` application pattern lets us pre-load the `prior` with custom arguments and pass this pre-loaded function into the generative model. We use this technique to use `p1` and `p2` as parameters in the prior callable.\n",
    "- We skip the generative model's consistency checks and setup outputs via `skip_test=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ed033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def generative_model_misspecified(p1, p2):\n",
    "    prior_ = partial(prior, D=2, mu=p1, sigma=p2)\n",
    "    simulator_ = partial(simulator, scale=1.0)\n",
    "    generative_model_ = bf.simulation.GenerativeModel(prior_, simulator_, simulator_is_batched=False, skip_test=True)\n",
    "    return generative_model_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9afb3b0",
   "metadata": {},
   "source": [
    "In the next step, we provide meta-information for the sensitivity analysis:\n",
    "\n",
    "- Names of the settings `p1` and `p2`: proper axis labels\n",
    "- Range of the settings `p1` and `p2`: defining the experiment's grid\n",
    "- well-specified value for the settings `p1` and `p2` (i.e., `p1=0` and `p2=1` in our example): dashed lines for the baseline configuration in the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f42d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_config = {\n",
    "    \"name\": r\"$\\mu_0$ (prior location)\",\n",
    "    \"values\": np.linspace(-0.1, 3.1, num=20),\n",
    "    \"well_specified_value\": 0.0,\n",
    "}\n",
    "p2_config = {\n",
    "    \"name\": r\"$\\tau_0$ (prior scale)\",\n",
    "    \"values\": np.linspace(0.1, 10.1, num=20),\n",
    "    \"well_specified_value\": 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c198a",
   "metadata": {},
   "source": [
    "## Computing the sensitivity\n",
    "\n",
    "As described above, the `bf.sensitivity.misspecification_experiment` function requires the converged `Trainer`, the factory for misspecified models, and meta-information on the settings. In addition, the number of posterior samples per simulated data set as well as the total number of simulated data sets per setting configuration can be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e87cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_error, summary_mmd = bf.sensitivity.misspecification_experiment(\n",
    "    trainer=trainer,\n",
    "    generator=generative_model_misspecified,\n",
    "    first_config_dict=p1_config,\n",
    "    second_config_dict=p2_config,\n",
    "    n_posterior_samples=500,\n",
    "    n_sim=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919073b",
   "metadata": {},
   "source": [
    "## Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9317ad22",
   "metadata": {},
   "source": [
    "Model misspecification with respect to both prior location $\\mu_0$ and scale $\\tau_0$ worsen the average posterior recovery in terms of aggregated RMSE. However, the converged posterior approximator seems to be relatively robust against moderate misspecifications in these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab10c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = bf.sensitivity.plot_model_misspecification_sensitivity(posterior_error, p1_config, p2_config,\n",
    "                                                          plot_config={'vmin': None})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee01b222",
   "metadata": {},
   "source": [
    "The MMD plot clearly shows that the summary space MMD is lowest when the model is well-specified (coordinates `(0, 1)`). When either the prior location $\\mu_0$ or the prior scale $\\tau_0$ changes, the summary MMD increases and we're alerted of the model misspecification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = bf.sensitivity.plot_model_misspecification_sensitivity(summary_mmd, p1_config, p2_config,\n",
    "                                                          plot_config={'vmin': None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b1b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fabafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "311px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
